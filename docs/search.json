[
  {
    "objectID": "sessions/13/session.html",
    "href": "sessions/13/session.html",
    "title": "Session 13 - From Data to Conclusions",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select “Save Link As”\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n← Session 12\n\n\nSchedule\n\n\n[End of Course] →",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 13 - From Data to Conclusions"
    ]
  },
  {
    "objectID": "sessions/11/session.html",
    "href": "sessions/11/session.html",
    "title": "Session 11 - Networks",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select “Save Link As”\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n← Session 10\n\n\nSchedule\n\n\nSession 12 →",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 11 - Networks"
    ]
  },
  {
    "objectID": "sessions/09/session.html",
    "href": "sessions/09/session.html",
    "title": "Session 09 - Text as Data: Topic Modeling",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select “Save Link As”\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n← Session 08\n\n\nSchedule\n\n\nSession 10 →",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 9 - Topic Modeling"
    ]
  },
  {
    "objectID": "sessions/07/session.html",
    "href": "sessions/07/session.html",
    "title": "Session 07 - Text as Data: Dictionary Methods and Word Embeddings",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select “Save Link As”\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n← Session 06\n\n\nSchedule\n\n\nSession 08 →",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 7 - Dictionary Methods & Word Embeddings"
    ]
  },
  {
    "objectID": "sessions/05/session.html",
    "href": "sessions/05/session.html",
    "title": "Session 05 - Descriptive Data",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select “Save Link As”\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n← Session 04\n\n\nSchedule\n\n\nSession 06 →",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 5 - Descriptive Data"
    ]
  },
  {
    "objectID": "sessions/03/session.html",
    "href": "sessions/03/session.html",
    "title": "Session 03 - Big Data: GGPlot and Visualizations",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select “Save Link As”\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n← Session 02\n\n\nSchedule\n\n\nSession 04 →",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 3 - GGPlot and Visualizations"
    ]
  },
  {
    "objectID": "sessions/01/session.html",
    "href": "sessions/01/session.html",
    "title": "Session 01 - Why Computational Social Science?",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select “Save Link As”\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n← Home\n\n\nSchedule\n\n\nSession 02 →",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 1 - Why Computational Social Science?"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Course Schedule - Masters",
    "section": "",
    "text": "Session\nDate\nTopic\nAssignment\nReadings\n\n\n\n\n1\nJan 26\nWhy Computational Social Science?\n\nView\n\n\n2\nFeb 2\nBig Data: Data Collection and Wrangling\n\nView\n\n\n3\nFeb 9\nBig Data: GGPlot and Visualizations\n\nView\n\n\n4\nFeb 17*\nComputational Research\n\nView\n\n\n5\nFeb 23\nDescriptive Data\n\nView\n\n\n6\nMar 2\nStatistics\nBenchmark 1 Due\nView\n\n\n7\nMar 16\nText as Data: Dictionary Methods\n\nView\n\n\n8\nMar 23\nMidterm Exam\nMidterm\n\n\n\n9\nMar 30\nText as Data: Topic Modeling\n\nView\n\n\n10\nApr 6\nUnsupervised Machine Learning\n\nView\n\n\n11\nApr 13\nNetworks\n\nView\n\n\n12\nApr 22*\nOnline Social Movements\n\nView\n\n\n13\nApr 27\nFrom Data to Conclusions\n\nView\n\n\n\nMay 4\n\nFinal Papers Due\n\n\n\n\n*Monday schedule (university holiday adjustment)",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - Masters"
    ]
  },
  {
    "objectID": "schedule.html#section-a---mondays-230-515-pm",
    "href": "schedule.html#section-a---mondays-230-515-pm",
    "title": "Course Schedule - Masters",
    "section": "",
    "text": "Session\nDate\nTopic\nAssignment\nReadings\n\n\n\n\n1\nJan 26\nWhy Computational Social Science?\n\nView\n\n\n2\nFeb 2\nBig Data: Data Collection and Wrangling\n\nView\n\n\n3\nFeb 9\nBig Data: GGPlot and Visualizations\n\nView\n\n\n4\nFeb 17*\nComputational Research\n\nView\n\n\n5\nFeb 23\nDescriptive Data\n\nView\n\n\n6\nMar 2\nStatistics\nBenchmark 1 Due\nView\n\n\n7\nMar 16\nText as Data: Dictionary Methods\n\nView\n\n\n8\nMar 23\nMidterm Exam\nMidterm\n\n\n\n9\nMar 30\nText as Data: Topic Modeling\n\nView\n\n\n10\nApr 6\nUnsupervised Machine Learning\n\nView\n\n\n11\nApr 13\nNetworks\n\nView\n\n\n12\nApr 22*\nOnline Social Movements\n\nView\n\n\n13\nApr 27\nFrom Data to Conclusions\n\nView\n\n\n\nMay 4\n\nFinal Papers Due\n\n\n\n\n*Monday schedule (university holiday adjustment)",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - Masters"
    ]
  },
  {
    "objectID": "schedule.html#section-b---wednesdays-230-515-pm",
    "href": "schedule.html#section-b---wednesdays-230-515-pm",
    "title": "Course Schedule - Masters",
    "section": "Section B - Wednesdays 2:30-5:15 PM",
    "text": "Section B - Wednesdays 2:30-5:15 PM\n\n\n\nSession\nDate\nTopic\nAssignment\nReadings\n\n\n\n\n1\nJan 21\nWhy Computational Social Science?\n\nView\n\n\n2\nJan 28\nBig Data: Data Collection and Wrangling\n\nView\n\n\n3\nFeb 4\nBig Data: GGPlot and Visualizations\n\nView\n\n\n4\nFeb 11\nComputational Research\n\nView\n\n\n5\nFeb 18\nDescriptive Data\n\nView\n\n\n6\nFeb 25\nStatistics\nBenchmark 1 Due\nView\n\n\n7\nMar 4\nText as Data: Dictionary Methods\n\nView\n\n\n8\nMar 18\nMidterm Exam\nMidterm\n\n\n\n9\nMar 25\nText as Data: Topic Modeling\n\nView\n\n\n10\nApr 1\nUnsupervised Machine Learning\n\nView\n\n\n11\nApr 8\nNetworks\n\nView\n\n\n12\nApr 15\nOnline Social Movements\n\nView\n\n\n13\nApr 29\nFrom Data to Conclusions\n\nView\n\n\n\nMay 4\n\nFinal Papers Due",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - Masters"
    ]
  },
  {
    "objectID": "schedule.html#required-readings",
    "href": "schedule.html#required-readings",
    "title": "Course Schedule - Masters",
    "section": "Required Readings",
    "text": "Required Readings\n\nSession 1: Why Computational Social Science?\n\nHofman, J. M., Watts, D. J., Athey, S., Garip, F., Griffiths, T. L., Kleinberg, J., Margetts, H., Mullainathan, S., Salganik, M. J., Vazire, S., Vespignani, A., & Yarkoni, T. (2021). Integrating explanation and prediction in computational social science. Nature, 595(7866), 181-188.\nLazer, D. M. J., Pentland, A., Watts, D. J., Aral, S., Athey, S., Contractor, N., Freelon, D., Gonzalez-Bailon, S., King, G., Margetts, H., Nelson, A., Salganik, M. J., Strohmaier, M., Vespignani, A., & Wagner, C. (2020). Computational social science: Obstacles and opportunities. Science, 369(6507), 1060-1062.\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for Data Science - Whole Game: Chapters 1-8. Available at: https://r4ds.hadley.nz/\n\nBack to Schedule\n\n\n\nSession 2: Big Data - Data Collection and Wrangling\n\nCappella, J. N. (2017). Vectors into the Future of Mass and Interpersonal Communication Research: Big Data, Social Media, and Computational Social Science. Human Communication Research, 43(4), 545-558.\nHealy, K. (2018). Data Visualization: A Practical Introduction - Chapter 2: Make a Plot.\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for Data Science - Section: Visualize. Available at: https://r4ds.hadley.nz/\n\nBack to Schedule\n\n\n\nSession 3: Big Data - GGPlot and Visualizations\n\nThulin, M. (2024). Modern Statistics with R - Chapter 5: Dealing with messy data. Available at: https://www.modernstatisticswithr.com/\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for Data Science - Section: Transform. Available at: https://r4ds.hadley.nz/\n\nBack to Schedule\n\n\n\nSession 4: Computational Research\n\nPeng, R. D. (2022). R Programming for Data Science - Chapters 13 & 14. Available at: https://bookdown.org/rdpeng/rprogdatascience/\n\nBack to Schedule\n\n\n\nSession 5: Descriptive Data\n\nCernat, A. (2014). Introduction to Regression Models. In Longitudinal Data Analysis Using R.\nHealy, K. (2018). Data Visualization: A Practical Introduction - Section 6: Work with models.\n\nBack to Schedule\n\n\n\nSession 6: Statistics\n\nThulin, M. (2024). Modern Statistics with R - Chapter 8: Regression models. Available at: https://www.modernstatisticswithr.com/\n\nBack to Schedule\n\n\n\nSession 7: Text as Data - Dictionary Methods\n\nCarley, K. (1994). Extracting culture through textual analysis. Poetics, 22(4), 291-312.\nPang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up? Sentiment Classification using Machine Learning Techniques. arXiv. http://arxiv.org/abs/cs/0205070\nStracqualursi, L., & Agati, P. (2022). Tweet topics and sentiments relating to distance learning among Italian Twitter users. Scientific Reports, 12(1).\n\nBack to Schedule\n\n\n\nSession 9: Text as Data - Topic Modeling\n\nDiMaggio, P., Nag, M., & Blei, D. (2013). Exploiting affinities between topic modeling and the sociological perspective on culture: Application to newspaper coverage of U.S. government arts funding. Poetics, 41(6), 570-606.\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267-297.\n\nBack to Schedule\n\n\n\nSession 10: Unsupervised Machine Learning\nNo assigned readings for this session.\nBack to Schedule\n\n\n\nSession 11: Networks\n\nBreiger, R. L. (1974). The Duality of Persons and Groups. Social Forces, 53(2), 181.\nOphir, Y., Walter, D., Arnon, D., Lokmanoglu, A. D., Tizzoni, M., Carota, J., D’Antiga, L., & Nicastro, E. (2021). The Framing of COVID-19 in Italian Media and Its Relationship with Community Mobility: A Mixed-Method Approach. Journal of Health Communication, 26(3), 161-173.\nTorres, M., & Cantú, F. (2022). Learning to See: Convolutional Neural Networks for the Analysis of Social Science Data. Political Analysis, 30(1), 113-131.\n\nBack to Schedule\n\n\n\nSession 12: Online Social Movements\n\nThulin, M. (2024). Modern Statistics with R - Chapter 11: Predictive modelling and machine learning. Available at: https://www.modernstatisticswithr.com/\n\nBack to Schedule\n\n\n\nSession 13: From Data to Conclusions\n\nBower, B. (2018, July 27). ‘Replication crisis’ spurs reforms in how science studies are done. Science News. https://www.sciencenews.org/blog/science-the-public/replication-crisis-psychology-science-studies-statistics\nLazer, D., Kennedy, R., King, G., & Vespignani, A. (2014). The Parable of Google Flu: Traps in Big Data Analysis. Science, 343(6176), 1203-1205.\n\nBack to Schedule",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - Masters"
    ]
  },
  {
    "objectID": "schedule.html#notes",
    "href": "schedule.html#notes",
    "title": "Course Schedule - Masters",
    "section": "Notes",
    "text": "Notes\nAll readings are available on Blackboard under the “Readings” folder.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - Masters"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html",
    "href": "resources/cheatsheets.html",
    "title": "R Cheatsheets",
    "section": "",
    "text": "Collection of helpful R cheatsheets for the course.\n\nR Markdown\ntidyr\ndata import\nRegular Expression\nLubridate\nggplot2",
    "crumbs": [
      "Home",
      "Resources",
      "R Cheatsheets"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html#essential-r-cheatsheets",
    "href": "resources/cheatsheets.html#essential-r-cheatsheets",
    "title": "R Cheatsheets",
    "section": "",
    "text": "Collection of helpful R cheatsheets for the course.\n\nR Markdown\ntidyr\ndata import\nRegular Expression\nLubridate\nggplot2",
    "crumbs": [
      "Home",
      "Resources",
      "R Cheatsheets"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html#external-resources",
    "href": "resources/cheatsheets.html#external-resources",
    "title": "R Cheatsheets",
    "section": "External Resources",
    "text": "External Resources\n\nRStudio Cheatsheets",
    "crumbs": [
      "Home",
      "Resources",
      "R Cheatsheets"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Data Analysis and Visualization",
    "section": "",
    "text": "Welcome to Social Data Analysis and Visualization (Masters) and Computer-Assisted Text Analysis (PhD)!\nAs researchers, we now have various big data and tools to learn about human behavior and nature. With the growth of electronic sources such as cell phones, online messaging platforms, and applications, every second new data is being added globally. This course provides a hands-on tutorial on introductory methods in Computational Social Science, which can be used in academic research and the non-academic industry specializing in data science.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Social Data Analysis and Visualization",
    "section": "",
    "text": "Welcome to Social Data Analysis and Visualization (Masters) and Computer-Assisted Text Analysis (PhD)!\nAs researchers, we now have various big data and tools to learn about human behavior and nature. With the growth of electronic sources such as cell phones, online messaging platforms, and applications, every second new data is being added globally. This course provides a hands-on tutorial on introductory methods in Computational Social Science, which can be used in academic research and the non-academic industry specializing in data science.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "Social Data Analysis and Visualization",
    "section": "Course Information",
    "text": "Course Information\nInstructor: Dr. Ayse D. Lokmanoglu\nEmail: alokman@bu.edu\nOffice Hours: Wednesdays 10:00 am - 1:30 pm\nOffice Hours Sign-up Link\nTeaching Assistant: Max Wong, maxwong@bu.edu",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#course-goals",
    "href": "index.html#course-goals",
    "title": "Social Data Analysis and Visualization",
    "section": "Course Goals",
    "text": "Course Goals\nThe course has three main goals:\n\nTechnical Skills: Introduce students to R Studio and methods of text-mining and analysis\nCritical Thinking: Connect theory and methods while discussing ethical and practical concerns\nIndependent Learning: Prepare students with skills to adapt to evolving methodologies and programming languages",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Social Data Analysis and Visualization",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the course, students will be able to:\n\nUnderstand various computational methods available to social scientists\nImplement these methods in social science research\nBe aware of ethical and practical concerns with these methods\nUse these methods in current and future research\nDevelop independent learning and problem-solving skills",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "Social Data Analysis and Visualization",
    "section": "Quick Links",
    "text": "Quick Links\n\n\nCourse Materials\n\nSyllabus on Blackboard\nSchedule\nInstallation Guide\n\n\n\nResources\n\nR Cheatsheets\nDatasets\nBlackboard\n\n\n\nExternal Links\n\nGitHub Repository",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Social Data Analysis and Visualization",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\n\n\n\nImportantBefore the First Class\n\n\n\n\nInstall R and RStudio - Follow our installation guide\nJoin Blackboard - Access course materials and readings\nReview the syllabus - Familiarize yourself with course policies\nCheck the schedule - See upcoming topics and due dates",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#what-youll-learn",
    "href": "index.html#what-youll-learn",
    "title": "Social Data Analysis and Visualization",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\nThis course covers the following computational methods:\n\n\n\n\n\n\n\n\nWeek\nTopic\nMethods\n\n\n\n\n1-3\nR Fundamentals\nR Studio basics, data wrangling, visualization\n\n\n4-6\nStatistical Analysis\nDescriptive statistics, regression models, trend analysis\n\n\n7-10\nText Analysis\nDictionary methods, sentiment analysis, topic modeling\n\n\n11-12\nAdvanced Methods\nNetwork analysis, machine learning, predictive modeling\n\n\n13\nEthics & Best Practices\nData statements, reproducibility, ethical considerations",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#required-materials",
    "href": "index.html#required-materials",
    "title": "Social Data Analysis and Visualization",
    "section": "Required Materials",
    "text": "Required Materials\n\nSoftware: R and RStudio (free, open-source)\nTextbooks: All readings provided on Blackboard\nComputer: Reliable laptop with internet access (contact IT if needed)",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#support-resources",
    "href": "index.html#support-resources",
    "title": "Social Data Analysis and Visualization",
    "section": "Support Resources",
    "text": "Support Resources\nNeed help? Check out these resources:\n\nCOM Writing Center: Writing assistance for all assignments\nBU IT Support: Technical help with software and computers\nOffice of Disability Services: Accommodations for students with disabilities\nDean of Students: Support for food/housing insecurity or other challenges\n\nSee syllabus at blackboard for complete contact information.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "Dr. Ayse D. Lokmanoglu Lecture 3, (B) Feb 4, (A) Feb 9\n\n\n\n\n\nSection\nTopic\n\n\n\n\n1\nIntroduction to ggplot2\n\n\n1.1\nThe Grammar of Graphics\n\n\n1.2\nBasic ggplot Structure\n\n\n2\nGeoms: Types of Graphs\n\n\n2.1\nCommon Geoms Overview\n\n\n2.2\nScatter Plots (geom_point())\n\n\n2.3\nLine Plots (geom_line())\n\n\n2.4\nBar Charts (geom_bar(), geom_col())\n\n\n2.5\nHistograms (geom_histogram())\n\n\n2.6\nBox Plots (geom_boxplot())\n\n\n3\nAesthetics\n\n\n3.1\nMapping vs. Setting Aesthetics\n\n\n3.2\nColor, Fill, and Alpha\n\n\n3.3\nSize, Shape, and Linetype\n\n\n4\nLabels and Annotations\n\n\n4.1\nAdding Labels with labs()\n\n\n4.2\nAdding Text and Annotations\n\n\n5\nScales and Axes\n\n\n5.1\nCustomizing Continuous Scales\n\n\n5.2\nCustomizing Discrete Scales\n\n\n5.3\nDate Scales\n\n\n5.4\nColor Scales\n\n\n6\nThemes\n\n\n6.1\nBuilt-in Themes\n\n\n6.2\nCustomizing Theme Elements\n\n\n6.3\nExternal Themes (ggthemes)\n\n\n7\nFacets\n\n\n7.1\nfacet_wrap()\n\n\n7.2\nfacet_grid()\n\n\n7.3\nCustomizing Facet Labels\n\n\n8\nColor Palettes\n\n\n8.1\nManual Colors\n\n\n8.2\nColorBrewer\n\n\n8.3\nViridis\n\n\n8.4\nFun Palettes (wesanderson, NatParksPalettes)\n\n\n\n\n\n\n\nALWAYS load our libraries first\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n\n\n\n\n\nggplot2 is a powerful visualization package that follows the Grammar of Graphics - a systematic approach to building plots layer by layer.\nCore Components:\n\n\n\n\n\n\n\n\nComponent\nDescription\nExample\n\n\n\n\nData\nThe dataset you’re plotting\ndata = mtcars\n\n\nAesthetics\nMappings between data and visual properties\naes(x = wt, y = mpg)\n\n\nGeometries\nThe type of plot\ngeom_point(), geom_bar()\n\n\nScales\nControl how data maps to visual properties\nscale_x_continuous()\n\n\nFacets\nSplit data into panels\nfacet_wrap(~ cyl)\n\n\nThemes\nControl non-data elements\ntheme_minimal()\n\n\n\nGGPlot Cheat Sheet\n\n\n\n\nEvery ggplot follows this basic structure:\nggplot(data = &lt;DATA&gt;, aes(&lt;MAPPINGS&gt;)) +\n  &lt;GEOM_FUNCTION&gt;() +\n  &lt;OPTIONAL_LAYERS&gt;\nCreate a sample dataset:\ndf &lt;- data.frame(\n  Movie_Title = c(\n    \"YOLO\", \"Successor\", \"Pegasus 2\", \"Deadpool & Wolverine\", \"Moana 2\", \n    \"The Hidden Blade\", \"Avatar: The Spirit Returns\"\n  ),\n  Release_Date = c(\n    \"January 2024\", \"February 2024\", \"March 2024\", \"July 2024\", \n    \"November 2024\", \"April 2024\", \"December 2024\"\n  ),\n  China_Box_Office_Gross = c(\n    479597304, 469612890, 466930272, 450000000, 350000000, 320000000, 550000000\n  ),\n  US_Box_Office_Gross = c(\n    310000000, 280000000, 290000000, 438000000, 221000000, 75000000, 600000000\n  ),\n  Total_Worldwide_Gross = c(\n    479597304, 469612890, 466930272, 850000000, 421000000, 400000000, 1300000000\n  )\n)\nBasic scatter plot:\nggplot(data = df, aes(x = China_Box_Office_Gross, y = US_Box_Office_Gross)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeom Function\nDescription\nKey Aesthetics\n\n\n\n\ngeom_point()\nScatter plot\nx, y, color, size, shape\n\n\ngeom_line()\nLine plot\nx, y, color, linetype\n\n\ngeom_bar()\nBar chart (counts)\nx, fill\n\n\ngeom_col()\nBar chart (values)\nx, y, fill\n\n\ngeom_histogram()\nHistogram\nx, fill, bins\n\n\ngeom_boxplot()\nBox plot\nx, y, fill\n\n\ngeom_smooth()\nTrend line\nx, y, method\n\n\ngeom_text()\nAdd text\nx, y, label\n\n\n\nAdditional Geoms:\n\n\n\nGeom Function\nDescription\n\n\n\n\ngeom_area()\nArea under a line\n\n\ngeom_violin()\nViolin plot\n\n\ngeom_density()\nDensity curve\n\n\ngeom_tile()\nHeatmap tiles\n\n\ngeom_segment()\nLine segments\n\n\ngeom_abline()\nReference line (slope/intercept)\n\n\ngeom_hline()\nHorizontal reference line\n\n\ngeom_vline()\nVertical reference line\n\n\n\n\n\n\n\n# Basic scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(\n    title = \"Fuel Efficiency vs. Weight\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\"\n  )\n\nWith color mapping:\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Fuel Efficiency by Weight and Cylinders\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\"\n  )\n\n\n\n\n\n# Create time series data\ntime_data &lt;- data.frame(\n  month = 1:12,\n  sales = c(100, 120, 115, 130, 145, 160, 155, 170, 180, 175, 190, 210)\n)\n\nggplot(time_data, aes(x = month, y = sales)) +\n  geom_line(color = \"steelblue\", size = 1) +\n  geom_point(color = \"steelblue\", size = 2) +\n  labs(\n    title = \"Monthly Sales Trend\",\n    x = \"Month\",\n    y = \"Sales\"\n  )\n\n\n\n\n\ngeom_bar() - counts observations (stat = “count”)\nggplot(mtcars, aes(x = factor(cyl))) +\n  geom_bar(fill = \"steelblue\") +\n  labs(\n    title = \"Count of Cars by Cylinder\",\n    x = \"Cylinders\",\n    y = \"Count\"\n  )\n\ngeom_col() - uses values directly (stat = “identity”)\nggplot(df, aes(x = Movie_Title, y = Total_Worldwide_Gross)) +\n  geom_col(fill = \"steelblue\") +\n  labs(\n    title = \"Worldwide Gross by Movie\",\n    x = \"Movie\",\n    y = \"Gross ($)\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nFlip coordinates for horizontal bars:\nggplot(df, aes(x = Movie_Title, y = Total_Worldwide_Gross)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Worldwide Gross by Movie\",\n    x = \"Movie\",\n    y = \"Gross ($)\"\n  )\n\n\n\n\n\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 2, fill = \"steelblue\", color = \"white\") +\n  labs(\n    title = \"Distribution of MPG\",\n    x = \"Miles Per Gallon\",\n    y = \"Count\"\n  )\n\n\n\n\n\nBox plots show distribution and outliers: - Box: Interquartile range (IQR) - middle 50% of data - Line in box: Median - Whiskers: Extend to 1.5 × IQR - Points: Outliers beyond whiskers\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_boxplot() +\n  labs(\n    title = \"MPG Distribution by Cylinders\",\n    x = \"Cylinders\",\n    y = \"Miles Per Gallon\"\n  ) +\n  theme(legend.position = \"none\")\n\nAdd jittered points to show actual data:\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"MPG Distribution by Cylinders\",\n    x = \"Cylinders\",\n    y = \"Miles Per Gallon\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nUsing the mtcars dataset:\n\nCreate a scatter plot of hp (horsepower) vs mpg (miles per gallon).\nCreate a bar chart showing the count of cars by number of gears (gear).\nCreate a histogram of hp with 10 bins.\nCreate a boxplot of mpg grouped by gear.\n\n### Your workspace\n\n\n\n\n\n\n\n\nA critical distinction! There’s an important difference between mapping a variable to an aesthetic and setting an aesthetic to a fixed value.\nMapping (inside aes()) - connects a variable to a visual property:\n# Color varies BY the data (each cylinder group gets a different color)\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 4)\n\nSetting (outside aes()) - applies a fixed value to all points:\n# ALL points are purple\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point(size = 4, color = \"purple\")\n\nCommon Error: What happens if you put a fixed value inside aes()?\n# WRONG! This creates a weird legend\nggplot(data = mtcars, aes(x = wt, y = mpg, color = \"purple\")) +\n  geom_point(size = 4)\n\nR thinks “purple” is a variable name, recycles it for every row, and picks its default color (salmon) — not what you wanted!\n\n\n\n\n\n\n\nAesthetic\nDescription\nUsed With\n\n\n\n\ncolor\nOutline/line color\nPoints, lines, text\n\n\nfill\nInterior color\nBars, boxes, areas\n\n\nalpha\nTransparency (0-1)\nAll geoms\n\n\n\n# color vs fill\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot(color = \"darkblue\", fill = \"lightblue\", alpha = 0.7)\n\n\n\n\n\nPoint Shapes:\n\n\n\nShape Integer\nShape Name\nVisualization\n\n\n\n\n0\nsquare open\n□\n\n\n1\ncircle open\n○\n\n\n2\ntriangle open\n△\n\n\n15\nsquare filled\n■\n\n\n16\ncircle filled\n●\n\n\n17\ntriangle filled\n▲\n\n\n18\ndiamond filled\n◆\n\n\n\n# Multiple aesthetics\nggplot(mtcars, aes(x = wt, y = mpg, \n                   color = factor(cyl), \n                   size = hp,\n                   shape = factor(gear))) +\n  geom_point(alpha = 0.7) +\n  labs(\n    color = \"Cylinders\",\n    size = \"Horsepower\",\n    shape = \"Gears\"\n  )\n\nLine Types:\n# Different line types\ndf_lines &lt;- data.frame(\n  x = rep(1:5, 3),\n  y = c(1:5, 2:6, 3:7),\n  group = rep(c(\"A\", \"B\", \"C\"), each = 5)\n)\n\nggplot(df_lines, aes(x = x, y = y, linetype = group, color = group)) +\n  geom_line(size = 1) +\n  labs(title = \"Different Line Types\")\n\n\n\n\n\n\n\n\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Fuel Efficiency by Vehicle Weight\",\n    subtitle = \"Data from mtcars dataset\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\",\n    caption = \"Source: mtcars dataset in R\"\n  )\n\n\n\n\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_text(aes(label = rownames(mtcars)), size = 2, vjust = -0.5) +\n  labs(title = \"MPG vs Weight with Car Names\")\n\nUse geom_label() for boxed labels or annotate() for single annotations:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  annotate(\"text\", x = 5, y = 30, label = \"High efficiency, heavy cars\", color = \"red\") +\n  annotate(\"rect\", xmin = 4.5, xmax = 5.5, ymin = 28, ymax = 35, alpha = 0.2, fill = \"red\")\n\n\n\n\n\n\n\n\nUse scale_x_continuous() and scale_y_continuous() to customize numeric axes.\nggplot(df, aes(x = China_Box_Office_Gross, y = US_Box_Office_Gross)) +\n  geom_point(size = 4, color = \"steelblue\") +\n  scale_x_continuous(\n    labels = scales::comma,\n    breaks = seq(300000000, 600000000, by = 50000000)\n  ) +\n  scale_y_continuous(\n    labels = scales::dollar,\n    limits = c(0, 700000000)\n  ) +\n  labs(\n    title = \"Box Office Performance\",\n    x = \"China Gross\",\n    y = \"US Gross\"\n  )\n\n\n\n\n\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_boxplot() +\n  scale_x_discrete(labels = c(\"4\" = \"Four\", \"6\" = \"Six\", \"8\" = \"Eight\")) +\n  scale_fill_discrete(name = \"Cylinders\") +\n  labs(x = \"Number of Cylinders\")\n\n\n\n\n\nWhen working with dates, use scale_x_date() to format axis labels.\n# Create date data\ndf_dates &lt;- df |&gt; \n  mutate(Release_Date_Parsed = dmy(paste(\"01\", Release_Date)))\n\nggplot(df_dates, aes(x = Release_Date_Parsed, y = Total_Worldwide_Gross)) +\n  geom_line(color = \"steelblue\", size = 1) +\n  geom_point(color = \"steelblue\", size = 3) +\n  scale_x_date(\n    date_breaks = \"1 month\",\n    date_labels = \"%b %Y\"\n  ) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(\n    title = \"Worldwide Gross Over Time\",\n    x = \"Release Date\",\n    y = \"Total Worldwide Gross\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nFunction\nPurpose\n\n\n\n\nscale_color_manual()\nSet colors manually\n\n\nscale_fill_manual()\nSet fill colors manually\n\n\nscale_color_brewer()\nUse ColorBrewer palettes\n\n\nscale_color_viridis_d()\nViridis discrete palette\n\n\nscale_color_viridis_c()\nViridis continuous palette\n\n\nscale_color_gradient()\nContinuous gradient\n\n\nscale_color_gradient2()\nDiverging gradient\n\n\n\n# Manual colors\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  scale_color_manual(values = c(\"red\", \"green\", \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\nTheme\nDescription\n\n\n\n\ntheme_minimal()\nClean, minimal design\n\n\ntheme_classic()\nClassic with axes, no gridlines\n\n\ntheme_light()\nLight background, subtle gridlines\n\n\ntheme_dark()\nDark background\n\n\ntheme_bw()\nBlack and white, good for printing\n\n\ntheme_void()\nEmpty, no axes or background\n\n\n\np &lt;- ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Theme Comparison\")\n\n# Compare themes\np + theme_minimal() + labs(subtitle = \"theme_minimal()\")\n\np + theme_classic() + labs(subtitle = \"theme_classic()\")\n\np + theme_bw() + labs(subtitle = \"theme_bw()\")\n\n\n\n\n\nUse theme() to customize individual elements:\n\n\n\nElement\nDescription\n\n\n\n\nplot.title\nMain title appearance\n\n\nplot.subtitle\nSubtitle appearance\n\n\naxis.title\nAxis label appearance\n\n\naxis.text\nAxis tick label appearance\n\n\nlegend.position\nLegend location\n\n\npanel.grid\nGridline appearance\n\n\npanel.background\nPlot area background\n\n\n\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 4) +\n  labs(\n    title = \"Fuel Efficiency by Vehicle Weight\",\n    subtitle = \"Custom themed plot\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\"\n  ) +\n  theme_light() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5, color = \"darkblue\"),\n    plot.subtitle = element_text(face = \"italic\", size = 12, hjust = 0.5),\n    axis.title = element_text(face = \"bold\", size = 12),\n    axis.text = element_text(size = 10),\n    legend.position = \"top\",\n    panel.grid.major = element_line(color = \"gray90\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\nThe ggthemes package provides additional themes:\ninstall.packages(\"ggthemes\")\nlibrary(ggthemes)\n\n# Economist theme\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Economist Theme\") +\n  theme_economist() +\n  scale_color_economist()\n\n# Wall Street Journal theme\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Wall Street Journal Theme\") +\n  theme_wsj()\n\n# Tufte theme (minimalist)\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Tufte Theme\") +\n  theme_tufte()\n\n\n\n\n\n\nFacets split data into multiple panels by a variable.\n\n\nUse facet_wrap() for a single grouping variable:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\", size = 2) +\n  facet_wrap(~ cyl) +\n  labs(title = \"MPG vs Weight by Cylinders\")\n\nControl layout with ncol and nrow:\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  facet_wrap(~ cyl, ncol = 3) +\n  labs(title = \"Faceted by Cylinders (3 columns)\") +\n  theme(legend.position = \"none\")\n\nFree scales:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\") +\n  facet_wrap(~ gear, scales = \"free\") +\n  labs(title = \"Faceted with Free Scales\")\n\n\n\n\n\nUse facet_grid() for two grouping variables:\n# Rows = cyl, Columns = gear\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  facet_grid(cyl ~ gear) +\n  labs(title = \"Facet Grid: Cylinders × Gears\") +\n  theme(legend.position = \"none\")\n\nFacet by rows only:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\") +\n  facet_grid(cyl ~ .) +\n  labs(title = \"Facet Grid: Rows by Cylinders\")\n\nFacet by columns only:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\") +\n  facet_grid(. ~ gear) +\n  labs(title = \"Facet Grid: Columns by Gears\")\n\n\n\n\n\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  facet_wrap(\n    ~ cyl,\n    labeller = labeller(cyl = c(\"4\" = \"4 Cylinders\", \"6\" = \"6 Cylinders\", \"8\" = \"8 Cylinders\"))\n  ) +\n  labs(title = \"Custom Facet Labels\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"tomato\", \"steelblue\", \"forestgreen\")) +\n  labs(title = \"Manual Color Selection\")\n\n\n\n\n\nColorBrewer provides color palettes designed for data visualization:\nlibrary(RColorBrewer)\ndisplay.brewer.all()\n\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"ColorBrewer: Set2\")\n\n\n\n\n\nViridis palettes are colorblind-friendly and print well in grayscale:\nggplot(mtcars, aes(x = wt, y = mpg, color = hp)) +\n  geom_point(size = 4) +\n  scale_color_viridis_c() +\n  labs(title = \"Viridis Continuous Scale\")\n\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_viridis_d() +\n  labs(title = \"Viridis Discrete Scale\")\n\n\n\n\n\nwesanderson:\ninstall.packages(\"wesanderson\")\nlibrary(wesanderson)\n\n# See available palettes\nnames(wes_palettes)\n##  [1] \"BottleRocket1\"     \"BottleRocket2\"     \"Rushmore1\"        \n##  [4] \"Rushmore\"          \"Royal1\"            \"Royal2\"           \n##  [7] \"Zissou1\"           \"Zissou1Continuous\" \"Darjeeling1\"      \n## [10] \"Darjeeling2\"       \"Chevalier1\"        \"FantasticFox1\"    \n## [13] \"Moonrise1\"         \"Moonrise2\"         \"Moonrise3\"        \n## [16] \"Cavalcanti1\"       \"GrandBudapest1\"    \"GrandBudapest2\"   \n## [19] \"IsleofDogs1\"       \"IsleofDogs2\"       \"FrenchDispatch\"   \n## [22] \"AsteroidCity1\"     \"AsteroidCity2\"     \"AsteroidCity3\"\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_manual(values = wes_palette(\"GrandBudapest1\", n = 3)) +\n  labs(title = \"Wes Anderson: Grand Budapest\")\n\nNatParksPalettes:\ninstall.packages(\"NatParksPalettes\")\nlibrary(NatParksPalettes)\n\n# See available palettes\nnames(NatParksPalettes)\n##  [1] \"Acadia\"      \"Arches\"      \"Arches2\"     \"Banff\"       \"BryceCanyon\"\n##  [6] \"CapitolReef\" \"Charmonix\"   \"CraterLake\"  \"Cuyahoga\"    \"DeathValley\"\n## [11] \"Denali\"      \"Everglades\"  \"Glacier\"     \"GrandCanyon\" \"Halekala\"   \n## [16] \"IguazuFalls\" \"KingsCanyon\" \"LakeNakuru\"  \"Olympic\"     \"Redwood\"    \n## [21] \"RockyMtn\"    \"Saguaro\"     \"SmokyMtns\"   \"SouthDowns\"  \"Torres\"     \n## [26] \"Triglav\"     \"WindCave\"    \"Volcanoes\"   \"Yellowstone\" \"Yosemite\"\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_manual(values = natparks.pals(\"Yellowstone\", n = 3)) +\n  labs(title = \"National Parks: Yellowstone\")\n\n\n\n\nUsing the iris dataset, create a publication-ready visualization:\n\nCreate a scatter plot of Sepal.Length vs Sepal.Width, colored by Species.\nAdd appropriate labels (title, subtitle, axis labels, caption).\nUse a custom color palette (try wesanderson or ColorBrewer).\nApply a theme (try theme_minimal() or theme_classic()).\nCustomize the theme with:\n\nBold, centered title\nLegend at the bottom\nCustom axis text size\n\nCreate a faceted version by Species.\n\n# Starter code\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(size = 3) +\n  # Add your customizations here\n  labs(\n    title = \"...\",\n    subtitle = \"...\",\n    x = \"...\",\n    y = \"...\",\n    caption = \"...\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic\nKey Points\n\n\n\n\nggplot Structure\nggplot(data, aes()) + geom_*() + ... - build plots layer by layer\n\n\nCommon Geoms\ngeom_point(): scatter; geom_line(): lines; geom_bar()/geom_col(): bars; geom_histogram(): distribution; geom_boxplot(): box plots\n\n\nMapping vs Setting\nInside aes(): map variable to aesthetic; Outside aes(): set fixed value\n\n\nAesthetics\ncolor: outlines/lines; fill: interior; alpha: transparency; size: point/line size; shape: point shape; linetype: line style\n\n\nLabels\nlabs(title, subtitle, x, y, color, fill, caption)\n\n\nScales\nscale_x_continuous(), scale_y_continuous(), scale_x_date(), scale_color_manual(), scale_fill_brewer()\n\n\nThemes\ntheme_minimal(), theme_classic(), theme_bw(), theme_void(); Customize with theme()\n\n\nFacets\nfacet_wrap(~ var): single variable; facet_grid(row ~ col): two variables\n\n\nColor Palettes\nManual: scale_*_manual(values = c(...)); Brewer: scale_*_brewer(palette = \"...\"); Viridis: scale_*_viridis_d()\n\n\nggthemes\ntheme_economist(), theme_wsj(), theme_tufte() for publication-ready styles"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#lecture-3-table-of-contents",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#lecture-3-table-of-contents",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "Section\nTopic\n\n\n\n\n1\nIntroduction to ggplot2\n\n\n1.1\nThe Grammar of Graphics\n\n\n1.2\nBasic ggplot Structure\n\n\n2\nGeoms: Types of Graphs\n\n\n2.1\nCommon Geoms Overview\n\n\n2.2\nScatter Plots (geom_point())\n\n\n2.3\nLine Plots (geom_line())\n\n\n2.4\nBar Charts (geom_bar(), geom_col())\n\n\n2.5\nHistograms (geom_histogram())\n\n\n2.6\nBox Plots (geom_boxplot())\n\n\n3\nAesthetics\n\n\n3.1\nMapping vs. Setting Aesthetics\n\n\n3.2\nColor, Fill, and Alpha\n\n\n3.3\nSize, Shape, and Linetype\n\n\n4\nLabels and Annotations\n\n\n4.1\nAdding Labels with labs()\n\n\n4.2\nAdding Text and Annotations\n\n\n5\nScales and Axes\n\n\n5.1\nCustomizing Continuous Scales\n\n\n5.2\nCustomizing Discrete Scales\n\n\n5.3\nDate Scales\n\n\n5.4\nColor Scales\n\n\n6\nThemes\n\n\n6.1\nBuilt-in Themes\n\n\n6.2\nCustomizing Theme Elements\n\n\n6.3\nExternal Themes (ggthemes)\n\n\n7\nFacets\n\n\n7.1\nfacet_wrap()\n\n\n7.2\nfacet_grid()\n\n\n7.3\nCustomizing Facet Labels\n\n\n8\nColor Palettes\n\n\n8.1\nManual Colors\n\n\n8.2\nColorBrewer\n\n\n8.3\nViridis\n\n\n8.4\nFun Palettes (wesanderson, NatParksPalettes)"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#r-exercises",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#r-exercises",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "ALWAYS load our libraries first\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(lubridate)"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#introduction-to-ggplot2",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#introduction-to-ggplot2",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "ggplot2 is a powerful visualization package that follows the Grammar of Graphics - a systematic approach to building plots layer by layer.\nCore Components:\n\n\n\n\n\n\n\n\nComponent\nDescription\nExample\n\n\n\n\nData\nThe dataset you’re plotting\ndata = mtcars\n\n\nAesthetics\nMappings between data and visual properties\naes(x = wt, y = mpg)\n\n\nGeometries\nThe type of plot\ngeom_point(), geom_bar()\n\n\nScales\nControl how data maps to visual properties\nscale_x_continuous()\n\n\nFacets\nSplit data into panels\nfacet_wrap(~ cyl)\n\n\nThemes\nControl non-data elements\ntheme_minimal()\n\n\n\nGGPlot Cheat Sheet\n\n\n\n\nEvery ggplot follows this basic structure:\nggplot(data = &lt;DATA&gt;, aes(&lt;MAPPINGS&gt;)) +\n  &lt;GEOM_FUNCTION&gt;() +\n  &lt;OPTIONAL_LAYERS&gt;\nCreate a sample dataset:\ndf &lt;- data.frame(\n  Movie_Title = c(\n    \"YOLO\", \"Successor\", \"Pegasus 2\", \"Deadpool & Wolverine\", \"Moana 2\", \n    \"The Hidden Blade\", \"Avatar: The Spirit Returns\"\n  ),\n  Release_Date = c(\n    \"January 2024\", \"February 2024\", \"March 2024\", \"July 2024\", \n    \"November 2024\", \"April 2024\", \"December 2024\"\n  ),\n  China_Box_Office_Gross = c(\n    479597304, 469612890, 466930272, 450000000, 350000000, 320000000, 550000000\n  ),\n  US_Box_Office_Gross = c(\n    310000000, 280000000, 290000000, 438000000, 221000000, 75000000, 600000000\n  ),\n  Total_Worldwide_Gross = c(\n    479597304, 469612890, 466930272, 850000000, 421000000, 400000000, 1300000000\n  )\n)\nBasic scatter plot:\nggplot(data = df, aes(x = China_Box_Office_Gross, y = US_Box_Office_Gross)) +\n  geom_point()"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#geoms-types-of-graphs",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#geoms-types-of-graphs",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "Geom Function\nDescription\nKey Aesthetics\n\n\n\n\ngeom_point()\nScatter plot\nx, y, color, size, shape\n\n\ngeom_line()\nLine plot\nx, y, color, linetype\n\n\ngeom_bar()\nBar chart (counts)\nx, fill\n\n\ngeom_col()\nBar chart (values)\nx, y, fill\n\n\ngeom_histogram()\nHistogram\nx, fill, bins\n\n\ngeom_boxplot()\nBox plot\nx, y, fill\n\n\ngeom_smooth()\nTrend line\nx, y, method\n\n\ngeom_text()\nAdd text\nx, y, label\n\n\n\nAdditional Geoms:\n\n\n\nGeom Function\nDescription\n\n\n\n\ngeom_area()\nArea under a line\n\n\ngeom_violin()\nViolin plot\n\n\ngeom_density()\nDensity curve\n\n\ngeom_tile()\nHeatmap tiles\n\n\ngeom_segment()\nLine segments\n\n\ngeom_abline()\nReference line (slope/intercept)\n\n\ngeom_hline()\nHorizontal reference line\n\n\ngeom_vline()\nVertical reference line\n\n\n\n\n\n\n\n# Basic scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(\n    title = \"Fuel Efficiency vs. Weight\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\"\n  )\n\nWith color mapping:\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Fuel Efficiency by Weight and Cylinders\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\"\n  )\n\n\n\n\n\n# Create time series data\ntime_data &lt;- data.frame(\n  month = 1:12,\n  sales = c(100, 120, 115, 130, 145, 160, 155, 170, 180, 175, 190, 210)\n)\n\nggplot(time_data, aes(x = month, y = sales)) +\n  geom_line(color = \"steelblue\", size = 1) +\n  geom_point(color = \"steelblue\", size = 2) +\n  labs(\n    title = \"Monthly Sales Trend\",\n    x = \"Month\",\n    y = \"Sales\"\n  )\n\n\n\n\n\ngeom_bar() - counts observations (stat = “count”)\nggplot(mtcars, aes(x = factor(cyl))) +\n  geom_bar(fill = \"steelblue\") +\n  labs(\n    title = \"Count of Cars by Cylinder\",\n    x = \"Cylinders\",\n    y = \"Count\"\n  )\n\ngeom_col() - uses values directly (stat = “identity”)\nggplot(df, aes(x = Movie_Title, y = Total_Worldwide_Gross)) +\n  geom_col(fill = \"steelblue\") +\n  labs(\n    title = \"Worldwide Gross by Movie\",\n    x = \"Movie\",\n    y = \"Gross ($)\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nFlip coordinates for horizontal bars:\nggplot(df, aes(x = Movie_Title, y = Total_Worldwide_Gross)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Worldwide Gross by Movie\",\n    x = \"Movie\",\n    y = \"Gross ($)\"\n  )\n\n\n\n\n\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 2, fill = \"steelblue\", color = \"white\") +\n  labs(\n    title = \"Distribution of MPG\",\n    x = \"Miles Per Gallon\",\n    y = \"Count\"\n  )\n\n\n\n\n\nBox plots show distribution and outliers: - Box: Interquartile range (IQR) - middle 50% of data - Line in box: Median - Whiskers: Extend to 1.5 × IQR - Points: Outliers beyond whiskers\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_boxplot() +\n  labs(\n    title = \"MPG Distribution by Cylinders\",\n    x = \"Cylinders\",\n    y = \"Miles Per Gallon\"\n  ) +\n  theme(legend.position = \"none\")\n\nAdd jittered points to show actual data:\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"MPG Distribution by Cylinders\",\n    x = \"Cylinders\",\n    y = \"Miles Per Gallon\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nUsing the mtcars dataset:\n\nCreate a scatter plot of hp (horsepower) vs mpg (miles per gallon).\nCreate a bar chart showing the count of cars by number of gears (gear).\nCreate a histogram of hp with 10 bins.\nCreate a boxplot of mpg grouped by gear.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#aesthetics",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#aesthetics",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "A critical distinction! There’s an important difference between mapping a variable to an aesthetic and setting an aesthetic to a fixed value.\nMapping (inside aes()) - connects a variable to a visual property:\n# Color varies BY the data (each cylinder group gets a different color)\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 4)\n\nSetting (outside aes()) - applies a fixed value to all points:\n# ALL points are purple\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point(size = 4, color = \"purple\")\n\nCommon Error: What happens if you put a fixed value inside aes()?\n# WRONG! This creates a weird legend\nggplot(data = mtcars, aes(x = wt, y = mpg, color = \"purple\")) +\n  geom_point(size = 4)\n\nR thinks “purple” is a variable name, recycles it for every row, and picks its default color (salmon) — not what you wanted!\n\n\n\n\n\n\n\nAesthetic\nDescription\nUsed With\n\n\n\n\ncolor\nOutline/line color\nPoints, lines, text\n\n\nfill\nInterior color\nBars, boxes, areas\n\n\nalpha\nTransparency (0-1)\nAll geoms\n\n\n\n# color vs fill\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot(color = \"darkblue\", fill = \"lightblue\", alpha = 0.7)\n\n\n\n\n\nPoint Shapes:\n\n\n\nShape Integer\nShape Name\nVisualization\n\n\n\n\n0\nsquare open\n□\n\n\n1\ncircle open\n○\n\n\n2\ntriangle open\n△\n\n\n15\nsquare filled\n■\n\n\n16\ncircle filled\n●\n\n\n17\ntriangle filled\n▲\n\n\n18\ndiamond filled\n◆\n\n\n\n# Multiple aesthetics\nggplot(mtcars, aes(x = wt, y = mpg, \n                   color = factor(cyl), \n                   size = hp,\n                   shape = factor(gear))) +\n  geom_point(alpha = 0.7) +\n  labs(\n    color = \"Cylinders\",\n    size = \"Horsepower\",\n    shape = \"Gears\"\n  )\n\nLine Types:\n# Different line types\ndf_lines &lt;- data.frame(\n  x = rep(1:5, 3),\n  y = c(1:5, 2:6, 3:7),\n  group = rep(c(\"A\", \"B\", \"C\"), each = 5)\n)\n\nggplot(df_lines, aes(x = x, y = y, linetype = group, color = group)) +\n  geom_line(size = 1) +\n  labs(title = \"Different Line Types\")"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#labels-and-annotations",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#labels-and-annotations",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Fuel Efficiency by Vehicle Weight\",\n    subtitle = \"Data from mtcars dataset\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\",\n    caption = \"Source: mtcars dataset in R\"\n  )\n\n\n\n\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_text(aes(label = rownames(mtcars)), size = 2, vjust = -0.5) +\n  labs(title = \"MPG vs Weight with Car Names\")\n\nUse geom_label() for boxed labels or annotate() for single annotations:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  annotate(\"text\", x = 5, y = 30, label = \"High efficiency, heavy cars\", color = \"red\") +\n  annotate(\"rect\", xmin = 4.5, xmax = 5.5, ymin = 28, ymax = 35, alpha = 0.2, fill = \"red\")"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#scales-and-axes",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#scales-and-axes",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "Use scale_x_continuous() and scale_y_continuous() to customize numeric axes.\nggplot(df, aes(x = China_Box_Office_Gross, y = US_Box_Office_Gross)) +\n  geom_point(size = 4, color = \"steelblue\") +\n  scale_x_continuous(\n    labels = scales::comma,\n    breaks = seq(300000000, 600000000, by = 50000000)\n  ) +\n  scale_y_continuous(\n    labels = scales::dollar,\n    limits = c(0, 700000000)\n  ) +\n  labs(\n    title = \"Box Office Performance\",\n    x = \"China Gross\",\n    y = \"US Gross\"\n  )\n\n\n\n\n\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_boxplot() +\n  scale_x_discrete(labels = c(\"4\" = \"Four\", \"6\" = \"Six\", \"8\" = \"Eight\")) +\n  scale_fill_discrete(name = \"Cylinders\") +\n  labs(x = \"Number of Cylinders\")\n\n\n\n\n\nWhen working with dates, use scale_x_date() to format axis labels.\n# Create date data\ndf_dates &lt;- df |&gt; \n  mutate(Release_Date_Parsed = dmy(paste(\"01\", Release_Date)))\n\nggplot(df_dates, aes(x = Release_Date_Parsed, y = Total_Worldwide_Gross)) +\n  geom_line(color = \"steelblue\", size = 1) +\n  geom_point(color = \"steelblue\", size = 3) +\n  scale_x_date(\n    date_breaks = \"1 month\",\n    date_labels = \"%b %Y\"\n  ) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(\n    title = \"Worldwide Gross Over Time\",\n    x = \"Release Date\",\n    y = \"Total Worldwide Gross\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nFunction\nPurpose\n\n\n\n\nscale_color_manual()\nSet colors manually\n\n\nscale_fill_manual()\nSet fill colors manually\n\n\nscale_color_brewer()\nUse ColorBrewer palettes\n\n\nscale_color_viridis_d()\nViridis discrete palette\n\n\nscale_color_viridis_c()\nViridis continuous palette\n\n\nscale_color_gradient()\nContinuous gradient\n\n\nscale_color_gradient2()\nDiverging gradient\n\n\n\n# Manual colors\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  scale_color_manual(values = c(\"red\", \"green\", \"blue\"))"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#themes",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#themes",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "Theme\nDescription\n\n\n\n\ntheme_minimal()\nClean, minimal design\n\n\ntheme_classic()\nClassic with axes, no gridlines\n\n\ntheme_light()\nLight background, subtle gridlines\n\n\ntheme_dark()\nDark background\n\n\ntheme_bw()\nBlack and white, good for printing\n\n\ntheme_void()\nEmpty, no axes or background\n\n\n\np &lt;- ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Theme Comparison\")\n\n# Compare themes\np + theme_minimal() + labs(subtitle = \"theme_minimal()\")\n\np + theme_classic() + labs(subtitle = \"theme_classic()\")\n\np + theme_bw() + labs(subtitle = \"theme_bw()\")\n\n\n\n\n\nUse theme() to customize individual elements:\n\n\n\nElement\nDescription\n\n\n\n\nplot.title\nMain title appearance\n\n\nplot.subtitle\nSubtitle appearance\n\n\naxis.title\nAxis label appearance\n\n\naxis.text\nAxis tick label appearance\n\n\nlegend.position\nLegend location\n\n\npanel.grid\nGridline appearance\n\n\npanel.background\nPlot area background\n\n\n\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 4) +\n  labs(\n    title = \"Fuel Efficiency by Vehicle Weight\",\n    subtitle = \"Custom themed plot\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\"\n  ) +\n  theme_light() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5, color = \"darkblue\"),\n    plot.subtitle = element_text(face = \"italic\", size = 12, hjust = 0.5),\n    axis.title = element_text(face = \"bold\", size = 12),\n    axis.text = element_text(size = 10),\n    legend.position = \"top\",\n    panel.grid.major = element_line(color = \"gray90\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\nThe ggthemes package provides additional themes:\ninstall.packages(\"ggthemes\")\nlibrary(ggthemes)\n\n# Economist theme\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Economist Theme\") +\n  theme_economist() +\n  scale_color_economist()\n\n# Wall Street Journal theme\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Wall Street Journal Theme\") +\n  theme_wsj()\n\n# Tufte theme (minimalist)\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Tufte Theme\") +\n  theme_tufte()"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#facets",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#facets",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "Facets split data into multiple panels by a variable.\n\n\nUse facet_wrap() for a single grouping variable:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\", size = 2) +\n  facet_wrap(~ cyl) +\n  labs(title = \"MPG vs Weight by Cylinders\")\n\nControl layout with ncol and nrow:\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  facet_wrap(~ cyl, ncol = 3) +\n  labs(title = \"Faceted by Cylinders (3 columns)\") +\n  theme(legend.position = \"none\")\n\nFree scales:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\") +\n  facet_wrap(~ gear, scales = \"free\") +\n  labs(title = \"Faceted with Free Scales\")\n\n\n\n\n\nUse facet_grid() for two grouping variables:\n# Rows = cyl, Columns = gear\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  facet_grid(cyl ~ gear) +\n  labs(title = \"Facet Grid: Cylinders × Gears\") +\n  theme(legend.position = \"none\")\n\nFacet by rows only:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\") +\n  facet_grid(cyl ~ .) +\n  labs(title = \"Facet Grid: Rows by Cylinders\")\n\nFacet by columns only:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\") +\n  facet_grid(. ~ gear) +\n  labs(title = \"Facet Grid: Columns by Gears\")\n\n\n\n\n\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  facet_wrap(\n    ~ cyl,\n    labeller = labeller(cyl = c(\"4\" = \"4 Cylinders\", \"6\" = \"6 Cylinders\", \"8\" = \"8 Cylinders\"))\n  ) +\n  labs(title = \"Custom Facet Labels\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#color-palettes",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#color-palettes",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "ggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"tomato\", \"steelblue\", \"forestgreen\")) +\n  labs(title = \"Manual Color Selection\")\n\n\n\n\n\nColorBrewer provides color palettes designed for data visualization:\nlibrary(RColorBrewer)\ndisplay.brewer.all()\n\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"ColorBrewer: Set2\")\n\n\n\n\n\nViridis palettes are colorblind-friendly and print well in grayscale:\nggplot(mtcars, aes(x = wt, y = mpg, color = hp)) +\n  geom_point(size = 4) +\n  scale_color_viridis_c() +\n  labs(title = \"Viridis Continuous Scale\")\n\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_viridis_d() +\n  labs(title = \"Viridis Discrete Scale\")\n\n\n\n\n\nwesanderson:\ninstall.packages(\"wesanderson\")\nlibrary(wesanderson)\n\n# See available palettes\nnames(wes_palettes)\n##  [1] \"BottleRocket1\"     \"BottleRocket2\"     \"Rushmore1\"        \n##  [4] \"Rushmore\"          \"Royal1\"            \"Royal2\"           \n##  [7] \"Zissou1\"           \"Zissou1Continuous\" \"Darjeeling1\"      \n## [10] \"Darjeeling2\"       \"Chevalier1\"        \"FantasticFox1\"    \n## [13] \"Moonrise1\"         \"Moonrise2\"         \"Moonrise3\"        \n## [16] \"Cavalcanti1\"       \"GrandBudapest1\"    \"GrandBudapest2\"   \n## [19] \"IsleofDogs1\"       \"IsleofDogs2\"       \"FrenchDispatch\"   \n## [22] \"AsteroidCity1\"     \"AsteroidCity2\"     \"AsteroidCity3\"\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_manual(values = wes_palette(\"GrandBudapest1\", n = 3)) +\n  labs(title = \"Wes Anderson: Grand Budapest\")\n\nNatParksPalettes:\ninstall.packages(\"NatParksPalettes\")\nlibrary(NatParksPalettes)\n\n# See available palettes\nnames(NatParksPalettes)\n##  [1] \"Acadia\"      \"Arches\"      \"Arches2\"     \"Banff\"       \"BryceCanyon\"\n##  [6] \"CapitolReef\" \"Charmonix\"   \"CraterLake\"  \"Cuyahoga\"    \"DeathValley\"\n## [11] \"Denali\"      \"Everglades\"  \"Glacier\"     \"GrandCanyon\" \"Halekala\"   \n## [16] \"IguazuFalls\" \"KingsCanyon\" \"LakeNakuru\"  \"Olympic\"     \"Redwood\"    \n## [21] \"RockyMtn\"    \"Saguaro\"     \"SmokyMtns\"   \"SouthDowns\"  \"Torres\"     \n## [26] \"Triglav\"     \"WindCave\"    \"Volcanoes\"   \"Yellowstone\" \"Yosemite\"\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_manual(values = natparks.pals(\"Yellowstone\", n = 3)) +\n  labs(title = \"National Parks: Yellowstone\")\n\n\n\n\nUsing the iris dataset, create a publication-ready visualization:\n\nCreate a scatter plot of Sepal.Length vs Sepal.Width, colored by Species.\nAdd appropriate labels (title, subtitle, axis labels, caption).\nUse a custom color palette (try wesanderson or ColorBrewer).\nApply a theme (try theme_minimal() or theme_classic()).\nCustomize the theme with:\n\nBold, centered title\nLegend at the bottom\nCustom axis text size\n\nCreate a faceted version by Species.\n\n# Starter code\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(size = 3) +\n  # Add your customizations here\n  labs(\n    title = \"...\",\n    subtitle = \"...\",\n    x = \"...\",\n    y = \"...\",\n    caption = \"...\"\n  )"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#lecture-3-cheat-sheet",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#lecture-3-cheat-sheet",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "Topic\nKey Points\n\n\n\n\nggplot Structure\nggplot(data, aes()) + geom_*() + ... - build plots layer by layer\n\n\nCommon Geoms\ngeom_point(): scatter; geom_line(): lines; geom_bar()/geom_col(): bars; geom_histogram(): distribution; geom_boxplot(): box plots\n\n\nMapping vs Setting\nInside aes(): map variable to aesthetic; Outside aes(): set fixed value\n\n\nAesthetics\ncolor: outlines/lines; fill: interior; alpha: transparency; size: point/line size; shape: point shape; linetype: line style\n\n\nLabels\nlabs(title, subtitle, x, y, color, fill, caption)\n\n\nScales\nscale_x_continuous(), scale_y_continuous(), scale_x_date(), scale_color_manual(), scale_fill_brewer()\n\n\nThemes\ntheme_minimal(), theme_classic(), theme_bw(), theme_void(); Customize with theme()\n\n\nFacets\nfacet_wrap(~ var): single variable; facet_grid(row ~ col): two variables\n\n\nColor Palettes\nManual: scale_*_manual(values = c(...)); Brewer: scale_*_brewer(palette = \"...\"); Viridis: scale_*_viridis_d()\n\n\nggthemes\ntheme_economist(), theme_wsj(), theme_tufte() for publication-ready styles"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html",
    "title": "Why computational social science?",
    "section": "",
    "text": "Dr. Ayse D. Lokmanoglu Lecture 1, (B) Jan 21, 2026, (A) Jan 26, 2026"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#lecture-1-table-of-contents",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#lecture-1-table-of-contents",
    "title": "Why computational social science?",
    "section": "Lecture 1 Table of Contents",
    "text": "Lecture 1 Table of Contents\n\n\n\nSection\nTopic\n\n\n\n\n1\nIntro to R Studio\n\n\n1.1\nThe Console\n\n\n1.2\nThe Terminal\n\n\n1.3\nProjects\n\n\n1.4\nOrganize your project file\n\n\n1.5\nScripts\n\n\n2\nWorking directory and paths\n\n\n2.1\nCheck your working directory\n\n\n3\nR Markdown\n\n\n3.1\nHow to create a document in R Markdown\n\n\n3.2\nEmbed Code\n\n\n3.3\nGlobal Chunk Options\n\n\n3.4\nRunning vs knitting\n\n\n4\nR Packages\n\n\n4.1\nInstalling & Loading\n\n\n5\nR Basic Grammar\n\n\n5.1\nVariable Assignment\n\n\n5.2\nCase Sensitivity\n\n\n5.3\nComments\n\n\n5.4\nReserved Keywords\n\n\n6\nData Types\n\n\n6.1\nNumeric, Character, Logical, Factor\n\n\n6.2\nDates\n\n\n6.3\nLubridate Package\n\n\n7\nBasic Arithmetic\n\n\n8\nVectors\n\n\n8.1\nCreating Vectors\n\n\n8.2\nVector Indexing\n\n\n8.3\nVector Functions\n\n\n8.4\nSequences\n\n\n8.5\nCombining Vectors & Type Coercion\n\n\n8.6\nVector Arithmetic & Recycling\n\n\n9\nData Frames\n\n\n9.1\nCreating Data Frames\n\n\n9.2\nExploring/Inspecting Data Frames\n\n\n9.3\nAccessing Data\n\n\n9.4\nAdding and Removing Data\n\n\n9.5\nFiltering and Subsetting\n\n\n9.6\nOrdering Data\n\n\n9.7\nCommon Data Frame Functions\n\n\n9.8\nBasic Aggregations"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#intro-to-r-studio",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#intro-to-r-studio",
    "title": "Why computational social science?",
    "section": "1. Intro to R Studio",
    "text": "1. Intro to R Studio\nRStudio is an integrated development environment (IDE) for R, a programming language widely used for data analysis, statistical modeling, and visualization\n\n\n\n1.1 The Console\nThe Console is where you directly interact with R. You can type R commands, hit Enter, and see the immediate results.\nKey Features: - Executes code line-by-line. - Useful for quick calculations or testing small snippets of code. - Does not save commands—once you close RStudio, the history of commands in the Console is gone unless explicitly saved.\nTRY: write 2 + 2 in the console and press Enter.\n\n\n\n\n1.2 The Terminal\nThe Terminal is separate from the Console and provides access to your computer’s command-line interface (CLI).\nKey Features: - Allows you to run system-level commands (e.g., navigating file systems, managing files). - Useful for integrating with tools like Git or installing software packages.\nKey Difference from Console: - The Console is R-specific, while the Terminal is for general command-line tasks.\nWe will not use Terminal most of the time\nTRY: In the Terminal, type ls (Mac/Linux) or dir (Windows) to list files in your current directory.\n\n\n\n\n1.3 Project\nAn RStudio Project is a way to organize your work by grouping files, data, and scripts for a specific task or analysis.\nWhy Use Projects? - Keeps your workspace clean and focused. - Automatically sets the working directory to the project folder. - Ensures reproducibility by keeping everything needed for a project in one place.\nHow to Create a Project: 1. Click on File → New Project….. 2. Choose whether to create a new directory, use an existing directory, or clone a Git repository. 3. Give it a clear name (example: EMS747_Project) 4. Choose a location you can easily find again\nTRY: Create a project called “EMS747_Class_Exercises” and notice how RStudio creates a .Rproj file for managing it.\nWhen you create a project, RStudio makes a file ending in .Rproj. Always open your work by clicking that .Rproj file.\n\n\n\n\n1.4 Organize your project file\nInside your project folder, create these folders:\n\ndata/ for datasets (CSV, Excel, etc.)\n\nscripts/ for R scripts you write during the semester\n\nIn RStudio: use the Files pane → New Folder.\n\nYour project should look like this:\nEMS747_Project/\n  EMS747_Project.Rproj\n  data/\n  scripts/\n  bigdata_L1_github.Rmd\n\n\n\n1.5 Scripts\nA script is a text file where you write and save R code for future use. - Scripts let you document your work, making it reproducible and shareable. - You can save time by re-running pre-written code instead of typing commands repeatedly. How to Create and Use a Script: - Click on File → New File → R Script. - Write R code in the script editor. - E.g.\nx &lt;- 5\ny &lt;- 10\nz &lt;- x + y\nprint(z)\n## [1] 15\n\nHighlight the code you want to run and press Ctrl+Enter (Windows/Linux) or Cmd+Enter (Mac) to execute it in the Console."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#working-directory-and-paths",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#working-directory-and-paths",
    "title": "Why computational social science?",
    "section": "2. Working directory and paths",
    "text": "2. Working directory and paths\n\n2.1 Check your working directory\nWhen your project is open, run:\ngetwd()\n## [1] \"/Users/alokman/Library/CloudStorage/GoogleDrive-alokman@bu.edu/My Drive/Teaching/S26_EMS747/ClassSlides\"\nYou should see a path ending in your project folder name.\n\nWhy this matters\nIn the next lecture, we will load datasets from your data/ folder using relative paths like:\nread.csv(\"data/my_file.csv\")"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#r-markdown",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#r-markdown",
    "title": "Why computational social science?",
    "section": "3. R Markdown",
    "text": "3. R Markdown\n\nR Markdown is a framework that allows you to integrate code, output, and text in one document.\nYou can produce reports in multiple formats: HTML, PDF, Word, etc.\n\nInstead of copying results into a separate document, you generate the document directly from the code.\nWhy do we use R Markdown? - Combine code and text for reproducible research. - Create interactive and visually appealing documents. - Easy to share analyses with others.\nR Markdown Cheathseet\n\n\n3.1 How to create a document in R Markdown\n\nClick on File &gt; New File &gt; R Markdown.\nChoose a title, author, and output format (HTML, PDF, or Word).\nEdit the template provided in the new .Rmd file.\nFor our class you can use the Markdown from BB and take notes on that!\n\n\n\n\n3.2 Embed Code\nR code in R Markdown is written inside code chunks.\nYou can create a chunk by typing it manually or by using the Insert Code Chunk button in RStudio.\nCode chunks start with {r} and end with.\nYou can also add: - OPTIONAL: a chunk label (for identification) - chunk options (to control how code and output appear)\nExample:\n\nChunk labels are OPTIONAL and if you do put one it should be short and descriptive.\nChunk options are written after the label, separated by commas.\n\n\n\n\n\n3.3 Global Chunk Options\nYou can set options that apply to all code chunks in the document. This is usually done in the first chunk of the file.\nExample, this displays code in the output document, prevents messages (such as package startup messages) and warnings from appearing in the output document. :\nknitr::opts_chunk$set(\n  echo = TRUE,\n  message = FALSE, \n  warning = FALSE\n)\n\n\n\n\n\n\n\n\nOption\nDefault\nEffects\n\n\n\n\necho\nTRUE\nDisplay code in the output document\n\n\nerror\nFALSE\nTRUE = display error messages in the document; FALSE = stop rendering when an error occurs\n\n\neval\nTRUE\nRun code in the chunk\n\n\ninclude\nTRUE\nInclude the chunk output in the document after running\n\n\nmessage\nTRUE\nDisplay messages in the document\n\n\nwarning\nTRUE\nDisplay warnings in the document\n\n\nresults\n\"markup\"\n\"asis\" = pass results through as-is; \"hide\" = hide results; \"hold\" = show all results after all code\n\n\nfig.align\n\"default\"\nAlign figure: \"left\", \"right\", or \"center\"\n\n\nfig.alt\nNULL\nAlt text for a figure\n\n\nfig.cap\nNULL\nFigure caption (character string)\n\n\nfig.path\n\"figure/\"\nPrefix folder for generated figure file paths\n\n\nfig.width / fig.height\n7\nPlot dimensions in inches\n\n\nout.width\n(none)\nRescale output width (e.g., \"75%\", \"300px\")\n\n\ncollapse\nFALSE\nCollapse source and output into a single block\n\n\ncomment\n\"##\"\nPrefix for each line of results\n\n\nchild\nNULL\nFile(s) to knit and include in the document\n\n\npurl\nTRUE\nInclude/exclude chunk when extracting code with knitr::purl()\n\n\n\nSee more options and defaults by running str(knitr::opts_chunk$get()).\n\n\n\n3.4 Running vs knitting\n\nRun a chunk to execute code line by line\n\nKnit to rebuild the entire document from top to bottom.\nIn RMD empty space is for text.\nWhen you add {r, eval = FALSE} that code chunk will NOT run."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#r-packages",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#r-packages",
    "title": "Why computational social science?",
    "section": "4. R Packages",
    "text": "4. R Packages\n\nPackages are collections of R functions, data, and compiled code.\nExtend the functionality of base R.\nSimplifies complex tasks.\nWidely used for specialized analyses.\n\nE.g., tidy, dplyr, ggplot2\n\n\n4.1 Installing & Loading Packages\n\nInstallation\n\nUse the install.packages() function to install a package from CRAN.\n\n\ninstall.packages(\"tidyr\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"stringr\")\n\nLoading\n\nUse the library() function to load an installed package.\n\n\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(stringr)\n\nCheck the package documentation with ?\n\n?tidyr\n?dplyr\nN.B. Packages only need to be installed once but must be loaded in each session library(). Keep packages updated with update.packages()"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#r-basic-grammar",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#r-basic-grammar",
    "title": "Why computational social science?",
    "section": "5. R: Basic Grammar",
    "text": "5. R: Basic Grammar\n\n5.1 Variable Assignment\nVariables are used to store data or values.\n= (Simple Assignment) Similar to python\n&lt;- (Leftward Assignment) Most Common used by R coders and we will use this\n-&gt; (Rightward Assignment) Rarely used\nx = \"Simple_Assignment\" \nprint(x)\n## [1] \"Simple_Assignment\"\ny &lt;- \"Leftward Assignment\"\nprint(y)\n## [1] \"Leftward Assignment\"\n\"Rightward_Assignment\" -&gt; z\nprint(z)\n## [1] \"Rightward_Assignment\"\n\n\n\n5.2 R is case-sensitive\nx &lt;- 2\n\nprint(X)\n## Error: object 'X' not found\nIt gave an error, why?\nCause R is  case sensitive \nprint(x)\n## [1] 2\n\n\n\n5.3 Comments\nTo comment - Comments are notes in the code that R ignores. Use # to write comments. - R only has single line comments so if you want multiple lines you need to repeat the # for each line.\nvariable_2 &lt;- \"Leftward Assignment\" ## this is the most common used by R coders\n# Other's work as well \n\n\n\n5.4 R Reserved Keywords\nYou cannot use these keywords as variable names. These are reserved keywords for R.\n\n\n\n\n\n\n\nWords\nDescription\n\n\n\n\nif\nUsed for conditional execution of code blocks.\n\n\nelse\nSpecifies an alternative block of code to execute if the if condition is false.\n\n\nwhile\nExecutes a block of code repeatedly as long as a condition is true.\n\n\nrepeat\nCreates an infinite loop that must be terminated with a break statement.\n\n\nfor\nLoops through a sequence of elements.\n\n\nfunction\nDefines a function, a reusable block of code.\n\n\nin\nUsed in for loops to specify the sequence being iterated over.\n\n\nnext\nSkips the current iteration in a loop and moves to the next one.\n\n\nbreak\nExits a loop immediately.\n\n\nTRUE\nLogical constant representing a boolean value of true.\n\n\nFALSE\nLogical constant representing a boolean value of false.\n\n\nNULL\nRepresents the absence of a value or an undefined value.\n\n\nInf\nRepresents infinity (e.g., division by zero).\n\n\nNaN\nRepresents “Not a Number,” often resulting from undefined mathematical operations.\n\n\nNA\nRepresents missing data or “Not Available.”\n\n\nNA_integer_\nRepresents a missing integer value.\n\n\nNA_complex_\nRepresents a missing complex number.\n\n\nNA_real_\nRepresents a missing real (numeric) value."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#data-types",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#data-types",
    "title": "Why computational social science?",
    "section": "6. Data Types",
    "text": "6. Data Types\n\n6.1 Numeric, Character, Logical, Factor\n\nNumeric: Numbers\n\ne.g., 3.14, 42\n\n-Character: Text or strings\n\ne.g., \"Hello\", \"R\"\n\nLogical: Boolean values\n\nTRUE, FALSE\n\nFactor: Categorical data\n\ne.g., \"Male\", \"Female\"\n\nYou can use typeof() to see what type of data it is\n\nTRY: Writing different data types\nage &lt;- 25  # Numeric\ntypeof(age)\n## [1] \"double\"\nname &lt;- \"Alice\"  # Character\ntypeof(name)\n## [1] \"character\"\nis_student &lt;- TRUE  # Logical\ntypeof(is_student)\n## [1] \"logical\"\nQuestion? What is double?\n\nTRY: Create variables and see what types they are\n\n\n\n6.2 Dates\n\nDate: Represents calendar dates.\n\ne.g., “2023-01-01”\n\nPOSIXct/POSIXlt: Represents date and time.\n\ne.g., “2023-01-01 12:34:56”\n\nR uses the Date and POSIXct/POSIXlt classes for working with dates and times.\nUse as.Date() to convert strings to dates.\nUse Sys.Date() for the current date.\nUse Sys.time() for the current date and time.\n\na &lt;- \"2023-01-01\"\ntypeof(a)\n## [1] \"character\"\nb &lt;- as.Date(a)\ntypeof(b)\n## [1] \"double\"\nTRY: Converting strings to dates\n# Convert a string to a date\nmy_date &lt;- as.Date(\"2023-01-01\")\ntypeof(my_date)\n\n# Get the current date\ntoday &lt;- Sys.Date()\n\n# Add 7 days to a date\nfuture_date &lt;- today + 7\n\n# Display the date and class\nprint(future_date)\nclass(future_date)\n\n\n6.3 Lubridate Package\nLubridate is a package that makes working with dates easier.\n\nLubridate Cheat Sheet - It provides easy functions to parse, manipulate, and extract date-time components.\n\ninstall.packages(\"lubridate\") # only once\nlibrary(lubridate) # everytime you start R\nKey Functions:\n\nParsing Dates and Times:\n\nymd(), dmy(), mdy(): Convert strings to dates.\nymd_hms(), mdy_hms(): Handle date-time strings with hours, minutes, seconds.\n\nExtracting Components:\n\nyear(), month(), day(): Extract parts of a date.\nhour(), minute(), second(): Extract time components.\n\nManipulating Dates:\n\ntoday(), now(): Current date or date-time.\nArithmetic: Add or subtract days, months, etc.\n\nTime Zones:\n\nSet or change time zones with with_tz() or force_tz().\n\n\n\nTRY: Play with dates\nlibrary(lubridate)\n\n# Parse a date\nmy_date &lt;- ymd(\"2023-01-01\")\n\n# Parse a date-time\nmy_datetime &lt;- ymd_hms(\"2023-01-01 12:34:56\")\n\n# Extract components\nyear(my_date)    # 2023\n## [1] 2023\nmonth(my_date)   # 1\n## [1] 1\nday(my_date)     # 1\n## [1] 1\nhour(my_datetime) # 12\n## [1] 12\n# Add 7 days\nfuture_date &lt;- my_date + days(7)\nmy_date + months(6)\n## [1] \"2023-07-01\"\n# Set a time zone\nnew_timezone &lt;- with_tz(my_datetime, tzone = \"America/New_York\")\nprint(new_timezone)\n## [1] \"2023-01-01 07:34:56 EST\""
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#basic-arithmetic",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#basic-arithmetic",
    "title": "Why computational social science?",
    "section": "7. Basic Arithmetic",
    "text": "7. Basic Arithmetic\nOperators in R:\n\nAddition: +\nSubtraction: -\nMultiplication: *\nDivision: /\nExponentiation: ^\n\nTRY\na &lt;- 10\nb &lt;- 3\n\nsum &lt;- a + b  # Addition\nprint(sum)  \n## [1] 13\nproduct &lt;- a * b  # Multiplication\nprint(product)\n## [1] 30\npower &lt;- a ^ b  # Exponentiation\nprint(power)\n## [1] 1000\nTRY - Create variables and use basic arithmetic, I started for you play with them more!\nx&lt;-20\nx&lt;-5\nx\n## [1] 5\ny&lt;-10\nx*y\n## [1] 50"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#vectors",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#vectors",
    "title": "Why computational social science?",
    "section": "8. Vectors",
    "text": "8. Vectors\nA vector is a sequence of data elements of the same type.\n\n\n8.1 Creating Vectors\n\nCreating Vectors: Use the c() function.\n\nTRY\nnumbers &lt;- c(1, 2, 3, 4, 5)  # Numeric vector\nnames &lt;- c(\"Alice\", \"Bob\", \"Charlie\")  # Character vector\nis_student &lt;- c(TRUE, FALSE, TRUE)  # Logical vector\nTRY what types they are\ntypeof(numbers)\n## [1] \"double\"\nYou can access an element in a vector with square brackets []\nprint(numbers[2])  # Prints the second element of the vector\n## [1] 2\n\n\nVectors beyond numbers\n\nVectors can hold various types of data, including:\nCharacter: Strings of text.\nNumeric: Numbers.\nLogical: TRUE, FALSE.\n\nphrase &lt;- \"Vectors are fun!\"\nphrase\n## [1] \"Vectors are fun!\"\nfruits &lt;- c(\"Banana\", \"Mango\", \"Strawberry\", \"Grapes\")  # ADD YOUR FAVORITES!\nfruits\n## [1] \"Banana\"     \"Mango\"      \"Strawberry\" \"Grapes\"\n\n\n\n\n8.2 Vector functions\n\nAccess specific items with [] (we practices above)\nlength(): Finds the number of elements in a vector.\n\n# Vector properties\nlength(fruits)       # Number of elements\n## [1] 4\nfruits * 2           # Produces an ERROR\n## Error in fruits * 2: non-numeric argument to binary operator\n1:length(fruits)     # Sequence from 1 to the vector length\n## [1] 1 2 3 4\nfruits[2:3]          # Access multiple items\n## [1] \"Mango\"      \"Strawberry\"\nfruits[-c(1, 4)]     # Exclude items 1 and 4\n## [1] \"Mango\"      \"Strawberry\"\nWhy does fruits * 2 produce an error?\n\n\n\n8.3 Vector Sequences\nWe can create sequences with vectors using :\nA &lt;- 11:20\nprint(A)\n##  [1] 11 12 13 14 15 16 17 18 19 20\nB &lt;- 0:10\nprint(B)\n##  [1]  0  1  2  3  4  5  6  7  8  9 10\nTRY: Vector functions with your number vectors\n# Vector properties\nlength(A)       # Number of elements\n## [1] 10\nB * 2           # This time no error\n##  [1]  0  2  4  6  8 10 12 14 16 18 20\n1:length(A)     # Sequence from 1 to the vector length\n##  [1]  1  2  3  4  5  6  7  8  9 10\nB[2:3]          # Access multiple items\n## [1] 1 2\nA[-c(1, 4)]     # Exclude items 1 and 4\n## [1] 12 13 15 16 17 18 19 20\n\n\n\n8.4 Vector Indexing\n\nAccess items with []\nExclude an item with -:\n\nfruits[3]  # Third item\n## [1] \"Strawberry\"\nfruits[c(1, 4)]  # First and fourth items\n## [1] \"Banana\" \"Grapes\"\nfruits[-2]  # Exclude the second item\n## [1] \"Banana\"     \"Strawberry\" \"Grapes\"\nPractice: - How do you select only items 1, 3, and 5? - How do you exclude items 2 and 4?\n# Selecting items:\nfruits[c(1, 3, 5)]\n## [1] \"Banana\"     \"Strawberry\" NA\n# Excluding items:\nfruits[-c(2, 4)]\n## [1] \"Banana\"     \"Strawberry\"\n# Saving a subset:\nfavorite_fruits &lt;- fruits[c(1, 3)]\nfavorite_fruits\n## [1] \"Banana\"     \"Strawberry\"\n\n\n\n8.5 Combining Vectors & Type Coercion\nCombine vectors of different types:\nnumbers &lt;- 1:5\ntext &lt;- \"Hello\"\ncombo &lt;- c(numbers, text)\ncombo\n## [1] \"1\"     \"2\"     \"3\"     \"4\"     \"5\"     \"Hello\"\nType Coercion:\n\nR converts all elements in a vector to the same type.\nOrder of precedence:\n\nCharacter &gt; Numeric &gt; Logical.\n\n\ntypeof(numbers)      # \"integer\"\n## [1] \"integer\"\ntypeof(text)         # \"character\"\n## [1] \"character\"\ntypeof(combo)        # \"character\"\n## [1] \"character\"\nPractice:\n\nCombine a vector of numbers and a vector of colors.\nCheck the resulting vector’s type using typeof().\n\n\n\nVectors class exercises\n\nCreate a vector of your favorite hobbies.\n\nAccess the first two items.\nExclude the last item.\n\nCombine vectors of numbers and character strings.\n\nWhat is the type of the resulting vector?\nWhat happens if you add TRUE to the vector?\n\n\nnumbers &lt;- 1:3\nstrings &lt;- c(\"One\", \"Two\", \"Three\")\ncombined &lt;- c(numbers, strings)\n\n\n\n\n8.6 Vector Arithmetic & Recycling\nWe can also do artihmetics with vectors!\nA + B\n##  [1] 11 13 15 17 19 21 23 25 27 29 21\nWhat will happen if we vectors of different sizes?\nD &lt;- 20:24\nE &lt;- 25:30\n\nD+E\n## [1] 45 47 49 51 53 50\nR applies recycling which means…\n\nD (length 5) is recycled to match the length of E (length 6).\nD becomes: 20, 21, 22, 23, 24, 20 (repeating the first element).\nThen, element-wise addition is performed:\n\n(20 + 25), (21 + 26), (22 + 27), (23 + 28), (24 + 29), (20 + 30) - 45, 47, 49, 51, 53, 50"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#data-frames",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#data-frames",
    "title": "Why computational social science?",
    "section": "9. Data Frames",
    "text": "9. Data Frames\nA data frame is a table-like structure (multiple vectors combined into rows and column) in R where each column can have a different data type (numeric, character, logical, etc.). Think of it as a spreadsheet where each column is a variable, and each row is an observation.\n\n9.1 Create a dataframe\n\nUse the data.frame() function to create a data frame.\n\n# Create a data frame\ndf &lt;- data.frame(\n  Name = c(\"Ayse\", \"Jessy\", \"Chris\"),  # Character column\n  Age = c(25, 30, 35),  # Numeric column\n  Professor = c(TRUE, FALSE, TRUE)  # Logical column\n)\n\n# Print the data frame\nprint(df)\n##    Name Age Professor\n## 1  Ayse  25      TRUE\n## 2 Jessy  30     FALSE\n## 3 Chris  35      TRUE\n### OR\n\nName &lt;- c(\"Ayse\", \"Jessy\", \"Chris\")  # Character column\nAge &lt;- c(25, 30, 35)  # Numeric column\nProfessor &lt;- c(TRUE, FALSE, TRUE)  # Logical column\n\ndf &lt;- data.frame(Name, Age, Professor)\nprint(df)\n##    Name Age Professor\n## 1  Ayse  25      TRUE\n## 2 Jessy  30     FALSE\n## 3 Chris  35      TRUE\n\n\nVectors in a data frame must have the same length!!\n# Mismatched lengths will fail\nnames &lt;- c(\"Alice\", \"Bob\", \"Charlie\")\nages &lt;- c(25, 30, NA)  # Fewer elements\npeople &lt;- data.frame(names, ages)  # ERROR\nLet’s fix the error\n# Fix the error:\nages &lt;- c(25, 30, 35)\npeople &lt;- data.frame(names, ages)\npeople\n##     names ages\n## 1   Alice   25\n## 2     Bob   30\n## 3 Charlie   35\n\nTRY: Practice creating a dataframe\n\nCreate a vector of your favorite sports.\n\nSelect items 2 through 4.\nExclude item 1.\n\nCombining Vectors:\n\nCombine a vector of numbers (1:5) with a vector of shapes (\"Circle\", \"Square\", \"Triangle\").\nCheck the type of the resulting vector.\n\nBuilding Data Frames:\n\nCreate a data frame with us_df:\n\nus_cities &lt;- c(\"Boston\", \"Chicago\", \"Seattle\")\nus_populations &lt;- c(700000, 2700000, 750000)\n\nCreate another data frame with asia_df:\n\nasia_cities &lt;- c(\"Beijing\", \"Shanghai\", \"Taipei\", \"Kaohsiung\")\nasia_populations &lt;- c(21540000, 24280000, 2640000, 2775000)\n\n\n\n\nCreate a data frame with your own set of favorite cities (at least 3 cities and estimated populations).\nAdd more cities to the asia_df:\n\nInclude “Guangzhou” with a population of 18,810,000.\nInclude “New Taipei City” with a population of 4,000,000.\n\nCombine all cities into one data frame:\n\nMerge US and Asian city data frames into one combined table.\n\n\n\n\n\n\n9.2 Exploring/Inspecting Data Frames\nBasic Functions to Explore a Data Frame:\n\nhead(df): Displays the first 6 rows of the data frame.\ntail(df): Displays the last 6 rows.\ndim(df): Returns the dimensions (rows, columns).\nstr(df): Shows the structure of the data frame, including data types.\nsummary(df): Provides summary statistics for each column.\n\nhead(df)        # First 6 rows\n##    Name Age Professor\n## 1  Ayse  25      TRUE\n## 2 Jessy  30     FALSE\n## 3 Chris  35      TRUE\ntail(df)        # Last 6 rows\n##    Name Age Professor\n## 1  Ayse  25      TRUE\n## 2 Jessy  30     FALSE\n## 3 Chris  35      TRUE\ndim(df)         # Dimensions: rows and columns\n## [1] 3 3\ndim(mtcars)\n## [1] 32 11\nstr(df)  \n## 'data.frame':    3 obs. of  3 variables:\n##  $ Name     : chr  \"Ayse\" \"Jessy\" \"Chris\"\n##  $ Age      : num  25 30 35\n##  $ Professor: logi  TRUE FALSE TRUE\nstr(mtcars) # Structure\n## 'data.frame':    32 obs. of  11 variables:\n##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n##  $ disp: num  160 160 108 258 360 ...\n##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n##  $ qsec: num  16.5 17 18.6 19.4 17 ...\n##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\nsummary(df)     # Summary statistics\n##      Name                Age       Professor      \n##  Length:3           Min.   :25.0   Mode :logical  \n##  Class :character   1st Qu.:27.5   FALSE:1        \n##  Mode  :character   Median :30.0   TRUE :2        \n##                     Mean   :30.0                  \n##                     3rd Qu.:32.5                  \n##                     Max.   :35.0\n\n\n\n9.3 Accessing Data\nAccessing Columns:\n\nUse the $ operator:df$ColumnName.\nUse bracket notation: df[ , \"ColumnName\"].\n\nprint(df$Name)  # Access the 'Name' column\n## [1] \"Ayse\"  \"Jessy\" \"Chris\"\nsummary(df$Name)\n##    Length     Class      Mode \n##         3 character character\nprint(df[, 1])\n## [1] \"Ayse\"  \"Jessy\" \"Chris\"\nprint(df[, \"Age\"])  # Access the 'Age' column\n## [1] 25 30 35\nAccessing Rows:\n\nUse bracket notation with a row index: df[row_number, ].\n\nprint(df[2, ])  # Access the first row\n##    Name Age Professor\n## 2 Jessy  30     FALSE\nAccessing Specific Elements:\n\nUse df[row, column].\n\nprint(df[2, 3])  # Access the element in the 2nd row and 3rd column\n## [1] FALSE\nprint(df[2, \"Professor\"])\n## [1] FALSE\n\n\n\n9.4 Adding and Removing Data\n Don’t forget [row, column]\nAdding Columns:\n\nUse the $ operator or bracket notation to add a new column.\n\ndf$Graduation &lt;- c(\"2021\", \"2026\", \"2019\")  # Add a new column 'Graduation'\nprint(df)\n##    Name Age Professor Graduation\n## 1  Ayse  25      TRUE       2021\n## 2 Jessy  30     FALSE       2026\n## 3 Chris  35      TRUE       2019\nAdding Rows:\n\nUse the rbind() function.\n\nnew_row &lt;- data.frame(Name = \"Donpeng\", Age = 28, Professor = NA, Graduation = \"2025\")\ndf &lt;- rbind(df, new_row)  # Add a new row\nprint(df)\n##      Name Age Professor Graduation\n## 1    Ayse  25      TRUE       2021\n## 2   Jessy  30     FALSE       2026\n## 3   Chris  35      TRUE       2019\n## 4 Donpeng  28        NA       2025\nRemoving Columns:\n\nUse the NULL assignment, - or subset the data frame.\n\ndf$Graduation &lt;- NULL  # Remove the 'Graduation' column\nprint(df)\n##      Name Age Professor\n## 1    Ayse  25      TRUE\n## 2   Jessy  30     FALSE\n## 3   Chris  35      TRUE\n## 4 Donpeng  28        NA\ndf2 &lt;- df[,-3]\nprint(df2)\n##      Name Age\n## 1    Ayse  25\n## 2   Jessy  30\n## 3   Chris  35\n## 4 Donpeng  28\nSame with rows, use -.\ndf3 &lt;- df[-2,]\nprint(df3)\n##      Name Age Professor\n## 1    Ayse  25      TRUE\n## 3   Chris  35      TRUE\n## 4 Donpeng  28        NA\n\n\n\n9.5 Filtering and Subsetting\nManipulating data is key to preparing it for analysis.\nCommon tasks:\n\nFiltering rows.\nSelecting columns.\nSorting data.\nAggregating data.\n\nFiltering Rows: Use logical conditions inside square brackets.\nTRY: Filter rows where Age &gt; 25\n# Filter rows where Age &gt; 25\ndf_filtered &lt;- df[df$Age &gt; 25, ]\nprint(df_filtered)\n##      Name Age Professor\n## 2   Jessy  30     FALSE\n## 3   Chris  35      TRUE\n## 4 Donpeng  28        NA\nSelecting Specific Columns: Use column indices or names.\nTRY: Select only ‘Name’ and ‘Age’ columns\n# Select only 'Name' and 'Age' columns\ndf_subset &lt;- df[, c(\"Name\", \"Age\")]\nprint(df_subset)\n##      Name Age\n## 1    Ayse  25\n## 2   Jessy  30\n## 3   Chris  35\n## 4 Donpeng  28\ndf_subset &lt;- df[, c(1, 2)]\n\n\n\n9.6 Ordering Data\nRearrange rows based on column values.\nTRY: Sort by Age in ascending order\n# Sort by Age in ascending order\nsorted_df &lt;- df[order(df$Age), ]\n\n# Sort by Age in descending order\nsorted_df_desc &lt;- df[order(-df$Age), ]\nTRY: Sort rows by Name in alphabetical order.\n\n\n\n9.7 Common Data Frame Functions\n\nnrow(df): Returns the number of rows.\nncol(df): Returns the number of columns.\ncolnames(df): Returns column names.\nrownames(df): Returns row names.\nmerge(df1, df2): Combines two data frames by matching rows. We will learn to do this in Tidy next week\n\n\n\n\n9.8 Basic Aggregations\nCompute summary statistics (e.g., mean, sum) for groups of data.\nTRY:\n# Example: Calculate mean Age \nmean(df$Age)\n## [1] 29.5\nmax(df$Age)\n## [1] 35\nmin(df$Age)\n## [1] 25\nsum(df$Age)\n## [1] 118\nsd(df$Age)\n## [1] 4.203173\nTRY: Try with other functions such as max(), min(), sum(), and others\n\n\nClass Exercise\n\nCreate a Data Frame: Create a data frame of your favorite movies, including columns for Title, Year, and Rating.\nFilter Rows: Filter the data frame to show only movies released after 2010.\nAdd a New Column: Add a column indicating whether the movie has won an Oscar (TRUE or FALSE).\n\n\n\n\n\nLecture 1 Cheat Sheet\n\n\n\n\n\n\n\nTopic\nKey Points / Commands\n\n\n\n\nRStudio\nIntegrated Development Environment (IDE) for R. Main panes: Console, Source, Environment, Files/Plots/Packages/Help.\n\n\nConsole\nRuns R code line by line. Good for quick tests. Code is not saved unless written in a script or R Markdown file.\n\n\nTerminal\nSystem command line (not R). Used for OS-level commands (e.g., ls, dir). We will rarely use it.\n\n\nRStudio Projects\nOrganize files for one analysis. Automatically sets working directory. Always open work via the .Rproj file.\n\n\nProject Folder Structure\nRecommended folders: data/ (datasets), scripts/ (R scripts), .Rmd files for notes/analysis.\n\n\nScripts (.R)\nPlain text files for writing and saving R code. Run selected lines with Cmd/Ctrl + Enter.\n\n\nWorking Directory\nCheck with getwd(). Should point to your project folder. Enables relative paths like \"data/file.csv\".\n\n\nR Markdown (.Rmd)\nCombines text, code, and output in one reproducible document. Used for notes, assignments, and reports.\n\n\nRunning vs Knitting\nRun chunks to execute code interactively. Knit rebuilds the entire document from top to bottom in a clean session.\n\n\nCode Chunks\nR code lives inside chunks: {r} code . Output appears below the chunk.\n\n\nR Packages\nExtend R functionality. Install once with install.packages(), load every session with library().\n\n\nVariable Assignment\n&lt;- (preferred), = (allowed), -&gt; (rare). R is case-sensitive.\n\n\nComments\nUse # for comments. R only supports single-line comments.\n\n\nReserved Keywords\nCannot be used as variable names (e.g., if, for, TRUE, FALSE, NA, NULL, Inf).\n\n\nBasic Data Types\nNumeric, Character, Logical, Factor. Check with typeof().\n\n\nDates\nDate, POSIXct, POSIXlt. Use as.Date(), Sys.Date(), Sys.time().\n\n\nLubridate\nEasier date handling: ymd(), mdy(), year(), month(), day(), + days(), + months().\n\n\nBasic Arithmetic\n+, -, *, /, ^. Works on numbers and vectors.\n\n\nVectors\nOne-dimensional objects with same data type. Create with c().\n\n\nVector Indexing\nUse []. Select (x[1:3]), exclude (x[-2]), multiple indices (x[c(1,3)]).\n\n\nVector Functions\nlength(), sequences with :, recycling in vector arithmetic.\n\n\nType Coercion\nCombining types converts to one type: Character &gt; Numeric &gt; Logical.\n\n\nData Frames\nTable-like structure with columns of equal length. Create with data.frame().\n\n\nInspecting Data Frames\nhead(), tail(), str(), summary(), dim(), nrow(), ncol().\n\n\nAccessing Data\nColumns: df$col, df[, \"col\"]. Rows: df[row, ]. Elements: df[row, col].\n\n\nAdding/Removing Data\nAdd columns with $. Add rows with rbind(). Remove with NULL or negative indexing.\n\n\nFiltering & Subsetting\nUse logical conditions: df[df$Age &gt; 25, ]. Select columns by name or index.\n\n\nSorting Data\norder(df$col) for ascending, order(-df$col) for descending.\n\n\nBasic Aggregations\nmean(), max(), min(), sum(), sd() on vectors or data frame columns."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Dr. Ayse D. Lokmanoglu Lecture 2, (B) Jan 28, (A) Feb 2\n\n\n\n\n\nSection\nTopic\n\n\n\n\n1\nData Import\n\n\n1.1\nUnderstanding File Paths\n\n\n1.2\nReading CSV Files with readr\n\n\n1.3\nReading Excel Files with readxl\n\n\n1.4\nLoading via RStudio Files Pane\n\n\n1.5\nColumn Specifications\n\n\n2\nTidy Data Principles\n\n\n2.1\nWhat is Tidy Data?\n\n\n2.2\nTibbles\n\n\n2.3\nThe Pipe Operator\n\n\n3\nDPLYR\n\n\n3.1\nCommon dplyr Functions Overview\n\n\n3.2\nfilter()\n\n\n3.3\nselect()\n\n\n3.4\nmutate()\n\n\n3.5\nsummarize()\n\n\n3.6\ngroup_by()\n\n\n3.7\narrange()\n\n\n3.8\nrename()\n\n\n4\nData Reshaping\n\n\n4.1\npivot_longer()\n\n\n4.2\npivot_wider()\n\n\n4.3\nseparate()\n\n\n4.4\nunite()\n\n\n5\nHandling Missing Values\n\n\n5.1\ndrop_na()\n\n\n5.2\nfill()\n\n\n5.3\nreplace_na()\n\n\n6\nExpanding Tables\n\n\n6.1\nexpand()\n\n\n6.2\ncomplete()\n\n\n7\nJoins in Tidyverse\n\n\n7.1\nleft_join()\n\n\n7.2\nright_join()\n\n\n7.3\ninner_join()\n\n\n7.4\nfull_join()\n\n\n7.5\nanti_join()\n\n\n8\nRegular Expressions\n\n\n8.1\nIntroduction to Regex\n\n\n8.2\nBasic Pattern Matching\n\n\n8.3\nstringr Functions\n\n\n8.4\nCommon Regex Patterns\n\n\n8.5\nPractical Examples\n\n\n\n\n\n\n\nOne of the first steps of any project is importing data into R. Data is often stored in tabular formats like CSV files, Excel spreadsheets, or databases.\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(readxl)\nlibrary(stringr)\n\n\n\nIn Lecture 1, we set up our project folder structure:\nEMS747_Project/\n  EMS747_Project.Rproj\n  data/\n  scripts/\n  bigdata_L2_github.Rmd\nWhen you open your project (by clicking the .Rproj file), R automatically sets your working directory to the project folder. This means you can use relative paths to access files.\n\n\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nAbsolute Path\nFull path from the root of your computer\n\"/Users/ayse/Documents/EMS747_Project/data/file.csv\"\n\n\nRelative Path\nPath relative to your working directory\n\"data/file.csv\"\n\n\n\nAlways use relative paths in your projects! They make your code portable and reproducible.\n\n\n\nThe ../ means “go up one folder level” (to the parent directory).\nExample project structure:\nEMS747_Project/\n  data/\n    Starbucks_User_Data.csv\n  scripts/\n    my_analysis.R          &lt;- If you're working here\n  bigdata_L2_github.Rmd    &lt;- Or working here\n\nIf your script is in the main project folder: use \"data/file.csv\"\nIf your script is in the scripts folder: use \"../data/file.csv\" (go up one level, then into data)\n\n# From the main project folder\nstarbucks_user_data &lt;- read_csv(\"data/Starbucks_User_Data.csv\")\n\n# From the scripts folder (need to go up one level first)\nstarbucks_user_data &lt;- read_csv(\"../data/Starbucks_User_Data.csv\")\nTRY: Check your working directory\ngetwd()  # This should show your project folder path\n## [1] \"/Users/alokman/Library/CloudStorage/GoogleDrive-alokman@bu.edu/My Drive/Teaching/S26_EMS747/ClassSlides\"\n\n\n\n\n\nThe readr package provides fast and friendly functions for reading rectangular data.\n\n\n\nFunction\nDescription\n\n\n\n\nread_csv()\nRead comma delimited files\n\n\nread_csv2()\nRead semicolon delimited files (European format)\n\n\nread_tsv()\nRead tab delimited files\n\n\nread_delim()\nRead files with any delimiter\n\n\n\nTRY: Load data from a URL\n# Load the data from the URL\nurl &lt;- \"https://raw.githubusercontent.com/aysedeniz09/Social_Media_Listening/refs/heads/main/MSC_social_media_list_data/Starbucks_User_Data.csv\"\nstarbucks_user_data &lt;- read_csv(url)\n\nhead(starbucks_user_data)\n## # A tibble: 6 × 16\n##   author_id conversation_id created_at          hashtag lang  like_count mention\n##       &lt;dbl&gt;           &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;  \n## 1     30973         1.61e18 2022-12-27 15:43:16 &lt;NA&gt;    en            10 &lt;NA&gt;   \n## 2     30973         1.60e18 2022-11-29 05:23:55 &lt;NA&gt;    en             9 Mo_sha…\n## 3     30973         1.59e18 2022-11-28 20:14:09 &lt;NA&gt;    en             2 Mixxed…\n## 4     30973         1.60e18 2022-11-28 12:51:28 &lt;NA&gt;    en             0 BihhKa…\n## 5     30973         1.60e18 2022-11-27 15:14:26 &lt;NA&gt;    en             0 BihhKa…\n## 6     30973         1.60e18 2022-11-24 17:47:24 &lt;NA&gt;    en             1 therea…\n## # ℹ 9 more variables: quote_count &lt;dbl&gt;, referenced_status_id &lt;dbl&gt;,\n## #   referenced_user_id &lt;dbl&gt;, reply_count &lt;dbl&gt;, retweet_count &lt;dbl&gt;,\n## #   row_id &lt;dbl&gt;, status_id &lt;dbl&gt;, text &lt;chr&gt;, type &lt;chr&gt;\nLoad from your computer\n# From the main project folder (data is a subfolder)\nstarbucks_user_data &lt;- read_csv(\"data/Starbucks_User_Data.csv\")\n\n# From a scripts folder (need to go up one level first)\nstarbucks_user_data &lt;- read_csv(\"../data/Starbucks_User_Data.csv\")\n\n\n\n\n\n\n\n\n\n\n\nArgument\nDescription\nExample\n\n\n\n\ncol_names\nUse first row as names or provide your own\ncol_names = FALSE or col_names = c(\"x\", \"y\", \"z\")\n\n\nskip\nNumber of lines to skip before reading\nskip = 1\n\n\nn_max\nMaximum number of rows to read\nn_max = 100\n\n\nna\nCharacter vector of strings to interpret as NA\nna = c(\"\", \"NA\", \"NULL\")\n\n\n\n# Skip header row and provide custom column names\nread_csv(\"data/file.csv\", col_names = c(\"x\", \"y\", \"z\"), skip = 1)\n\n# Read only first 100 rows\nread_csv(\"data/file.csv\", n_max = 100)\n\n# Treat \"NULL\" as missing values\nread_csv(\"data/file.csv\", na = c(\"\", \"NA\", \"NULL\"))\n\n\n\n\n\nThe readxl package reads both .xls and .xlsx files.\n# Install if needed\ninstall.packages(\"readxl\")\nlibrary(readxl)\n\n# Read an Excel file\ndata &lt;- read_excel(\"../data/Starbucks_User_Data.xlsx\")\n\n# Read a specific sheet by name or position\ndata &lt;- read_excel(\"../data/Starbucks_User_Data.xlsx\", sheet = \"Sheet2\")\ndata &lt;- read_excel(\"../data/Starbucks_User_Data.xlsx\", sheet = 2)\n\n# Get all sheet names\nexcel_sheets(\"../data/Starbucks_User_Data.xlsx\")\n\n# Read a specific range of cells\ndata &lt;- read_excel(\"../data/Starbucks_User_Data.xlsx\", range = \"B1:D10\")\n\n\n\n\nYou can also import data using RStudio’s point-and-click interface:\n\nClick on the Files pane\nNavigate to your data file\nClick Import Dataset\nConfigure import options\nClick Import\n\n \n\n\n\n\nColumn specifications define what data type each column will be imported as. By default, readr guesses column types based on the first 1000 rows.\nColumn Types:\n\n\n\nType\nFunction\nAbbreviation\n\n\n\n\nLogical\ncol_logical()\n“l”\n\n\nInteger\ncol_integer()\n“i”\n\n\nDouble\ncol_double()\n“d”\n\n\nCharacter\ncol_character()\n“c”\n\n\nFactor\ncol_factor()\n“f”\n\n\nDate\ncol_date()\n“D”\n\n\nDateTime\ncol_datetime()\n“T”\n\n\nSkip\ncol_skip()\n“-” or “_”\n\n\n\n# Set specific column types\nread_csv(\"file.csv\", \n         col_types = list(\n           x = col_double(),\n           y = col_character(),\n           z = col_date()\n         ))\n\n# Use abbreviation string\nread_csv(\"file.csv\", col_types = \"dcD\")\n\n# Select specific columns to import\nread_csv(\"file.csv\", col_select = c(name, age, score))\n\n\n\nR comes with many built-in datasets for practice:\n# See all available datasets\ndata()\n\n# Load a built-in dataset\ndata(\"mtcars\")\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n\n\n\n\n\nTidy data is a consistent way to organize tabular data. A dataset is tidy if:\n\nEach variable is in its own column\nEach observation (case) is in its own row\nEach value is in its own cell\n\n\nImage from: Hassan, F. (2023, March 21). Tidy Data in Python. Medium.\nWhy Tidy Data?\n\nSimplifies data manipulation and visualization\nWorks seamlessly with tidyverse packages (dplyr, ggplot2, tidyr)\nMakes data analysis more reproducible\n\nTidy Cheat Sheet\n\n\n\n\nTibbles are a modern reimagining of data frames provided by the tibble package. They have improved behaviors:\n\nBetter printing (shows only first 10 rows and columns that fit on screen)\nNo partial matching when subsetting columns\nNever convert strings to factors automatically\nSubset with [] for a tibble, [[]] or $ for a vector\n\nlibrary(tibble)\n\n# Create a tibble by columns\nmy_tibble &lt;- tibble(\n  x = 1:3,\n  y = c(\"a\", \"b\", \"c\"),\n  z = c(TRUE, FALSE, TRUE)\n)\nmy_tibble\n## # A tibble: 3 × 3\n##       x y     z    \n##   &lt;int&gt; &lt;chr&gt; &lt;lgl&gt;\n## 1     1 a     TRUE \n## 2     2 b     FALSE\n## 3     3 c     TRUE\n# Create a tibble by rows (useful for small datasets)\nmy_tibble2 &lt;- tribble(\n  ~x, ~y, ~z,\n  1, \"a\", TRUE,\n  2, \"b\", FALSE,\n  3, \"c\", TRUE\n)\nmy_tibble2\n## # A tibble: 3 × 3\n##       x y     z    \n##   &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;\n## 1     1 a     TRUE \n## 2     2 b     FALSE\n## 3     3 c     TRUE\n# Convert data frame to tibble\nas_tibble(mtcars)\n## # A tibble: 32 × 11\n##      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n##  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n##  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n##  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n##  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n##  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n##  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n##  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n##  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n## 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n## # ℹ 22 more rows\n# Check if something is a tibble\nis_tibble(my_tibble)\n## [1] TRUE\nis_tibble(mtcars)\n## [1] FALSE\n\n\n\n\nThe pipe operator |&gt; (or %&gt;% from magrittr) allows you to chain operations together, making code more readable.\nShortcut: - Mac: Cmd + Shift + M - Windows: Ctrl + Shift + M\n# Without pipe (nested functions - hard to read)\nhead(arrange(filter(mtcars, mpg &gt; 20), desc(hp)), 3)\n##                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Lotus Europa  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Mazda RX4     21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n# With pipe (sequential - easy to read)\nmtcars |&gt; \n  filter(mpg &gt; 20) |&gt; \n  arrange(desc(hp)) |&gt; \n  head(3)\n##                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Lotus Europa  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Mazda RX4     21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nThe pipe takes the output of the left side and passes it as the first argument to the function on the right side.\n\n\n\n\n\nThe dplyr package provides a powerful toolkit for data manipulation with intuitive “verb” functions.\nlibrary(dplyr)\nCreate a sample dataset:\ndf &lt;- data.frame(\n  Movie_Title = c(\n    \"YOLO\", \"Successor\", \"Pegasus 2\", \"Deadpool & Wolverine\", \"Moana 2\", \n    \"The Hidden Blade\", \"Avatar: The Spirit Returns\"\n  ),\n  Release_Date = c(\n    \"January 2024\", \"February 2024\", \"March 2024\", \"July 2024\", \n    \"November 2024\", \"April 2024\", \"December 2024\"\n  ),\n  China_Box_Office_Gross = c(\n    479597304, 469612890, 466930272, 450000000, 350000000, 320000000, 550000000\n  ),\n  US_Box_Office_Gross = c(\n    310000000, 280000000, 290000000, 438000000, 221000000, 75000000, 600000000\n  ),\n  Total_Worldwide_Gross = c(\n    479597304, 469612890, 466930272, 850000000, 421000000, 400000000, 1300000000\n  )\n)\n\nhead(df)\n##            Movie_Title  Release_Date China_Box_Office_Gross US_Box_Office_Gross\n## 1                 YOLO  January 2024              479597304            3.10e+08\n## 2            Successor February 2024              469612890            2.80e+08\n## 3            Pegasus 2    March 2024              466930272            2.90e+08\n## 4 Deadpool & Wolverine     July 2024              450000000            4.38e+08\n## 5              Moana 2 November 2024              350000000            2.21e+08\n## 6     The Hidden Blade    April 2024              320000000            7.50e+07\n##   Total_Worldwide_Gross\n## 1             479597304\n## 2             469612890\n## 3             466930272\n## 4             850000000\n## 5             421000000\n## 6             400000000\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nfilter()\nSelect rows based on conditions\n\n\nselect()\nChoose specific columns\n\n\nmutate()\nCreate or transform columns\n\n\nsummarize()\nCompute summary statistics\n\n\ngroup_by()\nGroup data by variables\n\n\narrange()\nSort rows\n\n\nrename()\nRename columns\n\n\n\n\n\n\n\nSelect rows based on specific conditions.\n# Filter movies with China gross &gt; 400 million\ndf |&gt; \n  filter(China_Box_Office_Gross &gt; 400000000)\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1                       YOLO  January 2024              479597304\n## 2                  Successor February 2024              469612890\n## 3                  Pegasus 2    March 2024              466930272\n## 4       Deadpool & Wolverine     July 2024              450000000\n## 5 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross Total_Worldwide_Gross\n## 1            3.10e+08             479597304\n## 2            2.80e+08             469612890\n## 3            2.90e+08             466930272\n## 4            4.38e+08             850000000\n## 5            6.00e+08            1300000000\n# Multiple conditions with AND (&)\nmtcars |&gt; \n  filter(mpg &gt; 20 & cyl == 6)\n##                 mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4      21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag  21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n# Multiple conditions with OR (|)\nmtcars |&gt; \n  filter(mpg &gt; 25 | hp &gt; 200)\n##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n\n\n\n\nChoose specific columns from a dataset.\n# Select specific columns by name\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross)\n##                  Movie_Title Total_Worldwide_Gross\n## 1                       YOLO             479597304\n## 2                  Successor             469612890\n## 3                  Pegasus 2             466930272\n## 4       Deadpool & Wolverine             850000000\n## 5                    Moana 2             421000000\n## 6           The Hidden Blade             400000000\n## 7 Avatar: The Spirit Returns            1300000000\n# Select a range of columns\ndf |&gt; \n  select(Movie_Title:US_Box_Office_Gross)\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1                       YOLO  January 2024              479597304\n## 2                  Successor February 2024              469612890\n## 3                  Pegasus 2    March 2024              466930272\n## 4       Deadpool & Wolverine     July 2024              450000000\n## 5                    Moana 2 November 2024              350000000\n## 6           The Hidden Blade    April 2024              320000000\n## 7 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross\n## 1            3.10e+08\n## 2            2.80e+08\n## 3            2.90e+08\n## 4            4.38e+08\n## 5            2.21e+08\n## 6            7.50e+07\n## 7            6.00e+08\n# Exclude columns with minus sign\ndf |&gt; \n  select(-Release_Date)\n##                  Movie_Title China_Box_Office_Gross US_Box_Office_Gross\n## 1                       YOLO              479597304            3.10e+08\n## 2                  Successor              469612890            2.80e+08\n## 3                  Pegasus 2              466930272            2.90e+08\n## 4       Deadpool & Wolverine              450000000            4.38e+08\n## 5                    Moana 2              350000000            2.21e+08\n## 6           The Hidden Blade              320000000            7.50e+07\n## 7 Avatar: The Spirit Returns              550000000            6.00e+08\n##   Total_Worldwide_Gross\n## 1             479597304\n## 2             469612890\n## 3             466930272\n## 4             850000000\n## 5             421000000\n## 6             400000000\n## 7            1300000000\nHelper functions for select():\n\n\n\nHelper\nDescription\n\n\n\n\nstarts_with(\"x\")\nColumns starting with “x”\n\n\nends_with(\"x\")\nColumns ending with “x”\n\n\ncontains(\"x\")\nColumns containing “x”\n\n\neverything()\nAll columns\n\n\n\n# Select columns containing \"Gross\"\ndf |&gt; \n  select(Movie_Title, contains(\"Gross\"))\n##                  Movie_Title China_Box_Office_Gross US_Box_Office_Gross\n## 1                       YOLO              479597304            3.10e+08\n## 2                  Successor              469612890            2.80e+08\n## 3                  Pegasus 2              466930272            2.90e+08\n## 4       Deadpool & Wolverine              450000000            4.38e+08\n## 5                    Moana 2              350000000            2.21e+08\n## 6           The Hidden Blade              320000000            7.50e+07\n## 7 Avatar: The Spirit Returns              550000000            6.00e+08\n##   Total_Worldwide_Gross\n## 1             479597304\n## 2             469612890\n## 3             466930272\n## 4             850000000\n## 5             421000000\n## 6             400000000\n## 7            1300000000\n\n\n\n\nCreate new columns or transform existing ones.\n# Create a new column\ndf &lt;- df |&gt; \n  mutate(Profit = Total_Worldwide_Gross - US_Box_Office_Gross)\n\n# Multiple new columns at once\ndf |&gt; \n  mutate(\n    Profit = Total_Worldwide_Gross - US_Box_Office_Gross,\n    China_Pct = China_Box_Office_Gross / Total_Worldwide_Gross * 100\n  ) |&gt; \n  select(Movie_Title, Profit, China_Pct)\n##                  Movie_Title    Profit China_Pct\n## 1                       YOLO 169597304 100.00000\n## 2                  Successor 189612890 100.00000\n## 3                  Pegasus 2 176930272 100.00000\n## 4       Deadpool & Wolverine 412000000  52.94118\n## 5                    Moana 2 200000000  83.13539\n## 6           The Hidden Blade 325000000  80.00000\n## 7 Avatar: The Spirit Returns 700000000  42.30769\n\n\n\n\nCompute summary statistics. Often used with group_by().\n# Overall summary\ndf |&gt; \n  summarize(\n    Total_China = sum(China_Box_Office_Gross),\n    Avg_Worldwide = mean(Total_Worldwide_Gross),\n    Count = n()\n  )\n##   Total_China Avg_Worldwide Count\n## 1  3086140466     626734352     7\n# Summary with mtcars\nmtcars |&gt; \n  summarize(\n    mean_mpg = mean(mpg),\n    sd_mpg = sd(mpg),\n    median_hp = median(hp)\n  )\n##   mean_mpg   sd_mpg median_hp\n## 1 20.09062 6.026948       123\n\n\n\n\nGroup data by one or more variables for grouped operations.\n# Group by cylinders and summarize\nmtcars |&gt; \n  group_by(cyl) |&gt; \n  summarize(\n    mean_mpg = mean(mpg),\n    count = n()\n  )\n## # A tibble: 3 × 3\n##     cyl mean_mpg count\n##   &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n## 1     4     26.7    11\n## 2     6     19.7     7\n## 3     8     15.1    14\n# Group by multiple variables\nmtcars |&gt; \n  group_by(cyl, gear) |&gt; \n  summarize(\n    mean_mpg = mean(mpg),\n    count = n(),\n    .groups = \"drop\"  # Ungroup after summarizing\n  )\n## # A tibble: 8 × 4\n##     cyl  gear mean_mpg count\n##   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n## 1     4     3     21.5     1\n## 2     4     4     26.9     8\n## 3     4     5     28.2     2\n## 4     6     3     19.8     2\n## 5     6     4     19.8     4\n## 6     6     5     19.7     1\n## 7     8     3     15.0    12\n## 8     8     5     15.4     2\n\n\n\n\nSort rows by one or more variables.\n# Sort ascending (default)\ndf |&gt; \n  arrange(Total_Worldwide_Gross)\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1           The Hidden Blade    April 2024              320000000\n## 2                    Moana 2 November 2024              350000000\n## 3                  Pegasus 2    March 2024              466930272\n## 4                  Successor February 2024              469612890\n## 5                       YOLO  January 2024              479597304\n## 6       Deadpool & Wolverine     July 2024              450000000\n## 7 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross Total_Worldwide_Gross    Profit\n## 1            7.50e+07             400000000 325000000\n## 2            2.21e+08             421000000 200000000\n## 3            2.90e+08             466930272 176930272\n## 4            2.80e+08             469612890 189612890\n## 5            3.10e+08             479597304 169597304\n## 6            4.38e+08             850000000 412000000\n## 7            6.00e+08            1300000000 700000000\n# Sort descending\ndf |&gt; \n  arrange(desc(Total_Worldwide_Gross))\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1 Avatar: The Spirit Returns December 2024              550000000\n## 2       Deadpool & Wolverine     July 2024              450000000\n## 3                       YOLO  January 2024              479597304\n## 4                  Successor February 2024              469612890\n## 5                  Pegasus 2    March 2024              466930272\n## 6                    Moana 2 November 2024              350000000\n## 7           The Hidden Blade    April 2024              320000000\n##   US_Box_Office_Gross Total_Worldwide_Gross    Profit\n## 1            6.00e+08            1300000000 700000000\n## 2            4.38e+08             850000000 412000000\n## 3            3.10e+08             479597304 169597304\n## 4            2.80e+08             469612890 189612890\n## 5            2.90e+08             466930272 176930272\n## 6            2.21e+08             421000000 200000000\n## 7            7.50e+07             400000000 325000000\n# Sort by multiple columns\nmtcars |&gt; \n  arrange(cyl, desc(mpg)) |&gt; \n  head()\n##                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Toyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Fiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Lotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Fiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n\n\n\n\nRename columns. Syntax: rename(new_name = old_name)\ndf_renamed &lt;- df |&gt; \n  rename(\n    China_Gross = China_Box_Office_Gross,\n    US_Gross = US_Box_Office_Gross,\n    Worldwide_Gross = Total_Worldwide_Gross\n  )\n\nnames(df_renamed)\n## [1] \"Movie_Title\"     \"Release_Date\"    \"China_Gross\"     \"US_Gross\"       \n## [5] \"Worldwide_Gross\" \"Profit\"\n\n\n\nUse the movies dataset and perform the following:\n\nfilter(): Select movies with a total worldwide gross greater than $500M.\nselect(): Choose only Movie_Title and columns containing “Gross”.\nmutate(): Add a new column for US percentage of worldwide gross.\nsummarize(): Compute the total and average gross for China.\ngroup_by() + summarize(): Calculate average gross by release month (hint: you’ll need to extract month first).\narrange(): Sort movies by worldwide gross in descending order.\n\n### Your workspace\n\n\n\n\n\n\nData often needs to be reshaped between “wide” and “long” formats for different analyses.\n\n\n\n\n\nTransform data from wide to long format by collapsing multiple columns into two: one for names and one for values.\nKey Arguments: - cols: Columns to pivot - names_to: Name for the new column holding original column names - values_to: Name for the new column holding values\n# Convert movie data to long format\nlong_df &lt;- df |&gt; \n  pivot_longer(\n    cols = China_Box_Office_Gross:Total_Worldwide_Gross,\n    names_to = \"Box_Office_Type\",\n    values_to = \"Gross\"\n  )\n\nprint(long_df)\n## # A tibble: 21 × 5\n##    Movie_Title          Release_Date     Profit Box_Office_Type            Gross\n##    &lt;chr&gt;                &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt;\n##  1 YOLO                 January 2024  169597304 China_Box_Office_Gross 479597304\n##  2 YOLO                 January 2024  169597304 US_Box_Office_Gross    310000000\n##  3 YOLO                 January 2024  169597304 Total_Worldwide_Gross  479597304\n##  4 Successor            February 2024 189612890 China_Box_Office_Gross 469612890\n##  5 Successor            February 2024 189612890 US_Box_Office_Gross    280000000\n##  6 Successor            February 2024 189612890 Total_Worldwide_Gross  469612890\n##  7 Pegasus 2            March 2024    176930272 China_Box_Office_Gross 466930272\n##  8 Pegasus 2            March 2024    176930272 US_Box_Office_Gross    290000000\n##  9 Pegasus 2            March 2024    176930272 Total_Worldwide_Gross  466930272\n## 10 Deadpool & Wolverine July 2024     412000000 China_Box_Office_Gross 450000000\n## # ℹ 11 more rows\n# Another example with mtcars\nmtcars_long &lt;- mtcars |&gt; \n  rownames_to_column(\"car\") |&gt; \n  pivot_longer(\n    cols = mpg:carb,\n    names_to = \"metric\",\n    values_to = \"value\"\n  )\n\nhead(mtcars_long)\n## # A tibble: 6 × 3\n##   car       metric  value\n##   &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;\n## 1 Mazda RX4 mpg     21   \n## 2 Mazda RX4 cyl      6   \n## 3 Mazda RX4 disp   160   \n## 4 Mazda RX4 hp     110   \n## 5 Mazda RX4 drat     3.9 \n## 6 Mazda RX4 wt       2.62\n\n\n\n\nTransform data from long to wide format by spreading values across multiple columns.\nKey Arguments: - names_from: Column whose values become new column names - values_from: Column whose values fill the new columns\n# Convert back to wide format\nwide_df &lt;- long_df |&gt; \n  pivot_wider(\n    names_from = Box_Office_Type,\n    values_from = Gross\n  )\n\nprint(wide_df)\n## # A tibble: 7 × 6\n##   Movie_Title     Release_Date Profit China_Box_Office_Gross US_Box_Office_Gross\n##   &lt;chr&gt;           &lt;chr&gt;         &lt;dbl&gt;                  &lt;dbl&gt;               &lt;dbl&gt;\n## 1 YOLO            January 2024 1.70e8              479597304           310000000\n## 2 Successor       February 20… 1.90e8              469612890           280000000\n## 3 Pegasus 2       March 2024   1.77e8              466930272           290000000\n## 4 Deadpool & Wol… July 2024    4.12e8              450000000           438000000\n## 5 Moana 2         November 20… 2   e8              350000000           221000000\n## 6 The Hidden Bla… April 2024   3.25e8              320000000            75000000\n## 7 Avatar: The Sp… December 20… 7   e8              550000000           600000000\n## # ℹ 1 more variable: Total_Worldwide_Gross &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\n\nFeature\npivot_longer()\npivot_wider()\n\n\n\n\nDirection\nWide → Long\nLong → Wide\n\n\nPurpose\nConsolidate columns\nSpread values across columns\n\n\nKey Arguments\ncols, names_to, values_to\nnames_from, values_from\n\n\nTypical Use\nTidying data for analysis\nSummarizing for presentation\n\n\n\n\n\n\n\n\nSplit the contents of a single column into multiple columns.\n# Separate Release_Date into Month and Year\ndf_separated &lt;- df |&gt; \n  separate(Release_Date, into = c(\"Month\", \"Year\"), sep = \" \")\n\nprint(df_separated)\n##                  Movie_Title    Month Year China_Box_Office_Gross\n## 1                       YOLO  January 2024              479597304\n## 2                  Successor February 2024              469612890\n## 3                  Pegasus 2    March 2024              466930272\n## 4       Deadpool & Wolverine     July 2024              450000000\n## 5                    Moana 2 November 2024              350000000\n## 6           The Hidden Blade    April 2024              320000000\n## 7 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross Total_Worldwide_Gross    Profit\n## 1            3.10e+08             479597304 169597304\n## 2            2.80e+08             469612890 189612890\n## 3            2.90e+08             466930272 176930272\n## 4            4.38e+08             850000000 412000000\n## 5            2.21e+08             421000000 200000000\n## 6            7.50e+07             400000000 325000000\n## 7            6.00e+08            1300000000 700000000\nRelated functions: - separate_wider_delim(): Separate by delimiter into columns - separate_wider_position(): Separate by position into columns - separate_longer_delim(): Separate into rows instead of columns\n\n\n\n\nCombine multiple columns into a single column.\n# Combine Month and Year back into Release_Date\ndf_united &lt;- df_separated |&gt; \n  unite(\"Release_Date\", Month, Year, sep = \" \")\n\nprint(df_united)\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1                       YOLO  January 2024              479597304\n## 2                  Successor February 2024              469612890\n## 3                  Pegasus 2    March 2024              466930272\n## 4       Deadpool & Wolverine     July 2024              450000000\n## 5                    Moana 2 November 2024              350000000\n## 6           The Hidden Blade    April 2024              320000000\n## 7 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross Total_Worldwide_Gross    Profit\n## 1            3.10e+08             479597304 169597304\n## 2            2.80e+08             469612890 189612890\n## 3            2.90e+08             466930272 176930272\n## 4            4.38e+08             850000000 412000000\n## 5            2.21e+08             421000000 200000000\n## 6            7.50e+07             400000000 325000000\n## 7            6.00e+08            1300000000 700000000\n\n\n\n\n\nMissing values (NA) are common in real-world data. The tidyr package provides functions to handle them.\n# Create sample data with missing values\nmissing_df &lt;- tibble(\n  x1 = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n  x2 = c(1, NA, NA, 3, NA)\n)\nmissing_df\n## # A tibble: 5 × 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B        NA\n## 3 C        NA\n## 4 D         3\n## 5 E        NA\n\n\n\nRemove rows containing NA values.\n# Drop rows with NA in any column\nmissing_df |&gt; \n  drop_na()\n## # A tibble: 2 × 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 D         3\n# Drop rows with NA only in specific columns\nmissing_df |&gt; \n  drop_na(x2)\n## # A tibble: 2 × 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 D         3\n\n\n\n\nFill in NA values using the previous or next value.\n# Fill down (default)\nmissing_df |&gt; \n  fill(x2, .direction = \"down\")\n## # A tibble: 5 × 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         1\n## 3 C         1\n## 4 D         3\n## 5 E         3\n# Fill up\nmissing_df |&gt; \n  fill(x2, .direction = \"up\")\n## # A tibble: 5 × 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         3\n## 3 C         3\n## 4 D         3\n## 5 E        NA\n# Fill in both directions (down first, then up)\nmissing_df |&gt; \n  fill(x2, .direction = \"downup\")\n## # A tibble: 5 × 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         1\n## 3 C         1\n## 4 D         3\n## 5 E         3\n\n\n\n\nReplace NA values with a specified value.\n# Replace NA with a specific value\nmissing_df |&gt; \n  replace_na(list(x2 = 0))\n## # A tibble: 5 × 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         0\n## 3 C         0\n## 4 D         3\n## 5 E         0\n# Replace NA with the mean (requires mutate)\nmissing_df |&gt; \n  mutate(x2 = replace_na(x2, mean(x2, na.rm = TRUE)))\n## # A tibble: 5 × 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         2\n## 3 C         2\n## 4 D         3\n## 5 E         2\n\n\n\n\n\nCreate new combinations of variables or identify implicit missing values.\n\n\n\nCreate a tibble with all possible combinations of specified variables.\n# All combinations of cyl and gear\nmtcars |&gt; \n  expand(cyl, gear)\n## # A tibble: 9 × 2\n##     cyl  gear\n##   &lt;dbl&gt; &lt;dbl&gt;\n## 1     4     3\n## 2     4     4\n## 3     4     5\n## 4     6     3\n## 5     6     4\n## 6     6     5\n## 7     8     3\n## 8     8     4\n## 9     8     5\n\n\n\n\nAdd missing combinations of values to a dataset, filling other variables with NA.\n# Sample data with implicit missing combinations\nsales_df &lt;- tibble(\n  store = c(\"A\", \"A\", \"B\"),\n  product = c(\"X\", \"Y\", \"X\"),\n  sales = c(100, 150, 200)\n)\nsales_df\n## # A tibble: 3 × 3\n##   store product sales\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 A     X         100\n## 2 A     Y         150\n## 3 B     X         200\n# Complete with all combinations\nsales_df |&gt; \n  complete(store, product)\n## # A tibble: 4 × 3\n##   store product sales\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 A     X         100\n## 2 A     Y         150\n## 3 B     X         200\n## 4 B     Y          NA\n# Complete and fill missing values with 0\nsales_df |&gt; \n  complete(store, product, fill = list(sales = 0))\n## # A tibble: 4 × 3\n##   store product sales\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 A     X         100\n## 2 A     Y         150\n## 3 B     X         200\n## 4 B     Y           0\n\n\n\n\n\nJoins combine two datasets based on a common key (column).\n\n\n\nJoin Type\nDescription\n\n\n\n\nleft_join()\nKeep all rows from the left dataset\n\n\nright_join()\nKeep all rows from the right dataset\n\n\ninner_join()\nKeep only rows that match in both datasets\n\n\nfull_join()\nKeep all rows from both datasets\n\n\nanti_join()\nKeep rows from left that don’t match right\n\n\n\n\n\nCreate datasets for joining:\n# Directors dataset\ndf_directors &lt;- tibble(\n  Movie_Title = c(\"YOLO\", \"Successor\", \"Deadpool & Wolverine\", \"Moana 2\"),\n  Director = c(\"Jia Ling\", \"Xu Zheng\", \"Shawn Levy\", \"David Derrick Jr.\")\n)\n\n# Ratings dataset  \ndf_ratings &lt;- tibble(\n  Movie_Title = c(\"YOLO\", \"Pegasus 2\", \"Moana 2\", \"Wicked\"),\n  Rating = c(8.1, 7.5, 7.8, 8.0)\n)\n\n\n\nKeep all rows from the left dataset, add matching data from the right.\n# Add directors to movies (keep all movies)\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross) |&gt; \n  left_join(df_directors, by = \"Movie_Title\")\n##                  Movie_Title Total_Worldwide_Gross          Director\n## 1                       YOLO             479597304          Jia Ling\n## 2                  Successor             469612890          Xu Zheng\n## 3                  Pegasus 2             466930272              &lt;NA&gt;\n## 4       Deadpool & Wolverine             850000000        Shawn Levy\n## 5                    Moana 2             421000000 David Derrick Jr.\n## 6           The Hidden Blade             400000000              &lt;NA&gt;\n## 7 Avatar: The Spirit Returns            1300000000              &lt;NA&gt;\n\n\n\n\nKeep all rows from the right dataset, add matching data from the left.\n# Keep all directors, add movie data\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross) |&gt; \n  right_join(df_directors, by = \"Movie_Title\")\n##            Movie_Title Total_Worldwide_Gross          Director\n## 1                 YOLO             479597304          Jia Ling\n## 2            Successor             469612890          Xu Zheng\n## 3 Deadpool & Wolverine             850000000        Shawn Levy\n## 4              Moana 2             421000000 David Derrick Jr.\n\n\n\n\nKeep only rows that have matches in both datasets.\n# Only movies that have both gross data AND ratings\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross) |&gt; \n  inner_join(df_ratings, by = \"Movie_Title\")\n##   Movie_Title Total_Worldwide_Gross Rating\n## 1        YOLO             479597304    8.1\n## 2   Pegasus 2             466930272    7.5\n## 3     Moana 2             421000000    7.8\n\n\n\n\nKeep all rows from both datasets, filling with NA where there’s no match.\n# Combine all movies and ratings\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross) |&gt; \n  full_join(df_ratings, by = \"Movie_Title\")\n##                  Movie_Title Total_Worldwide_Gross Rating\n## 1                       YOLO             479597304    8.1\n## 2                  Successor             469612890     NA\n## 3                  Pegasus 2             466930272    7.5\n## 4       Deadpool & Wolverine             850000000     NA\n## 5                    Moana 2             421000000    7.8\n## 6           The Hidden Blade             400000000     NA\n## 7 Avatar: The Spirit Returns            1300000000     NA\n## 8                     Wicked                    NA    8.0\n\n\n\n\nReturn rows from the left dataset that do NOT have a match in the right.\n# Movies WITHOUT directors\ndf |&gt; \n  select(Movie_Title) |&gt; \n  anti_join(df_directors, by = \"Movie_Title\")\n##                  Movie_Title\n## 1                  Pegasus 2\n## 2           The Hidden Blade\n## 3 Avatar: The Spirit Returns\n# Movies WITHOUT ratings\ndf |&gt; \n  select(Movie_Title) |&gt; \n  anti_join(df_ratings, by = \"Movie_Title\")\n##                  Movie_Title\n## 1                  Successor\n## 2       Deadpool & Wolverine\n## 3           The Hidden Blade\n## 4 Avatar: The Spirit Returns\n\n\n\nUse the provided datasets (df, df_directors, df_ratings):\n\nleft_join(): Merge directors into the main movie dataset.\ninner_join(): Find movies that have both director and rating information.\nanti_join(): Identify movies without ratings.\nfull_join(): Create a comprehensive dataset with all movies, directors, and ratings.\n\n### Your workspace\n\n\n\n\n\n\nRegular expressions (regex) are powerful patterns used to match, search, and manipulate text. They are essential for data cleaning and text processing.\nWe will use the stringr package, which is part of the tidyverse.\nlibrary(stringr)\n\n\n\nA regular expression is a sequence of characters that defines a search pattern. Think of it as a sophisticated “find and replace” tool.\nWhy use regex?\n\nClean messy text data (remove special characters, standardize formats)\nExtract specific patterns (emails, phone numbers, dates)\nFilter rows based on text patterns\nTransform text data\n\n\n\n\n\n\n\nThe simplest regex matches exact text:\nfruits &lt;- c(\"apple\", \"banana\", \"pineapple\", \"grape\", \"grapefruit\")\n\n# Find fruits containing \"apple\"\nstr_detect(fruits, \"apple\")\n## [1]  TRUE FALSE  TRUE FALSE FALSE\n# Extract matches\nstr_subset(fruits, \"apple\")\n## [1] \"apple\"     \"pineapple\"\n\n\n\n\nThese characters have special meanings in regex:\n\n\n\n\n\n\n\n\n\nCharacter\nMeaning\nExample\nMatches\n\n\n\n\n.\nAny single character\n\"a.c\"\n“abc”, “a1c”, “a c”\n\n\n^\nStart of string\n\"^The\"\n“The dog” but not “See The dog”\n\n\n$\nEnd of string\n\"end$\"\n“the end” but not “endless”\n\n\n*\nZero or more of previous\n\"ab*c\"\n“ac”, “abc”, “abbc”\n\n\n+\nOne or more of previous\n\"ab+c\"\n“abc”, “abbc” but not “ac”\n\n\n?\nZero or one of previous\n\"colou?r\"\n“color”, “colour”\n\n\n\\\\\nEscape special character\n\"\\\\.\"\nLiteral period “.”\n\n\n\ntext &lt;- c(\"cat\", \"car\", \"card\", \"care\", \"scar\")\n\n# Match \"ca\" followed by any character\nstr_subset(text, \"ca.\")\n## [1] \"cat\"  \"car\"  \"card\" \"care\" \"scar\"\n# Match words starting with \"ca\"\nstr_subset(text, \"^ca\")\n## [1] \"cat\"  \"car\"  \"card\" \"care\"\n# Match words ending with \"r\"\nstr_subset(text, \"r$\")\n## [1] \"car\"  \"scar\"\n\n\n\n\nUse square brackets [] to match any character in a set:\n\n\n\n\n\n\n\n\nPattern\nMeaning\nExample\n\n\n\n\n[abc]\nMatch a, b, or c\n\"[aeiou]\" matches vowels\n\n\n[a-z]\nMatch any lowercase letter\n\n\n\n[A-Z]\nMatch any uppercase letter\n\n\n\n[0-9]\nMatch any digit\nSame as \\\\d\n\n\n[^abc]\nMatch anything EXCEPT a, b, c\n[^0-9] matches non-digits\n\n\n\nwords &lt;- c(\"hello\", \"HELLO\", \"Hello\", \"h3llo\", \"12345\")\n\n# Match words with lowercase letters\nstr_subset(words, \"[a-z]\")\n## [1] \"hello\" \"Hello\" \"h3llo\"\n# Match words with digits\nstr_subset(words, \"[0-9]\")\n## [1] \"h3llo\" \"12345\"\n# Match words starting with uppercase\nstr_subset(words, \"^[A-Z]\")\n## [1] \"HELLO\" \"Hello\"\n\n\n\n\n\n\n\nShorthand\nMeaning\nEquivalent\n\n\n\n\n\\\\d\nAny digit\n[0-9]\n\n\n\\\\D\nAny non-digit\n[^0-9]\n\n\n\\\\w\nAny word character\n[a-zA-Z0-9_]\n\n\n\\\\W\nAny non-word character\n[^a-zA-Z0-9_]\n\n\n\\\\s\nAny whitespace\nSpace, tab, newline\n\n\n\\\\S\nAny non-whitespace\n\n\n\n\nmixed &lt;- c(\"abc123\", \"hello world\", \"test_case\", \"no-hyphens\")\n\n# Find strings with digits\nstr_subset(mixed, \"\\\\d\")\n## [1] \"abc123\"\n# Find strings with whitespace\nstr_subset(mixed, \"\\\\s\")\n## [1] \"hello world\"\n# Find strings with word characters only (no spaces or hyphens)\nstr_detect(mixed, \"^\\\\w+$\")\n## [1]  TRUE FALSE  TRUE FALSE\n\n\n\n\n\nThe stringr package provides consistent, easy-to-use functions for working with strings and regex.\n\n\n\n\n\n\n\n\nFunction\nPurpose\nExample\n\n\n\n\nstr_detect()\nDoes pattern exist? (returns TRUE/FALSE)\nstr_detect(x, \"pattern\")\n\n\nstr_subset()\nReturn elements that match\nstr_subset(x, \"pattern\")\n\n\nstr_extract()\nExtract first match\nstr_extract(x, \"pattern\")\n\n\nstr_extract_all()\nExtract all matches\nstr_extract_all(x, \"pattern\")\n\n\nstr_replace()\nReplace first match\nstr_replace(x, \"pattern\", \"replacement\")\n\n\nstr_replace_all()\nReplace all matches\nstr_replace_all(x, \"pattern\", \"replacement\")\n\n\nstr_match()\nExtract groups from first match\nstr_match(x, \"pattern\")\n\n\nstr_count()\nCount matches\nstr_count(x, \"pattern\")\n\n\nstr_split()\nSplit string by pattern\nstr_split(x, \"pattern\")\n\n\n\n\n\n\nReturns TRUE or FALSE for each element. Great for filtering with dplyr::filter().\nemails &lt;- c(\"user@gmail.com\", \"test@yahoo.com\", \"invalid-email\", \"admin@bu.edu\")\n\n# Check which are valid emails (simple check)\nstr_detect(emails, \"@\")\n## [1]  TRUE  TRUE FALSE  TRUE\n# Use with filter\ndata.frame(email = emails) |&gt;\n  filter(str_detect(email, \"\\\\.edu$\"))\n##          email\n## 1 admin@bu.edu\n\n\n\n\nPulls out the first matching pattern from each string.\nsentences &lt;- c(\"Call me at 555-1234\", \"My number is 555-5678\", \"No phone here\")\n\n# Extract phone numbers\nstr_extract(sentences, \"\\\\d{3}-\\\\d{4}\")\n## [1] \"555-1234\" \"555-5678\" NA\n# Extract first word\nstr_extract(sentences, \"^\\\\w+\")\n## [1] \"Call\" \"My\"   \"No\"\n\n\n\n\nmessy_text &lt;- c(\"Hello   World\", \"Too    many   spaces\", \"Normal text\")\n\n# Replace multiple spaces with single space\nstr_replace_all(messy_text, \"\\\\s+\", \" \")\n## [1] \"Hello World\"     \"Too many spaces\" \"Normal text\"\n# Remove all digits\nstr_replace_all(\"Phone: 555-1234\", \"\\\\d\", \"X\")\n## [1] \"Phone: XXX-XXXX\"\n\n\n\n\nUse parentheses () to create capture groups. str_match() returns a matrix with the full match and each group.\ndates &lt;- c(\"2024-01-15\", \"2023-12-25\", \"2025-06-30\")\n\n# Extract year, month, day separately\nstr_match(dates, \"(\\\\d{4})-(\\\\d{2})-(\\\\d{2})\")\n##      [,1]         [,2]   [,3] [,4]\n## [1,] \"2024-01-15\" \"2024\" \"01\" \"15\"\n## [2,] \"2023-12-25\" \"2023\" \"12\" \"25\"\n## [3,] \"2025-06-30\" \"2025\" \"06\" \"30\"\n\n\n\n\n\nHere are some useful patterns for common data cleaning tasks:\n\n\n\nTask\nPattern\nExample\n\n\n\n\nEmail\n[\\\\w.-]+@[\\\\w.-]+\\\\.\\\\w+\nuser@example.com\n\n\nPhone (US)\n\\\\d{3}[-.\\\\s]?\\\\d{3}[-.\\\\s]?\\\\d{4}\n555-123-4567\n\n\nURL\nhttps?://[\\\\w./]+\nhttps://example.com\n\n\nTwitter handle\n@\\\\w+\n@username\n\n\nHashtag\n#\\\\w+\n#DataScience\n\n\nDate (YYYY-MM-DD)\n\\\\d{4}-\\\\d{2}-\\\\d{2}\n2024-01-15\n\n\nTime (HH:MM)\n\\\\d{2}:\\\\d{2}\n14:30\n\n\n\n\n\n\n\n\n\ntweets &lt;- c(\n  \"@user1 Check out this link https://t.co/abc123 #DataScience\",\n  \"Hello @user2! Great post! #RStats #coding\",\n  \"Just a regular tweet with no mentions\"\n)\n\n# Extract mentions (@username)\nstr_extract_all(tweets, \"@\\\\w+\")\n## [[1]]\n## [1] \"@user1\"\n## \n## [[2]]\n## [1] \"@user2\"\n## \n## [[3]]\n## character(0)\n# Extract hashtags\nstr_extract_all(tweets, \"#\\\\w+\")\n## [[1]]\n## [1] \"#DataScience\"\n## \n## [[2]]\n## [1] \"#RStats\" \"#coding\"\n## \n## [[3]]\n## character(0)\n# Remove URLs\nstr_replace_all(tweets, \"https?://\\\\S+\", \"[URL]\")\n## [1] \"@user1 Check out this link [URL] #DataScience\"\n## [2] \"Hello @user2! Great post! #RStats #coding\"    \n## [3] \"Just a regular tweet with no mentions\"\n\n\n\n\n# Sample data\ncustomer_data &lt;- data.frame(\n  info = c(\n    \"John Smith, Email: john@gmail.com, Phone: 555-1234\",\n    \"Jane Doe, Email: jane@yahoo.com, Phone: 555-5678\",\n    \"Bob Wilson, Email: bob@bu.edu, Phone: 555-9999\"\n  )\n)\n\n# Extract emails and phones into new columns\ncustomer_data |&gt;\n  mutate(\n    email = str_extract(info, \"[\\\\w.-]+@[\\\\w.-]+\\\\.\\\\w+\"),\n    phone = str_extract(info, \"\\\\d{3}-\\\\d{4}\"),\n    name = str_extract(info, \"^[A-Za-z]+ [A-Za-z]+\")\n  )\n##                                                 info          email    phone\n## 1 John Smith, Email: john@gmail.com, Phone: 555-1234 john@gmail.com 555-1234\n## 2   Jane Doe, Email: jane@yahoo.com, Phone: 555-5678 jane@yahoo.com 555-5678\n## 3     Bob Wilson, Email: bob@bu.edu, Phone: 555-9999     bob@bu.edu 555-9999\n##         name\n## 1 John Smith\n## 2   Jane Doe\n## 3 Bob Wilson\n\n\n\n\nlibrary(janeaustenr)\n\n# Get all Jane Austen books\nbooks &lt;- austen_books()\n\n# Find lines mentioning \"Mr.\" followed by a name\nbooks |&gt;\n  filter(str_detect(text, \"Mr\\\\.\\\\s[A-Z][a-z]+\")) |&gt;\n  mutate(mr_name = str_extract(text, \"Mr\\\\.\\\\s[A-Z][a-z]+\")) |&gt;\n  count(mr_name, sort = TRUE) |&gt;\n  head(10)\n\n\n\n\nUsing the Starbucks Twitter data:\n\nExtract all Twitter usernames (mentions starting with @) from the text column\nCount how many tweets contain hashtags\nFind tweets that mention “coffee” (case insensitive - hint: use (?i) or str_to_lower())\nExtract any URLs from the tweets\nCreate a new column with the text cleaned of URLs and mentions\n\n### Your workspace\n# Load the data if needed\nurl &lt;- \"https://raw.githubusercontent.com/aysedeniz09/Social_Media_Listening/refs/heads/main/MSC_social_media_list_data/Starbucks_User_Data.csv\"\nstarbucks &lt;- read_csv(url)\n\n# 1. Extract mentions\n\n\n# 2. Count hashtag tweets\n\n\n# 3. Find coffee tweets\n\n\n# 4. Extract URLs\n\n\n# 5. Clean text\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic\nKey Points\n\n\n\n\nFile Paths\nUse relative paths (e.g., \"data/file.csv\"). Use ../ to go up one directory level. Always use projects to set working directory automatically.\n\n\nData Import\nread_csv(), read_excel(), read_delim() for different file types. Use col_types to specify column types.\n\n\nTibbles\nModern data frames with better printing and subsetting. Create with tibble() or tribble(). Convert with as_tibble().\n\n\nPipe Operator\n|&gt; or %&gt;% chains operations together. Shortcut: Cmd/Ctrl + Shift + M.\n\n\nTidy Data\nEach variable in a column, each observation in a row, each value in a cell.\n\n\nDPLYR Functions\nfilter(): rows by condition; select(): columns; mutate(): create/transform; summarize(): aggregate; group_by(): group operations; arrange(): sort; rename(): rename columns.\n\n\nData Reshaping\npivot_longer(): wide → long; pivot_wider(): long → wide.\n\n\nSplit/Combine\nseparate(): split column into multiple; unite(): combine columns into one.\n\n\nMissing Values\ndrop_na(): remove NA rows; fill(): fill NA with adjacent values; replace_na(): replace NA with specific value.\n\n\nExpand Tables\nexpand(): all combinations; complete(): add missing combinations with NA.\n\n\nJoins\nleft_join(): keep all left; right_join(): keep all right; inner_join(): only matches; full_join(): keep all; anti_join(): non-matches.\n\n\nRegex Basics\n. any char; ^ start; $ end; * zero+; + one+; ? zero/one; [] character class; \\\\d digit; \\\\w word char; \\\\s whitespace.\n\n\nstringr Functions\nstr_detect(): check pattern; str_extract(): get match; str_replace(): replace match; str_match(): extract groups; str_subset(): filter by pattern."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#lecture-2-table-of-contents",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#lecture-2-table-of-contents",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Section\nTopic\n\n\n\n\n1\nData Import\n\n\n1.1\nUnderstanding File Paths\n\n\n1.2\nReading CSV Files with readr\n\n\n1.3\nReading Excel Files with readxl\n\n\n1.4\nLoading via RStudio Files Pane\n\n\n1.5\nColumn Specifications\n\n\n2\nTidy Data Principles\n\n\n2.1\nWhat is Tidy Data?\n\n\n2.2\nTibbles\n\n\n2.3\nThe Pipe Operator\n\n\n3\nDPLYR\n\n\n3.1\nCommon dplyr Functions Overview\n\n\n3.2\nfilter()\n\n\n3.3\nselect()\n\n\n3.4\nmutate()\n\n\n3.5\nsummarize()\n\n\n3.6\ngroup_by()\n\n\n3.7\narrange()\n\n\n3.8\nrename()\n\n\n4\nData Reshaping\n\n\n4.1\npivot_longer()\n\n\n4.2\npivot_wider()\n\n\n4.3\nseparate()\n\n\n4.4\nunite()\n\n\n5\nHandling Missing Values\n\n\n5.1\ndrop_na()\n\n\n5.2\nfill()\n\n\n5.3\nreplace_na()\n\n\n6\nExpanding Tables\n\n\n6.1\nexpand()\n\n\n6.2\ncomplete()\n\n\n7\nJoins in Tidyverse\n\n\n7.1\nleft_join()\n\n\n7.2\nright_join()\n\n\n7.3\ninner_join()\n\n\n7.4\nfull_join()\n\n\n7.5\nanti_join()\n\n\n8\nRegular Expressions\n\n\n8.1\nIntroduction to Regex\n\n\n8.2\nBasic Pattern Matching\n\n\n8.3\nstringr Functions\n\n\n8.4\nCommon Regex Patterns\n\n\n8.5\nPractical Examples"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#data-import",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#data-import",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "One of the first steps of any project is importing data into R. Data is often stored in tabular formats like CSV files, Excel spreadsheets, or databases.\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(readxl)\nlibrary(stringr)\n\n\n\nIn Lecture 1, we set up our project folder structure:\nEMS747_Project/\n  EMS747_Project.Rproj\n  data/\n  scripts/\n  bigdata_L2_github.Rmd\nWhen you open your project (by clicking the .Rproj file), R automatically sets your working directory to the project folder. This means you can use relative paths to access files.\n\n\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nAbsolute Path\nFull path from the root of your computer\n\"/Users/ayse/Documents/EMS747_Project/data/file.csv\"\n\n\nRelative Path\nPath relative to your working directory\n\"data/file.csv\"\n\n\n\nAlways use relative paths in your projects! They make your code portable and reproducible.\n\n\n\nThe ../ means “go up one folder level” (to the parent directory).\nExample project structure:\nEMS747_Project/\n  data/\n    Starbucks_User_Data.csv\n  scripts/\n    my_analysis.R          &lt;- If you're working here\n  bigdata_L2_github.Rmd    &lt;- Or working here\n\nIf your script is in the main project folder: use \"data/file.csv\"\nIf your script is in the scripts folder: use \"../data/file.csv\" (go up one level, then into data)\n\n# From the main project folder\nstarbucks_user_data &lt;- read_csv(\"data/Starbucks_User_Data.csv\")\n\n# From the scripts folder (need to go up one level first)\nstarbucks_user_data &lt;- read_csv(\"../data/Starbucks_User_Data.csv\")\nTRY: Check your working directory\ngetwd()  # This should show your project folder path\n## [1] \"/Users/alokman/Library/CloudStorage/GoogleDrive-alokman@bu.edu/My Drive/Teaching/S26_EMS747/ClassSlides\"\n\n\n\n\n\nThe readr package provides fast and friendly functions for reading rectangular data.\n\n\n\nFunction\nDescription\n\n\n\n\nread_csv()\nRead comma delimited files\n\n\nread_csv2()\nRead semicolon delimited files (European format)\n\n\nread_tsv()\nRead tab delimited files\n\n\nread_delim()\nRead files with any delimiter\n\n\n\nTRY: Load data from a URL\n# Load the data from the URL\nurl &lt;- \"https://raw.githubusercontent.com/aysedeniz09/Social_Media_Listening/refs/heads/main/MSC_social_media_list_data/Starbucks_User_Data.csv\"\nstarbucks_user_data &lt;- read_csv(url)\n\nhead(starbucks_user_data)\n## # A tibble: 6 × 16\n##   author_id conversation_id created_at          hashtag lang  like_count mention\n##       &lt;dbl&gt;           &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;  \n## 1     30973         1.61e18 2022-12-27 15:43:16 &lt;NA&gt;    en            10 &lt;NA&gt;   \n## 2     30973         1.60e18 2022-11-29 05:23:55 &lt;NA&gt;    en             9 Mo_sha…\n## 3     30973         1.59e18 2022-11-28 20:14:09 &lt;NA&gt;    en             2 Mixxed…\n## 4     30973         1.60e18 2022-11-28 12:51:28 &lt;NA&gt;    en             0 BihhKa…\n## 5     30973         1.60e18 2022-11-27 15:14:26 &lt;NA&gt;    en             0 BihhKa…\n## 6     30973         1.60e18 2022-11-24 17:47:24 &lt;NA&gt;    en             1 therea…\n## # ℹ 9 more variables: quote_count &lt;dbl&gt;, referenced_status_id &lt;dbl&gt;,\n## #   referenced_user_id &lt;dbl&gt;, reply_count &lt;dbl&gt;, retweet_count &lt;dbl&gt;,\n## #   row_id &lt;dbl&gt;, status_id &lt;dbl&gt;, text &lt;chr&gt;, type &lt;chr&gt;\nLoad from your computer\n# From the main project folder (data is a subfolder)\nstarbucks_user_data &lt;- read_csv(\"data/Starbucks_User_Data.csv\")\n\n# From a scripts folder (need to go up one level first)\nstarbucks_user_data &lt;- read_csv(\"../data/Starbucks_User_Data.csv\")\n\n\n\n\n\n\n\n\n\n\n\nArgument\nDescription\nExample\n\n\n\n\ncol_names\nUse first row as names or provide your own\ncol_names = FALSE or col_names = c(\"x\", \"y\", \"z\")\n\n\nskip\nNumber of lines to skip before reading\nskip = 1\n\n\nn_max\nMaximum number of rows to read\nn_max = 100\n\n\nna\nCharacter vector of strings to interpret as NA\nna = c(\"\", \"NA\", \"NULL\")\n\n\n\n# Skip header row and provide custom column names\nread_csv(\"data/file.csv\", col_names = c(\"x\", \"y\", \"z\"), skip = 1)\n\n# Read only first 100 rows\nread_csv(\"data/file.csv\", n_max = 100)\n\n# Treat \"NULL\" as missing values\nread_csv(\"data/file.csv\", na = c(\"\", \"NA\", \"NULL\"))\n\n\n\n\n\nThe readxl package reads both .xls and .xlsx files.\n# Install if needed\ninstall.packages(\"readxl\")\nlibrary(readxl)\n\n# Read an Excel file\ndata &lt;- read_excel(\"../data/Starbucks_User_Data.xlsx\")\n\n# Read a specific sheet by name or position\ndata &lt;- read_excel(\"../data/Starbucks_User_Data.xlsx\", sheet = \"Sheet2\")\ndata &lt;- read_excel(\"../data/Starbucks_User_Data.xlsx\", sheet = 2)\n\n# Get all sheet names\nexcel_sheets(\"../data/Starbucks_User_Data.xlsx\")\n\n# Read a specific range of cells\ndata &lt;- read_excel(\"../data/Starbucks_User_Data.xlsx\", range = \"B1:D10\")\n\n\n\n\nYou can also import data using RStudio’s point-and-click interface:\n\nClick on the Files pane\nNavigate to your data file\nClick Import Dataset\nConfigure import options\nClick Import\n\n \n\n\n\n\nColumn specifications define what data type each column will be imported as. By default, readr guesses column types based on the first 1000 rows.\nColumn Types:\n\n\n\nType\nFunction\nAbbreviation\n\n\n\n\nLogical\ncol_logical()\n“l”\n\n\nInteger\ncol_integer()\n“i”\n\n\nDouble\ncol_double()\n“d”\n\n\nCharacter\ncol_character()\n“c”\n\n\nFactor\ncol_factor()\n“f”\n\n\nDate\ncol_date()\n“D”\n\n\nDateTime\ncol_datetime()\n“T”\n\n\nSkip\ncol_skip()\n“-” or “_”\n\n\n\n# Set specific column types\nread_csv(\"file.csv\", \n         col_types = list(\n           x = col_double(),\n           y = col_character(),\n           z = col_date()\n         ))\n\n# Use abbreviation string\nread_csv(\"file.csv\", col_types = \"dcD\")\n\n# Select specific columns to import\nread_csv(\"file.csv\", col_select = c(name, age, score))\n\n\n\nR comes with many built-in datasets for practice:\n# See all available datasets\ndata()\n\n# Load a built-in dataset\ndata(\"mtcars\")\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#tidy-data-principles",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#tidy-data-principles",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Tidy data is a consistent way to organize tabular data. A dataset is tidy if:\n\nEach variable is in its own column\nEach observation (case) is in its own row\nEach value is in its own cell\n\n\nImage from: Hassan, F. (2023, March 21). Tidy Data in Python. Medium.\nWhy Tidy Data?\n\nSimplifies data manipulation and visualization\nWorks seamlessly with tidyverse packages (dplyr, ggplot2, tidyr)\nMakes data analysis more reproducible\n\nTidy Cheat Sheet\n\n\n\n\nTibbles are a modern reimagining of data frames provided by the tibble package. They have improved behaviors:\n\nBetter printing (shows only first 10 rows and columns that fit on screen)\nNo partial matching when subsetting columns\nNever convert strings to factors automatically\nSubset with [] for a tibble, [[]] or $ for a vector\n\nlibrary(tibble)\n\n# Create a tibble by columns\nmy_tibble &lt;- tibble(\n  x = 1:3,\n  y = c(\"a\", \"b\", \"c\"),\n  z = c(TRUE, FALSE, TRUE)\n)\nmy_tibble\n## # A tibble: 3 × 3\n##       x y     z    \n##   &lt;int&gt; &lt;chr&gt; &lt;lgl&gt;\n## 1     1 a     TRUE \n## 2     2 b     FALSE\n## 3     3 c     TRUE\n# Create a tibble by rows (useful for small datasets)\nmy_tibble2 &lt;- tribble(\n  ~x, ~y, ~z,\n  1, \"a\", TRUE,\n  2, \"b\", FALSE,\n  3, \"c\", TRUE\n)\nmy_tibble2\n## # A tibble: 3 × 3\n##       x y     z    \n##   &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;\n## 1     1 a     TRUE \n## 2     2 b     FALSE\n## 3     3 c     TRUE\n# Convert data frame to tibble\nas_tibble(mtcars)\n## # A tibble: 32 × 11\n##      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n##  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n##  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n##  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n##  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n##  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n##  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n##  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n##  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n## 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n## # ℹ 22 more rows\n# Check if something is a tibble\nis_tibble(my_tibble)\n## [1] TRUE\nis_tibble(mtcars)\n## [1] FALSE\n\n\n\n\nThe pipe operator |&gt; (or %&gt;% from magrittr) allows you to chain operations together, making code more readable.\nShortcut: - Mac: Cmd + Shift + M - Windows: Ctrl + Shift + M\n# Without pipe (nested functions - hard to read)\nhead(arrange(filter(mtcars, mpg &gt; 20), desc(hp)), 3)\n##                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Lotus Europa  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Mazda RX4     21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n# With pipe (sequential - easy to read)\nmtcars |&gt; \n  filter(mpg &gt; 20) |&gt; \n  arrange(desc(hp)) |&gt; \n  head(3)\n##                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Lotus Europa  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Mazda RX4     21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nThe pipe takes the output of the left side and passes it as the first argument to the function on the right side."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#dplyr",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#dplyr",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "The dplyr package provides a powerful toolkit for data manipulation with intuitive “verb” functions.\nlibrary(dplyr)\nCreate a sample dataset:\ndf &lt;- data.frame(\n  Movie_Title = c(\n    \"YOLO\", \"Successor\", \"Pegasus 2\", \"Deadpool & Wolverine\", \"Moana 2\", \n    \"The Hidden Blade\", \"Avatar: The Spirit Returns\"\n  ),\n  Release_Date = c(\n    \"January 2024\", \"February 2024\", \"March 2024\", \"July 2024\", \n    \"November 2024\", \"April 2024\", \"December 2024\"\n  ),\n  China_Box_Office_Gross = c(\n    479597304, 469612890, 466930272, 450000000, 350000000, 320000000, 550000000\n  ),\n  US_Box_Office_Gross = c(\n    310000000, 280000000, 290000000, 438000000, 221000000, 75000000, 600000000\n  ),\n  Total_Worldwide_Gross = c(\n    479597304, 469612890, 466930272, 850000000, 421000000, 400000000, 1300000000\n  )\n)\n\nhead(df)\n##            Movie_Title  Release_Date China_Box_Office_Gross US_Box_Office_Gross\n## 1                 YOLO  January 2024              479597304            3.10e+08\n## 2            Successor February 2024              469612890            2.80e+08\n## 3            Pegasus 2    March 2024              466930272            2.90e+08\n## 4 Deadpool & Wolverine     July 2024              450000000            4.38e+08\n## 5              Moana 2 November 2024              350000000            2.21e+08\n## 6     The Hidden Blade    April 2024              320000000            7.50e+07\n##   Total_Worldwide_Gross\n## 1             479597304\n## 2             469612890\n## 3             466930272\n## 4             850000000\n## 5             421000000\n## 6             400000000\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nfilter()\nSelect rows based on conditions\n\n\nselect()\nChoose specific columns\n\n\nmutate()\nCreate or transform columns\n\n\nsummarize()\nCompute summary statistics\n\n\ngroup_by()\nGroup data by variables\n\n\narrange()\nSort rows\n\n\nrename()\nRename columns\n\n\n\n\n\n\n\nSelect rows based on specific conditions.\n# Filter movies with China gross &gt; 400 million\ndf |&gt; \n  filter(China_Box_Office_Gross &gt; 400000000)\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1                       YOLO  January 2024              479597304\n## 2                  Successor February 2024              469612890\n## 3                  Pegasus 2    March 2024              466930272\n## 4       Deadpool & Wolverine     July 2024              450000000\n## 5 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross Total_Worldwide_Gross\n## 1            3.10e+08             479597304\n## 2            2.80e+08             469612890\n## 3            2.90e+08             466930272\n## 4            4.38e+08             850000000\n## 5            6.00e+08            1300000000\n# Multiple conditions with AND (&)\nmtcars |&gt; \n  filter(mpg &gt; 20 & cyl == 6)\n##                 mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4      21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag  21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n# Multiple conditions with OR (|)\nmtcars |&gt; \n  filter(mpg &gt; 25 | hp &gt; 200)\n##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n\n\n\n\nChoose specific columns from a dataset.\n# Select specific columns by name\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross)\n##                  Movie_Title Total_Worldwide_Gross\n## 1                       YOLO             479597304\n## 2                  Successor             469612890\n## 3                  Pegasus 2             466930272\n## 4       Deadpool & Wolverine             850000000\n## 5                    Moana 2             421000000\n## 6           The Hidden Blade             400000000\n## 7 Avatar: The Spirit Returns            1300000000\n# Select a range of columns\ndf |&gt; \n  select(Movie_Title:US_Box_Office_Gross)\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1                       YOLO  January 2024              479597304\n## 2                  Successor February 2024              469612890\n## 3                  Pegasus 2    March 2024              466930272\n## 4       Deadpool & Wolverine     July 2024              450000000\n## 5                    Moana 2 November 2024              350000000\n## 6           The Hidden Blade    April 2024              320000000\n## 7 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross\n## 1            3.10e+08\n## 2            2.80e+08\n## 3            2.90e+08\n## 4            4.38e+08\n## 5            2.21e+08\n## 6            7.50e+07\n## 7            6.00e+08\n# Exclude columns with minus sign\ndf |&gt; \n  select(-Release_Date)\n##                  Movie_Title China_Box_Office_Gross US_Box_Office_Gross\n## 1                       YOLO              479597304            3.10e+08\n## 2                  Successor              469612890            2.80e+08\n## 3                  Pegasus 2              466930272            2.90e+08\n## 4       Deadpool & Wolverine              450000000            4.38e+08\n## 5                    Moana 2              350000000            2.21e+08\n## 6           The Hidden Blade              320000000            7.50e+07\n## 7 Avatar: The Spirit Returns              550000000            6.00e+08\n##   Total_Worldwide_Gross\n## 1             479597304\n## 2             469612890\n## 3             466930272\n## 4             850000000\n## 5             421000000\n## 6             400000000\n## 7            1300000000\nHelper functions for select():\n\n\n\nHelper\nDescription\n\n\n\n\nstarts_with(\"x\")\nColumns starting with “x”\n\n\nends_with(\"x\")\nColumns ending with “x”\n\n\ncontains(\"x\")\nColumns containing “x”\n\n\neverything()\nAll columns\n\n\n\n# Select columns containing \"Gross\"\ndf |&gt; \n  select(Movie_Title, contains(\"Gross\"))\n##                  Movie_Title China_Box_Office_Gross US_Box_Office_Gross\n## 1                       YOLO              479597304            3.10e+08\n## 2                  Successor              469612890            2.80e+08\n## 3                  Pegasus 2              466930272            2.90e+08\n## 4       Deadpool & Wolverine              450000000            4.38e+08\n## 5                    Moana 2              350000000            2.21e+08\n## 6           The Hidden Blade              320000000            7.50e+07\n## 7 Avatar: The Spirit Returns              550000000            6.00e+08\n##   Total_Worldwide_Gross\n## 1             479597304\n## 2             469612890\n## 3             466930272\n## 4             850000000\n## 5             421000000\n## 6             400000000\n## 7            1300000000\n\n\n\n\nCreate new columns or transform existing ones.\n# Create a new column\ndf &lt;- df |&gt; \n  mutate(Profit = Total_Worldwide_Gross - US_Box_Office_Gross)\n\n# Multiple new columns at once\ndf |&gt; \n  mutate(\n    Profit = Total_Worldwide_Gross - US_Box_Office_Gross,\n    China_Pct = China_Box_Office_Gross / Total_Worldwide_Gross * 100\n  ) |&gt; \n  select(Movie_Title, Profit, China_Pct)\n##                  Movie_Title    Profit China_Pct\n## 1                       YOLO 169597304 100.00000\n## 2                  Successor 189612890 100.00000\n## 3                  Pegasus 2 176930272 100.00000\n## 4       Deadpool & Wolverine 412000000  52.94118\n## 5                    Moana 2 200000000  83.13539\n## 6           The Hidden Blade 325000000  80.00000\n## 7 Avatar: The Spirit Returns 700000000  42.30769\n\n\n\n\nCompute summary statistics. Often used with group_by().\n# Overall summary\ndf |&gt; \n  summarize(\n    Total_China = sum(China_Box_Office_Gross),\n    Avg_Worldwide = mean(Total_Worldwide_Gross),\n    Count = n()\n  )\n##   Total_China Avg_Worldwide Count\n## 1  3086140466     626734352     7\n# Summary with mtcars\nmtcars |&gt; \n  summarize(\n    mean_mpg = mean(mpg),\n    sd_mpg = sd(mpg),\n    median_hp = median(hp)\n  )\n##   mean_mpg   sd_mpg median_hp\n## 1 20.09062 6.026948       123\n\n\n\n\nGroup data by one or more variables for grouped operations.\n# Group by cylinders and summarize\nmtcars |&gt; \n  group_by(cyl) |&gt; \n  summarize(\n    mean_mpg = mean(mpg),\n    count = n()\n  )\n## # A tibble: 3 × 3\n##     cyl mean_mpg count\n##   &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n## 1     4     26.7    11\n## 2     6     19.7     7\n## 3     8     15.1    14\n# Group by multiple variables\nmtcars |&gt; \n  group_by(cyl, gear) |&gt; \n  summarize(\n    mean_mpg = mean(mpg),\n    count = n(),\n    .groups = \"drop\"  # Ungroup after summarizing\n  )\n## # A tibble: 8 × 4\n##     cyl  gear mean_mpg count\n##   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n## 1     4     3     21.5     1\n## 2     4     4     26.9     8\n## 3     4     5     28.2     2\n## 4     6     3     19.8     2\n## 5     6     4     19.8     4\n## 6     6     5     19.7     1\n## 7     8     3     15.0    12\n## 8     8     5     15.4     2\n\n\n\n\nSort rows by one or more variables.\n# Sort ascending (default)\ndf |&gt; \n  arrange(Total_Worldwide_Gross)\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1           The Hidden Blade    April 2024              320000000\n## 2                    Moana 2 November 2024              350000000\n## 3                  Pegasus 2    March 2024              466930272\n## 4                  Successor February 2024              469612890\n## 5                       YOLO  January 2024              479597304\n## 6       Deadpool & Wolverine     July 2024              450000000\n## 7 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross Total_Worldwide_Gross    Profit\n## 1            7.50e+07             400000000 325000000\n## 2            2.21e+08             421000000 200000000\n## 3            2.90e+08             466930272 176930272\n## 4            2.80e+08             469612890 189612890\n## 5            3.10e+08             479597304 169597304\n## 6            4.38e+08             850000000 412000000\n## 7            6.00e+08            1300000000 700000000\n# Sort descending\ndf |&gt; \n  arrange(desc(Total_Worldwide_Gross))\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1 Avatar: The Spirit Returns December 2024              550000000\n## 2       Deadpool & Wolverine     July 2024              450000000\n## 3                       YOLO  January 2024              479597304\n## 4                  Successor February 2024              469612890\n## 5                  Pegasus 2    March 2024              466930272\n## 6                    Moana 2 November 2024              350000000\n## 7           The Hidden Blade    April 2024              320000000\n##   US_Box_Office_Gross Total_Worldwide_Gross    Profit\n## 1            6.00e+08            1300000000 700000000\n## 2            4.38e+08             850000000 412000000\n## 3            3.10e+08             479597304 169597304\n## 4            2.80e+08             469612890 189612890\n## 5            2.90e+08             466930272 176930272\n## 6            2.21e+08             421000000 200000000\n## 7            7.50e+07             400000000 325000000\n# Sort by multiple columns\nmtcars |&gt; \n  arrange(cyl, desc(mpg)) |&gt; \n  head()\n##                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Toyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Fiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Lotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Fiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n\n\n\n\nRename columns. Syntax: rename(new_name = old_name)\ndf_renamed &lt;- df |&gt; \n  rename(\n    China_Gross = China_Box_Office_Gross,\n    US_Gross = US_Box_Office_Gross,\n    Worldwide_Gross = Total_Worldwide_Gross\n  )\n\nnames(df_renamed)\n## [1] \"Movie_Title\"     \"Release_Date\"    \"China_Gross\"     \"US_Gross\"       \n## [5] \"Worldwide_Gross\" \"Profit\"\n\n\n\nUse the movies dataset and perform the following:\n\nfilter(): Select movies with a total worldwide gross greater than $500M.\nselect(): Choose only Movie_Title and columns containing “Gross”.\nmutate(): Add a new column for US percentage of worldwide gross.\nsummarize(): Compute the total and average gross for China.\ngroup_by() + summarize(): Calculate average gross by release month (hint: you’ll need to extract month first).\narrange(): Sort movies by worldwide gross in descending order.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#data-reshaping",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#data-reshaping",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Data often needs to be reshaped between “wide” and “long” formats for different analyses.\n\n\n\n\n\nTransform data from wide to long format by collapsing multiple columns into two: one for names and one for values.\nKey Arguments: - cols: Columns to pivot - names_to: Name for the new column holding original column names - values_to: Name for the new column holding values\n# Convert movie data to long format\nlong_df &lt;- df |&gt; \n  pivot_longer(\n    cols = China_Box_Office_Gross:Total_Worldwide_Gross,\n    names_to = \"Box_Office_Type\",\n    values_to = \"Gross\"\n  )\n\nprint(long_df)\n## # A tibble: 21 × 5\n##    Movie_Title          Release_Date     Profit Box_Office_Type            Gross\n##    &lt;chr&gt;                &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt;\n##  1 YOLO                 January 2024  169597304 China_Box_Office_Gross 479597304\n##  2 YOLO                 January 2024  169597304 US_Box_Office_Gross    310000000\n##  3 YOLO                 January 2024  169597304 Total_Worldwide_Gross  479597304\n##  4 Successor            February 2024 189612890 China_Box_Office_Gross 469612890\n##  5 Successor            February 2024 189612890 US_Box_Office_Gross    280000000\n##  6 Successor            February 2024 189612890 Total_Worldwide_Gross  469612890\n##  7 Pegasus 2            March 2024    176930272 China_Box_Office_Gross 466930272\n##  8 Pegasus 2            March 2024    176930272 US_Box_Office_Gross    290000000\n##  9 Pegasus 2            March 2024    176930272 Total_Worldwide_Gross  466930272\n## 10 Deadpool & Wolverine July 2024     412000000 China_Box_Office_Gross 450000000\n## # ℹ 11 more rows\n# Another example with mtcars\nmtcars_long &lt;- mtcars |&gt; \n  rownames_to_column(\"car\") |&gt; \n  pivot_longer(\n    cols = mpg:carb,\n    names_to = \"metric\",\n    values_to = \"value\"\n  )\n\nhead(mtcars_long)\n## # A tibble: 6 × 3\n##   car       metric  value\n##   &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;\n## 1 Mazda RX4 mpg     21   \n## 2 Mazda RX4 cyl      6   \n## 3 Mazda RX4 disp   160   \n## 4 Mazda RX4 hp     110   \n## 5 Mazda RX4 drat     3.9 \n## 6 Mazda RX4 wt       2.62\n\n\n\n\nTransform data from long to wide format by spreading values across multiple columns.\nKey Arguments: - names_from: Column whose values become new column names - values_from: Column whose values fill the new columns\n# Convert back to wide format\nwide_df &lt;- long_df |&gt; \n  pivot_wider(\n    names_from = Box_Office_Type,\n    values_from = Gross\n  )\n\nprint(wide_df)\n## # A tibble: 7 × 6\n##   Movie_Title     Release_Date Profit China_Box_Office_Gross US_Box_Office_Gross\n##   &lt;chr&gt;           &lt;chr&gt;         &lt;dbl&gt;                  &lt;dbl&gt;               &lt;dbl&gt;\n## 1 YOLO            January 2024 1.70e8              479597304           310000000\n## 2 Successor       February 20… 1.90e8              469612890           280000000\n## 3 Pegasus 2       March 2024   1.77e8              466930272           290000000\n## 4 Deadpool & Wol… July 2024    4.12e8              450000000           438000000\n## 5 Moana 2         November 20… 2   e8              350000000           221000000\n## 6 The Hidden Bla… April 2024   3.25e8              320000000            75000000\n## 7 Avatar: The Sp… December 20… 7   e8              550000000           600000000\n## # ℹ 1 more variable: Total_Worldwide_Gross &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\n\nFeature\npivot_longer()\npivot_wider()\n\n\n\n\nDirection\nWide → Long\nLong → Wide\n\n\nPurpose\nConsolidate columns\nSpread values across columns\n\n\nKey Arguments\ncols, names_to, values_to\nnames_from, values_from\n\n\nTypical Use\nTidying data for analysis\nSummarizing for presentation\n\n\n\n\n\n\n\n\nSplit the contents of a single column into multiple columns.\n# Separate Release_Date into Month and Year\ndf_separated &lt;- df |&gt; \n  separate(Release_Date, into = c(\"Month\", \"Year\"), sep = \" \")\n\nprint(df_separated)\n##                  Movie_Title    Month Year China_Box_Office_Gross\n## 1                       YOLO  January 2024              479597304\n## 2                  Successor February 2024              469612890\n## 3                  Pegasus 2    March 2024              466930272\n## 4       Deadpool & Wolverine     July 2024              450000000\n## 5                    Moana 2 November 2024              350000000\n## 6           The Hidden Blade    April 2024              320000000\n## 7 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross Total_Worldwide_Gross    Profit\n## 1            3.10e+08             479597304 169597304\n## 2            2.80e+08             469612890 189612890\n## 3            2.90e+08             466930272 176930272\n## 4            4.38e+08             850000000 412000000\n## 5            2.21e+08             421000000 200000000\n## 6            7.50e+07             400000000 325000000\n## 7            6.00e+08            1300000000 700000000\nRelated functions: - separate_wider_delim(): Separate by delimiter into columns - separate_wider_position(): Separate by position into columns - separate_longer_delim(): Separate into rows instead of columns\n\n\n\n\nCombine multiple columns into a single column.\n# Combine Month and Year back into Release_Date\ndf_united &lt;- df_separated |&gt; \n  unite(\"Release_Date\", Month, Year, sep = \" \")\n\nprint(df_united)\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1                       YOLO  January 2024              479597304\n## 2                  Successor February 2024              469612890\n## 3                  Pegasus 2    March 2024              466930272\n## 4       Deadpool & Wolverine     July 2024              450000000\n## 5                    Moana 2 November 2024              350000000\n## 6           The Hidden Blade    April 2024              320000000\n## 7 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross Total_Worldwide_Gross    Profit\n## 1            3.10e+08             479597304 169597304\n## 2            2.80e+08             469612890 189612890\n## 3            2.90e+08             466930272 176930272\n## 4            4.38e+08             850000000 412000000\n## 5            2.21e+08             421000000 200000000\n## 6            7.50e+07             400000000 325000000\n## 7            6.00e+08            1300000000 700000000"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#handling-missing-values",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#handling-missing-values",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Missing values (NA) are common in real-world data. The tidyr package provides functions to handle them.\n# Create sample data with missing values\nmissing_df &lt;- tibble(\n  x1 = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n  x2 = c(1, NA, NA, 3, NA)\n)\nmissing_df\n## # A tibble: 5 × 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B        NA\n## 3 C        NA\n## 4 D         3\n## 5 E        NA\n\n\n\nRemove rows containing NA values.\n# Drop rows with NA in any column\nmissing_df |&gt; \n  drop_na()\n## # A tibble: 2 × 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 D         3\n# Drop rows with NA only in specific columns\nmissing_df |&gt; \n  drop_na(x2)\n## # A tibble: 2 × 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 D         3\n\n\n\n\nFill in NA values using the previous or next value.\n# Fill down (default)\nmissing_df |&gt; \n  fill(x2, .direction = \"down\")\n## # A tibble: 5 × 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         1\n## 3 C         1\n## 4 D         3\n## 5 E         3\n# Fill up\nmissing_df |&gt; \n  fill(x2, .direction = \"up\")\n## # A tibble: 5 × 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         3\n## 3 C         3\n## 4 D         3\n## 5 E        NA\n# Fill in both directions (down first, then up)\nmissing_df |&gt; \n  fill(x2, .direction = \"downup\")\n## # A tibble: 5 × 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         1\n## 3 C         1\n## 4 D         3\n## 5 E         3\n\n\n\n\nReplace NA values with a specified value.\n# Replace NA with a specific value\nmissing_df |&gt; \n  replace_na(list(x2 = 0))\n## # A tibble: 5 × 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         0\n## 3 C         0\n## 4 D         3\n## 5 E         0\n# Replace NA with the mean (requires mutate)\nmissing_df |&gt; \n  mutate(x2 = replace_na(x2, mean(x2, na.rm = TRUE)))\n## # A tibble: 5 × 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         2\n## 3 C         2\n## 4 D         3\n## 5 E         2"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#expanding-tables",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#expanding-tables",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Create new combinations of variables or identify implicit missing values.\n\n\n\nCreate a tibble with all possible combinations of specified variables.\n# All combinations of cyl and gear\nmtcars |&gt; \n  expand(cyl, gear)\n## # A tibble: 9 × 2\n##     cyl  gear\n##   &lt;dbl&gt; &lt;dbl&gt;\n## 1     4     3\n## 2     4     4\n## 3     4     5\n## 4     6     3\n## 5     6     4\n## 6     6     5\n## 7     8     3\n## 8     8     4\n## 9     8     5\n\n\n\n\nAdd missing combinations of values to a dataset, filling other variables with NA.\n# Sample data with implicit missing combinations\nsales_df &lt;- tibble(\n  store = c(\"A\", \"A\", \"B\"),\n  product = c(\"X\", \"Y\", \"X\"),\n  sales = c(100, 150, 200)\n)\nsales_df\n## # A tibble: 3 × 3\n##   store product sales\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 A     X         100\n## 2 A     Y         150\n## 3 B     X         200\n# Complete with all combinations\nsales_df |&gt; \n  complete(store, product)\n## # A tibble: 4 × 3\n##   store product sales\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 A     X         100\n## 2 A     Y         150\n## 3 B     X         200\n## 4 B     Y          NA\n# Complete and fill missing values with 0\nsales_df |&gt; \n  complete(store, product, fill = list(sales = 0))\n## # A tibble: 4 × 3\n##   store product sales\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 A     X         100\n## 2 A     Y         150\n## 3 B     X         200\n## 4 B     Y           0"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#joins-in-tidyverse",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#joins-in-tidyverse",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Joins combine two datasets based on a common key (column).\n\n\n\nJoin Type\nDescription\n\n\n\n\nleft_join()\nKeep all rows from the left dataset\n\n\nright_join()\nKeep all rows from the right dataset\n\n\ninner_join()\nKeep only rows that match in both datasets\n\n\nfull_join()\nKeep all rows from both datasets\n\n\nanti_join()\nKeep rows from left that don’t match right\n\n\n\n\n\nCreate datasets for joining:\n# Directors dataset\ndf_directors &lt;- tibble(\n  Movie_Title = c(\"YOLO\", \"Successor\", \"Deadpool & Wolverine\", \"Moana 2\"),\n  Director = c(\"Jia Ling\", \"Xu Zheng\", \"Shawn Levy\", \"David Derrick Jr.\")\n)\n\n# Ratings dataset  \ndf_ratings &lt;- tibble(\n  Movie_Title = c(\"YOLO\", \"Pegasus 2\", \"Moana 2\", \"Wicked\"),\n  Rating = c(8.1, 7.5, 7.8, 8.0)\n)\n\n\n\nKeep all rows from the left dataset, add matching data from the right.\n# Add directors to movies (keep all movies)\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross) |&gt; \n  left_join(df_directors, by = \"Movie_Title\")\n##                  Movie_Title Total_Worldwide_Gross          Director\n## 1                       YOLO             479597304          Jia Ling\n## 2                  Successor             469612890          Xu Zheng\n## 3                  Pegasus 2             466930272              &lt;NA&gt;\n## 4       Deadpool & Wolverine             850000000        Shawn Levy\n## 5                    Moana 2             421000000 David Derrick Jr.\n## 6           The Hidden Blade             400000000              &lt;NA&gt;\n## 7 Avatar: The Spirit Returns            1300000000              &lt;NA&gt;\n\n\n\n\nKeep all rows from the right dataset, add matching data from the left.\n# Keep all directors, add movie data\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross) |&gt; \n  right_join(df_directors, by = \"Movie_Title\")\n##            Movie_Title Total_Worldwide_Gross          Director\n## 1                 YOLO             479597304          Jia Ling\n## 2            Successor             469612890          Xu Zheng\n## 3 Deadpool & Wolverine             850000000        Shawn Levy\n## 4              Moana 2             421000000 David Derrick Jr.\n\n\n\n\nKeep only rows that have matches in both datasets.\n# Only movies that have both gross data AND ratings\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross) |&gt; \n  inner_join(df_ratings, by = \"Movie_Title\")\n##   Movie_Title Total_Worldwide_Gross Rating\n## 1        YOLO             479597304    8.1\n## 2   Pegasus 2             466930272    7.5\n## 3     Moana 2             421000000    7.8\n\n\n\n\nKeep all rows from both datasets, filling with NA where there’s no match.\n# Combine all movies and ratings\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross) |&gt; \n  full_join(df_ratings, by = \"Movie_Title\")\n##                  Movie_Title Total_Worldwide_Gross Rating\n## 1                       YOLO             479597304    8.1\n## 2                  Successor             469612890     NA\n## 3                  Pegasus 2             466930272    7.5\n## 4       Deadpool & Wolverine             850000000     NA\n## 5                    Moana 2             421000000    7.8\n## 6           The Hidden Blade             400000000     NA\n## 7 Avatar: The Spirit Returns            1300000000     NA\n## 8                     Wicked                    NA    8.0\n\n\n\n\nReturn rows from the left dataset that do NOT have a match in the right.\n# Movies WITHOUT directors\ndf |&gt; \n  select(Movie_Title) |&gt; \n  anti_join(df_directors, by = \"Movie_Title\")\n##                  Movie_Title\n## 1                  Pegasus 2\n## 2           The Hidden Blade\n## 3 Avatar: The Spirit Returns\n# Movies WITHOUT ratings\ndf |&gt; \n  select(Movie_Title) |&gt; \n  anti_join(df_ratings, by = \"Movie_Title\")\n##                  Movie_Title\n## 1                  Successor\n## 2       Deadpool & Wolverine\n## 3           The Hidden Blade\n## 4 Avatar: The Spirit Returns\n\n\n\nUse the provided datasets (df, df_directors, df_ratings):\n\nleft_join(): Merge directors into the main movie dataset.\ninner_join(): Find movies that have both director and rating information.\nanti_join(): Identify movies without ratings.\nfull_join(): Create a comprehensive dataset with all movies, directors, and ratings.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#regular-expressions",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#regular-expressions",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Regular expressions (regex) are powerful patterns used to match, search, and manipulate text. They are essential for data cleaning and text processing.\nWe will use the stringr package, which is part of the tidyverse.\nlibrary(stringr)\n\n\n\nA regular expression is a sequence of characters that defines a search pattern. Think of it as a sophisticated “find and replace” tool.\nWhy use regex?\n\nClean messy text data (remove special characters, standardize formats)\nExtract specific patterns (emails, phone numbers, dates)\nFilter rows based on text patterns\nTransform text data\n\n\n\n\n\n\n\nThe simplest regex matches exact text:\nfruits &lt;- c(\"apple\", \"banana\", \"pineapple\", \"grape\", \"grapefruit\")\n\n# Find fruits containing \"apple\"\nstr_detect(fruits, \"apple\")\n## [1]  TRUE FALSE  TRUE FALSE FALSE\n# Extract matches\nstr_subset(fruits, \"apple\")\n## [1] \"apple\"     \"pineapple\"\n\n\n\n\nThese characters have special meanings in regex:\n\n\n\n\n\n\n\n\n\nCharacter\nMeaning\nExample\nMatches\n\n\n\n\n.\nAny single character\n\"a.c\"\n“abc”, “a1c”, “a c”\n\n\n^\nStart of string\n\"^The\"\n“The dog” but not “See The dog”\n\n\n$\nEnd of string\n\"end$\"\n“the end” but not “endless”\n\n\n*\nZero or more of previous\n\"ab*c\"\n“ac”, “abc”, “abbc”\n\n\n+\nOne or more of previous\n\"ab+c\"\n“abc”, “abbc” but not “ac”\n\n\n?\nZero or one of previous\n\"colou?r\"\n“color”, “colour”\n\n\n\\\\\nEscape special character\n\"\\\\.\"\nLiteral period “.”\n\n\n\ntext &lt;- c(\"cat\", \"car\", \"card\", \"care\", \"scar\")\n\n# Match \"ca\" followed by any character\nstr_subset(text, \"ca.\")\n## [1] \"cat\"  \"car\"  \"card\" \"care\" \"scar\"\n# Match words starting with \"ca\"\nstr_subset(text, \"^ca\")\n## [1] \"cat\"  \"car\"  \"card\" \"care\"\n# Match words ending with \"r\"\nstr_subset(text, \"r$\")\n## [1] \"car\"  \"scar\"\n\n\n\n\nUse square brackets [] to match any character in a set:\n\n\n\n\n\n\n\n\nPattern\nMeaning\nExample\n\n\n\n\n[abc]\nMatch a, b, or c\n\"[aeiou]\" matches vowels\n\n\n[a-z]\nMatch any lowercase letter\n\n\n\n[A-Z]\nMatch any uppercase letter\n\n\n\n[0-9]\nMatch any digit\nSame as \\\\d\n\n\n[^abc]\nMatch anything EXCEPT a, b, c\n[^0-9] matches non-digits\n\n\n\nwords &lt;- c(\"hello\", \"HELLO\", \"Hello\", \"h3llo\", \"12345\")\n\n# Match words with lowercase letters\nstr_subset(words, \"[a-z]\")\n## [1] \"hello\" \"Hello\" \"h3llo\"\n# Match words with digits\nstr_subset(words, \"[0-9]\")\n## [1] \"h3llo\" \"12345\"\n# Match words starting with uppercase\nstr_subset(words, \"^[A-Z]\")\n## [1] \"HELLO\" \"Hello\"\n\n\n\n\n\n\n\nShorthand\nMeaning\nEquivalent\n\n\n\n\n\\\\d\nAny digit\n[0-9]\n\n\n\\\\D\nAny non-digit\n[^0-9]\n\n\n\\\\w\nAny word character\n[a-zA-Z0-9_]\n\n\n\\\\W\nAny non-word character\n[^a-zA-Z0-9_]\n\n\n\\\\s\nAny whitespace\nSpace, tab, newline\n\n\n\\\\S\nAny non-whitespace\n\n\n\n\nmixed &lt;- c(\"abc123\", \"hello world\", \"test_case\", \"no-hyphens\")\n\n# Find strings with digits\nstr_subset(mixed, \"\\\\d\")\n## [1] \"abc123\"\n# Find strings with whitespace\nstr_subset(mixed, \"\\\\s\")\n## [1] \"hello world\"\n# Find strings with word characters only (no spaces or hyphens)\nstr_detect(mixed, \"^\\\\w+$\")\n## [1]  TRUE FALSE  TRUE FALSE\n\n\n\n\n\nThe stringr package provides consistent, easy-to-use functions for working with strings and regex.\n\n\n\n\n\n\n\n\nFunction\nPurpose\nExample\n\n\n\n\nstr_detect()\nDoes pattern exist? (returns TRUE/FALSE)\nstr_detect(x, \"pattern\")\n\n\nstr_subset()\nReturn elements that match\nstr_subset(x, \"pattern\")\n\n\nstr_extract()\nExtract first match\nstr_extract(x, \"pattern\")\n\n\nstr_extract_all()\nExtract all matches\nstr_extract_all(x, \"pattern\")\n\n\nstr_replace()\nReplace first match\nstr_replace(x, \"pattern\", \"replacement\")\n\n\nstr_replace_all()\nReplace all matches\nstr_replace_all(x, \"pattern\", \"replacement\")\n\n\nstr_match()\nExtract groups from first match\nstr_match(x, \"pattern\")\n\n\nstr_count()\nCount matches\nstr_count(x, \"pattern\")\n\n\nstr_split()\nSplit string by pattern\nstr_split(x, \"pattern\")\n\n\n\n\n\n\nReturns TRUE or FALSE for each element. Great for filtering with dplyr::filter().\nemails &lt;- c(\"user@gmail.com\", \"test@yahoo.com\", \"invalid-email\", \"admin@bu.edu\")\n\n# Check which are valid emails (simple check)\nstr_detect(emails, \"@\")\n## [1]  TRUE  TRUE FALSE  TRUE\n# Use with filter\ndata.frame(email = emails) |&gt;\n  filter(str_detect(email, \"\\\\.edu$\"))\n##          email\n## 1 admin@bu.edu\n\n\n\n\nPulls out the first matching pattern from each string.\nsentences &lt;- c(\"Call me at 555-1234\", \"My number is 555-5678\", \"No phone here\")\n\n# Extract phone numbers\nstr_extract(sentences, \"\\\\d{3}-\\\\d{4}\")\n## [1] \"555-1234\" \"555-5678\" NA\n# Extract first word\nstr_extract(sentences, \"^\\\\w+\")\n## [1] \"Call\" \"My\"   \"No\"\n\n\n\n\nmessy_text &lt;- c(\"Hello   World\", \"Too    many   spaces\", \"Normal text\")\n\n# Replace multiple spaces with single space\nstr_replace_all(messy_text, \"\\\\s+\", \" \")\n## [1] \"Hello World\"     \"Too many spaces\" \"Normal text\"\n# Remove all digits\nstr_replace_all(\"Phone: 555-1234\", \"\\\\d\", \"X\")\n## [1] \"Phone: XXX-XXXX\"\n\n\n\n\nUse parentheses () to create capture groups. str_match() returns a matrix with the full match and each group.\ndates &lt;- c(\"2024-01-15\", \"2023-12-25\", \"2025-06-30\")\n\n# Extract year, month, day separately\nstr_match(dates, \"(\\\\d{4})-(\\\\d{2})-(\\\\d{2})\")\n##      [,1]         [,2]   [,3] [,4]\n## [1,] \"2024-01-15\" \"2024\" \"01\" \"15\"\n## [2,] \"2023-12-25\" \"2023\" \"12\" \"25\"\n## [3,] \"2025-06-30\" \"2025\" \"06\" \"30\"\n\n\n\n\n\nHere are some useful patterns for common data cleaning tasks:\n\n\n\nTask\nPattern\nExample\n\n\n\n\nEmail\n[\\\\w.-]+@[\\\\w.-]+\\\\.\\\\w+\nuser@example.com\n\n\nPhone (US)\n\\\\d{3}[-.\\\\s]?\\\\d{3}[-.\\\\s]?\\\\d{4}\n555-123-4567\n\n\nURL\nhttps?://[\\\\w./]+\nhttps://example.com\n\n\nTwitter handle\n@\\\\w+\n@username\n\n\nHashtag\n#\\\\w+\n#DataScience\n\n\nDate (YYYY-MM-DD)\n\\\\d{4}-\\\\d{2}-\\\\d{2}\n2024-01-15\n\n\nTime (HH:MM)\n\\\\d{2}:\\\\d{2}\n14:30\n\n\n\n\n\n\n\n\n\ntweets &lt;- c(\n  \"@user1 Check out this link https://t.co/abc123 #DataScience\",\n  \"Hello @user2! Great post! #RStats #coding\",\n  \"Just a regular tweet with no mentions\"\n)\n\n# Extract mentions (@username)\nstr_extract_all(tweets, \"@\\\\w+\")\n## [[1]]\n## [1] \"@user1\"\n## \n## [[2]]\n## [1] \"@user2\"\n## \n## [[3]]\n## character(0)\n# Extract hashtags\nstr_extract_all(tweets, \"#\\\\w+\")\n## [[1]]\n## [1] \"#DataScience\"\n## \n## [[2]]\n## [1] \"#RStats\" \"#coding\"\n## \n## [[3]]\n## character(0)\n# Remove URLs\nstr_replace_all(tweets, \"https?://\\\\S+\", \"[URL]\")\n## [1] \"@user1 Check out this link [URL] #DataScience\"\n## [2] \"Hello @user2! Great post! #RStats #coding\"    \n## [3] \"Just a regular tweet with no mentions\"\n\n\n\n\n# Sample data\ncustomer_data &lt;- data.frame(\n  info = c(\n    \"John Smith, Email: john@gmail.com, Phone: 555-1234\",\n    \"Jane Doe, Email: jane@yahoo.com, Phone: 555-5678\",\n    \"Bob Wilson, Email: bob@bu.edu, Phone: 555-9999\"\n  )\n)\n\n# Extract emails and phones into new columns\ncustomer_data |&gt;\n  mutate(\n    email = str_extract(info, \"[\\\\w.-]+@[\\\\w.-]+\\\\.\\\\w+\"),\n    phone = str_extract(info, \"\\\\d{3}-\\\\d{4}\"),\n    name = str_extract(info, \"^[A-Za-z]+ [A-Za-z]+\")\n  )\n##                                                 info          email    phone\n## 1 John Smith, Email: john@gmail.com, Phone: 555-1234 john@gmail.com 555-1234\n## 2   Jane Doe, Email: jane@yahoo.com, Phone: 555-5678 jane@yahoo.com 555-5678\n## 3     Bob Wilson, Email: bob@bu.edu, Phone: 555-9999     bob@bu.edu 555-9999\n##         name\n## 1 John Smith\n## 2   Jane Doe\n## 3 Bob Wilson\n\n\n\n\nlibrary(janeaustenr)\n\n# Get all Jane Austen books\nbooks &lt;- austen_books()\n\n# Find lines mentioning \"Mr.\" followed by a name\nbooks |&gt;\n  filter(str_detect(text, \"Mr\\\\.\\\\s[A-Z][a-z]+\")) |&gt;\n  mutate(mr_name = str_extract(text, \"Mr\\\\.\\\\s[A-Z][a-z]+\")) |&gt;\n  count(mr_name, sort = TRUE) |&gt;\n  head(10)\n\n\n\n\nUsing the Starbucks Twitter data:\n\nExtract all Twitter usernames (mentions starting with @) from the text column\nCount how many tweets contain hashtags\nFind tweets that mention “coffee” (case insensitive - hint: use (?i) or str_to_lower())\nExtract any URLs from the tweets\nCreate a new column with the text cleaned of URLs and mentions\n\n### Your workspace\n# Load the data if needed\nurl &lt;- \"https://raw.githubusercontent.com/aysedeniz09/Social_Media_Listening/refs/heads/main/MSC_social_media_list_data/Starbucks_User_Data.csv\"\nstarbucks &lt;- read_csv(url)\n\n# 1. Extract mentions\n\n\n# 2. Count hashtag tweets\n\n\n# 3. Find coffee tweets\n\n\n# 4. Extract URLs\n\n\n# 5. Clean text"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#lecture-2-cheat-sheet",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#lecture-2-cheat-sheet",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Topic\nKey Points\n\n\n\n\nFile Paths\nUse relative paths (e.g., \"data/file.csv\"). Use ../ to go up one directory level. Always use projects to set working directory automatically.\n\n\nData Import\nread_csv(), read_excel(), read_delim() for different file types. Use col_types to specify column types.\n\n\nTibbles\nModern data frames with better printing and subsetting. Create with tibble() or tribble(). Convert with as_tibble().\n\n\nPipe Operator\n|&gt; or %&gt;% chains operations together. Shortcut: Cmd/Ctrl + Shift + M.\n\n\nTidy Data\nEach variable in a column, each observation in a row, each value in a cell.\n\n\nDPLYR Functions\nfilter(): rows by condition; select(): columns; mutate(): create/transform; summarize(): aggregate; group_by(): group operations; arrange(): sort; rename(): rename columns.\n\n\nData Reshaping\npivot_longer(): wide → long; pivot_wider(): long → wide.\n\n\nSplit/Combine\nseparate(): split column into multiple; unite(): combine columns into one.\n\n\nMissing Values\ndrop_na(): remove NA rows; fill(): fill NA with adjacent values; replace_na(): replace NA with specific value.\n\n\nExpand Tables\nexpand(): all combinations; complete(): add missing combinations with NA.\n\n\nJoins\nleft_join(): keep all left; right_join(): keep all right; inner_join(): only matches; full_join(): keep all; anti_join(): non-matches.\n\n\nRegex Basics\n. any char; ^ start; $ end; * zero+; + one+; ? zero/one; [] character class; \\\\d digit; \\\\w word char; \\\\s whitespace.\n\n\nstringr Functions\nstr_detect(): check pattern; str_extract(): get match; str_replace(): replace match; str_match(): extract groups; str_subset(): filter by pattern."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html",
    "title": "Computational Research",
    "section": "",
    "text": "Dr. Ayse D. Lokmanoglu Lecture 4, (B) Feb 11, (A) Feb 17\n\n\n\n\n\nSection\nTopic\n\n\n\n\n1\nControl Structures\n\n\n1.1\nif-else Statements\n\n\n1.2\nfor Loops\n\n\n1.3\nLooping Over Vectors\n\n\n1.4\nNested Loops\n\n\n1.5\nwhile Loops\n\n\n1.6\nbreak and next\n\n\n2\nFunctions\n\n\n2.1\nWhy Write Functions?\n\n\n2.2\nCreating Your First Function\n\n\n2.3\nFunction Arguments\n\n\n2.4\nDefault Arguments\n\n\n2.5\nReturn Values\n\n\n2.6\nArgument Matching\n\n\n3\nText Analysis with TidyText\n\n\n3.1\nIntroduction to TidyText\n\n\n3.2\nTokenizing Text with unnest_tokens()\n\n\n3.3\nRemoving Stopwords\n\n\n4\nKeyword in Context (KWIC)\n\n\n4.1\nWhat is KWIC?\n\n\n4.2\nExtracting Keywords\n\n\n4.3\nExtracting Surrounding Context\n\n\n5\nDocument-Term Matrix (DTM)\n\n\n5.1\nWhat is a DTM?\n\n\n5.2\nFrom Text to DTM: The Pipeline\n\n\n5.3\nCreating the DTM\n\n\n5.4\nVisualizing the DTM as a Heatmap\n\n\n5.5\nVisualizing Top Words per Document\n\n\n5.6\nConverting DTM Back to Tidy Format\n\n\n6\nTF-IDF\n\n\n6.1\nUnderstanding TF-IDF\n\n\n6.2\nCalculating TF-IDF\n\n\n6.3\nVisualizing TF-IDF\n\n\n7\nWord Clouds\n\n\n7.1\nCreating Word Clouds\n\n\n7.2\nCustomizing Word Clouds\n\n\n\n\nALWAYS load our libraries first\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(ggplot2)\n\n\n\n\nControl structures allow you to control the flow of execution in your R code. Instead of running the same code every time, you can add logic to respond to different inputs or data conditions.\nCommon control structures include:\n\n\n\nStructure\nPurpose\n\n\n\n\nif, else\nTest a condition and act on it\n\n\nfor\nExecute a loop a fixed number of times\n\n\nwhile\nExecute a loop while a condition is true\n\n\nbreak\nExit a loop immediately\n\n\nnext\nSkip to the next iteration of a loop\n\n\n\n\n\n\nThe if-else combination tests a condition and executes different code depending on whether it’s TRUE or FALSE.\nBasic if statement:\nx &lt;- 7\n\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n}\n## [1] \"x is greater than 5\"\nif-else statement:\nx &lt;- 3\n\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n} else {\n  print(\"x is NOT greater than 5\")\n}\n## [1] \"x is NOT greater than 5\"\nMultiple conditions with else if:\nx &lt;- 5\n\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n} else if (x == 5) {\n  print(\"x is exactly 5\")\n} else {\n  print(\"x is less than 5\")\n}\n## [1] \"x is exactly 5\"\nCompact if-else (single line):\nx &lt;- 8\ny &lt;- if (x &gt; 5) \"big\" else \"small\"\nprint(y)\n## [1] \"big\"\n\n\n\n\nfor loops iterate over elements in a sequence (vector, list, etc.) and execute code for each element.\nBasic for loop:\nfor (i in 1:5) {\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\nLooping over a character vector:\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\")\n\nfor (fruit in fruits) {\n  print(paste(\"I like\", fruit))\n}\n## [1] \"I like apple\"\n## [1] \"I like banana\"\n## [1] \"I like cherry\"\n\n\n\n\nThere are multiple ways to loop over vectors. The seq_along() function is particularly useful:\ncolors &lt;- c(\"red\", \"green\", \"blue\", \"yellow\")\n\n# Method 1: Loop over indices\nfor (i in 1:length(colors)) {\n  print(paste(\"Color\", i, \"is\", colors[i]))\n}\n## [1] \"Color 1 is red\"\n## [1] \"Color 2 is green\"\n## [1] \"Color 3 is blue\"\n## [1] \"Color 4 is yellow\"\n# Method 2: Using seq_along() - safer!\nfor (i in seq_along(colors)) {\n  print(paste(\"Color\", i, \"is\", colors[i]))\n}\n## [1] \"Color 1 is red\"\n## [1] \"Color 2 is green\"\n## [1] \"Color 3 is blue\"\n## [1] \"Color 4 is yellow\"\n# Method 3: Loop directly over elements\nfor (color in colors) {\n  print(color)\n}\n## [1] \"red\"\n## [1] \"green\"\n## [1] \"blue\"\n## [1] \"yellow\"\nWhy use seq_along()? It’s safer because if the vector is empty, 1:length(x) would give 1:0 which creates c(1, 0), but seq_along(x) correctly returns an empty sequence.\n\n\n\n\nLoops can be nested inside each other. This is useful for working with matrices or multidimensional data:\n# Create a 3x3 matrix\nmat &lt;- matrix(1:9, nrow = 3, ncol = 3)\nprint(mat)\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n# Loop through rows and columns\nfor (i in 1:nrow(mat)) {\n  for (j in 1:ncol(mat)) {\n    print(paste(\"Row\", i, \"Col\", j, \"=\", mat[i, j]))\n  }\n}\n## [1] \"Row 1 Col 1 = 1\"\n## [1] \"Row 1 Col 2 = 4\"\n## [1] \"Row 1 Col 3 = 7\"\n## [1] \"Row 2 Col 1 = 2\"\n## [1] \"Row 2 Col 2 = 5\"\n## [1] \"Row 2 Col 3 = 8\"\n## [1] \"Row 3 Col 1 = 3\"\n## [1] \"Row 3 Col 2 = 6\"\n## [1] \"Row 3 Col 3 = 9\"\nWarning: Avoid nesting more than 2-3 levels deep. If you need more, consider using functions to break up the code.\n\n\n\n\nwhile loops execute as long as a condition is TRUE:\ncount &lt;- 1\n\nwhile (count &lt;= 5) {\n  print(paste(\"Count is:\", count))\n  count &lt;- count + 1\n}\n## [1] \"Count is: 1\"\n## [1] \"Count is: 2\"\n## [1] \"Count is: 3\"\n## [1] \"Count is: 4\"\n## [1] \"Count is: 5\"\nCaution: while loops can run forever if the condition never becomes FALSE. Always make sure your loop has a way to exit!\nExample with multiple conditions:\nset.seed(123)  # For reproducibility\nvalue &lt;- 5\n\nwhile (value &gt;= 2 && value &lt;= 8) {\n  # Random walk: add or subtract 1\n  coin &lt;- sample(c(-1, 1), 1)\n  value &lt;- value + coin\n  print(paste(\"Value is now:\", value))\n}\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 2\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 2\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 6\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 6\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 6\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 2\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 2\"\n## [1] \"Value is now: 1\"\n\n\n\n\n\nbreak: Exit the loop immediately\nnext: Skip the current iteration and continue to the next\n\nUsing break:\nfor (i in 1:10) {\n  if (i &gt; 5) {\n    print(\"Breaking out of loop!\")\n    break\n  }\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n## [1] \"Breaking out of loop!\"\nUsing next:\n# Print only odd numbers\nfor (i in 1:10) {\n  if (i %% 2 == 0) {  # If i is even, skip it\n    next\n  }\n  print(i)\n}\n## [1] 1\n## [1] 3\n## [1] 5\n## [1] 7\n## [1] 9\n\n\n\n\nWrite an if-else statement that checks if a number is positive, negative, or zero.\nCreate a for loop that prints the squares of numbers 1 through 10.\nWrite a while loop that starts at 100 and divides by 2 until the value is less than 1.\nUse a for loop with next to print only numbers divisible by 3 from 1 to 20.\n\n### Your workspace\n\n\n\n\n\n\nFunctions allow you to encapsulate code that you want to reuse. Instead of copying and pasting code, you write it once as a function and call it whenever needed.\n\n\n\n\nReusability: Write code once, use it many times\nReadability: Give meaningful names to complex operations\nMaintainability: Fix bugs in one place instead of many\nAbstraction: Hide implementation details from users\n\nRule of thumb: If you find yourself copying and pasting code more than twice, write a function!\n\n\n\n\nFunctions are created using the function() keyword:\n# A simple function that prints a greeting\nsay_hello &lt;- function() {\n  print(\"Hello, world!\")\n}\n\n# Call the function\nsay_hello()\n## [1] \"Hello, world!\"\nA function with a body that does computation:\n# Function to calculate the area of a circle\ncircle_area &lt;- function(radius) {\n  area &lt;- pi * radius^2\n  return(area)\n}\n\n# Use the function\ncircle_area(5)\n## [1] 78.53982\ncircle_area(10)\n## [1] 314.1593\n\n\n\n\nArguments are the inputs to your function. They let users customize the function’s behavior:\n# Function with multiple arguments\ngreet_person &lt;- function(name, greeting) {\n  message &lt;- paste(greeting, name)\n  print(message)\n}\n\ngreet_person(\"Alice\", \"Hello\")\n## [1] \"Hello Alice\"\ngreet_person(\"Bob\", \"Good morning\")\n## [1] \"Good morning Bob\"\n\n\n\n\nYou can set default values for arguments. This makes the function easier to use for common cases:\n# Function with default argument\ngreet_person &lt;- function(name, greeting = \"Hello\") {\n  message &lt;- paste(greeting, name)\n  print(message)\n}\n\n# Using default\ngreet_person(\"Alice\")\n## [1] \"Hello Alice\"\n# Overriding default\ngreet_person(\"Bob\", \"Good evening\")\n## [1] \"Good evening Bob\"\nAnother example:\n# Function to repeat a message\nrepeat_message &lt;- function(msg, times = 3) {\n  for (i in seq_len(times)) {\n    print(msg)\n  }\n}\n\nrepeat_message(\"R is fun!\")\n## [1] \"R is fun!\"\n## [1] \"R is fun!\"\n## [1] \"R is fun!\"\nrepeat_message(\"Learning loops!\", times = 2)\n## [1] \"Learning loops!\"\n## [1] \"Learning loops!\"\n\n\n\n\nFunctions return the last expression evaluated, or you can use return() explicitly:\n# Implicit return (last expression)\nadd_numbers &lt;- function(a, b) {\n  a + b  # This is returned\n}\n\nresult &lt;- add_numbers(3, 5)\nprint(result)\n## [1] 8\n# Explicit return\ncalculate_stats &lt;- function(numbers) {\n  if (length(numbers) == 0) {\n    return(NULL)  # Early return for edge case\n  }\n  \n  stats &lt;- list(\n    mean = mean(numbers),\n    sd = sd(numbers),\n    min = min(numbers),\n    max = max(numbers)\n  )\n  \n  return(stats)\n}\n\nmy_stats &lt;- calculate_stats(c(10, 20, 30, 40, 50))\nprint(my_stats)\n## $mean\n## [1] 30\n## \n## $sd\n## [1] 15.81139\n## \n## $min\n## [1] 10\n## \n## $max\n## [1] 50\n\n\n\n\nR matches arguments by position or by name:\n# Define a function\npower_calc &lt;- function(base, exponent) {\n  base^exponent\n}\n\n# Positional matching\npower_calc(2, 3)  # 2^3 = 8\n## [1] 8\n# Named matching\npower_calc(exponent = 3, base = 2)  # Same result\n## [1] 8\n# Mixed matching\npower_calc(2, exponent = 3)  # Same result\n## [1] 8\nTip: For functions with many arguments, use named arguments for clarity!\n\n\n\n\nWrite a function called fahrenheit_to_celsius that converts temperature from Fahrenheit to Celsius. Formula: C = (F - 32) * 5/9\nWrite a function called word_count that takes a text string and returns the number of words.\nWrite a function that takes a vector of numbers and returns a named list with the sum, mean, and length.\nModify your word_count function to have a default argument remove_punct = TRUE that removes punctuation before counting.\n\n### Your workspace\n\n\n\n\n\n\nNow let’s apply what we’ve learned to text analysis! The tidytext package provides tools for working with text in a tidy data format.\n\n\n\nTidy text format means having one token per row. A token can be:\n\nA word\nA sentence\nAn n-gram (sequence of n words)\nA paragraph\n\nThis format works seamlessly with tidyverse tools like dplyr and ggplot2.\nLoad the Jane Austen books dataset:\n# install.packages(\"janeaustenr\")\nlibrary(janeaustenr)\n\n# Combine all books into a single dataframe\noriginal_books &lt;- austen_books() |&gt; \n  group_by(book) |&gt; \n  mutate(\n    linenumber = row_number(),\n    chapter = cumsum(str_detect(text, \n                                regex(\"^chapter [\\\\divxlc]\",\n                                      ignore_case = TRUE)))\n  ) |&gt; \n  ungroup()\n\nhead(original_books)\n## # A tibble: 6 × 4\n##   text                    book                linenumber chapter\n##   &lt;chr&gt;                   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt;\n## 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility          1       0\n## 2 \"\"                      Sense & Sensibility          2       0\n## 3 \"by Jane Austen\"        Sense & Sensibility          3       0\n## 4 \"\"                      Sense & Sensibility          4       0\n## 5 \"(1811)\"                Sense & Sensibility          5       0\n## 6 \"\"                      Sense & Sensibility          6       0\n\n\n\n\nunnest_tokens() breaks text into individual tokens (usually words):\nSyntax:\nunnest_tokens(tbl, output, input, token = \"words\", ...)\n\n\n\nArgument\nDescription\n\n\n\n\ntbl\nThe data frame\n\n\noutput\nName of the new column for tokens\n\n\ninput\nName of the column containing text\n\n\ntoken\nType: “words”, “sentences”, “ngrams”, etc.\n\n\n\nTokenize the Jane Austen books:\ntidy_books &lt;- original_books |&gt; \n  unnest_tokens(word, text)\n\nhead(tidy_books)\n## # A tibble: 6 × 4\n##   book                linenumber chapter word       \n##   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      \n## 1 Sense & Sensibility          1       0 sense      \n## 2 Sense & Sensibility          1       0 and        \n## 3 Sense & Sensibility          1       0 sensibility\n## 4 Sense & Sensibility          3       0 by         \n## 5 Sense & Sensibility          3       0 jane       \n## 6 Sense & Sensibility          3       0 austen\nNotice that: - Punctuation is removed - Text is converted to lowercase - Each word is now its own row\nCount words:\ntidy_books |&gt; \n  count(word, sort = TRUE)\n## # A tibble: 14,520 × 2\n##    word      n\n##    &lt;chr&gt; &lt;int&gt;\n##  1 the   26351\n##  2 to    24044\n##  3 and   22515\n##  4 of    21178\n##  5 a     13408\n##  6 her   13055\n##  7 i     12006\n##  8 in    11217\n##  9 was   11204\n## 10 it    10234\n## # ℹ 14,510 more rows\n\n\n\n\nStopwords are common words like “the”, “and”, “of” that don’t carry much meaning. The tidytext package includes a stopwords dataset:\n# View stopwords\nhead(stop_words)\n## # A tibble: 6 × 2\n##   word      lexicon\n##   &lt;chr&gt;     &lt;chr&gt;  \n## 1 a         SMART  \n## 2 a's       SMART  \n## 3 able      SMART  \n## 4 about     SMART  \n## 5 above     SMART  \n## 6 according SMART\n# How many stopwords?\nnrow(stop_words)\n## [1] 1149\nRemove stopwords using anti_join():\ntidy_books_clean &lt;- tidy_books |&gt; \n  anti_join(stop_words, by = \"word\")\n\n# Compare counts\nnrow(tidy_books)        # Before\n## [1] 725055\nnrow(tidy_books_clean)  # After\n## [1] 217609\n# Most common words without stopwords\ntidy_books_clean |&gt; \n  count(word, sort = TRUE) |&gt; \n  head(15)\n## # A tibble: 15 × 2\n##    word          n\n##    &lt;chr&gt;     &lt;int&gt;\n##  1 miss       1855\n##  2 time       1337\n##  3 fanny       862\n##  4 dear        822\n##  5 lady        817\n##  6 sir         806\n##  7 day         797\n##  8 emma        787\n##  9 sister      727\n## 10 house       699\n## 11 elizabeth   687\n## 12 elinor      623\n## 13 hope        601\n## 14 friend      593\n## 15 family      578\n\n\n\nLoad the Starbucks Twitter data and practice tokenization:\nlibrary(readr)\n\nurl &lt;- \"https://raw.githubusercontent.com/aysedeniz09/Social_Media_Listening/refs/heads/main/MSC_social_media_list_data/Starbucks_User_Data.csv\"\nstarbucks_data &lt;- read_csv(url)\n\nhead(starbucks_data)\n## # A tibble: 6 × 16\n##   author_id conversation_id created_at          hashtag lang  like_count mention\n##       &lt;dbl&gt;           &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;  \n## 1     30973         1.61e18 2022-12-27 15:43:16 &lt;NA&gt;    en            10 &lt;NA&gt;   \n## 2     30973         1.60e18 2022-11-29 05:23:55 &lt;NA&gt;    en             9 Mo_sha…\n## 3     30973         1.59e18 2022-11-28 20:14:09 &lt;NA&gt;    en             2 Mixxed…\n## 4     30973         1.60e18 2022-11-28 12:51:28 &lt;NA&gt;    en             0 BihhKa…\n## 5     30973         1.60e18 2022-11-27 15:14:26 &lt;NA&gt;    en             0 BihhKa…\n## 6     30973         1.60e18 2022-11-24 17:47:24 &lt;NA&gt;    en             1 therea…\n## # ℹ 9 more variables: quote_count &lt;dbl&gt;, referenced_status_id &lt;dbl&gt;,\n## #   referenced_user_id &lt;dbl&gt;, reply_count &lt;dbl&gt;, retweet_count &lt;dbl&gt;,\n## #   row_id &lt;dbl&gt;, status_id &lt;dbl&gt;, text &lt;chr&gt;, type &lt;chr&gt;\n\nTokenize the text column into words.\nRemove stopwords.\nFind the 20 most common words.\nHow many total words are there before and after removing stopwords?\n\n### Your workspace\n\n\n\n\n\n\n\n\nKeyword in Context (KWIC) extracts and analyzes how specific keywords appear in text along with their surrounding context.\nWhy use KWIC?\n\nIdentify patterns in how words are used\nUnderstand the context around specific terms\nUseful for sentiment analysis and content understanding\n\n\n\n\n\nUse str_detect() to filter text containing a keyword:\n# Find all lines containing \"family\"\nfiltered_text &lt;- original_books |&gt; \n  filter(str_detect(text, \"family\"))\n\nhead(filtered_text)\n## # A tibble: 6 × 4\n##   text                                                  book  linenumber chapter\n##   &lt;chr&gt;                                                 &lt;fct&gt;      &lt;int&gt;   &lt;int&gt;\n## 1 The family of Dashwood had long been settled in Suss… Sens…         13       1\n## 2 into his house the family of his nephew Mr. Henry Da… Sens…         22       1\n## 3 family; but he was affected by a recommendation of s… Sens…         79       1\n## 4 any of her husband's family; but she had had no oppo… Sens…        116       1\n## 5 large a sum was parted with.  If he should have a nu… Sens…        223       2\n## 6 of her character, which half a year's residence in h… Sens…        398       3\nnrow(filtered_text)\n## [1] 572\n\n\n\n\nUse regex to extract characters before and after a keyword:\n# Extract 15 characters before and after \"family\"\ncontext &lt;- original_books |&gt;\n  filter(str_detect(text, \"family\")) |&gt;\n  mutate(context = str_extract(text, \".{0,15}family.{0,15}\"))\n\nhead(context$context, 10)\n##  [1] \"The family of Dashwood ha\"           \n##  [2] \" his house the family of his nephew \"\n##  [3] \"family; but he was af\"               \n##  [4] \" her husband's family; but she had h\"\n##  [5] \"ave a numerous family, for\"          \n##  [6] \"sidence in her family afforded;\"     \n##  [7] \" small for our family,\"              \n##  [8] \"terms with his family, and pressed\"  \n##  [9] \"than any other family in the neighbo\"\n## [10] \"rival of a new family in the country\"\nCreate a function for KWIC:\n# KWIC function\nkwic &lt;- function(data, text_col, keyword, window = 10) {\n  pattern &lt;- paste0(\".{0,\", window, \"}\", keyword, \".{0,\", window, \"}\")\n  \n  data |&gt;\n    filter(str_detect({{ text_col }}, keyword)) |&gt;\n    mutate(context = str_extract({{ text_col }}, pattern)) |&gt;\n    select(context)\n}\n\n# Use the function\nkwic(original_books, text, \"love\", window = 20) |&gt; \n  head(10)\n## # A tibble: 10 × 1\n##    context                                       \n##    &lt;chr&gt;                                         \n##  1 \" and her own tender love for all her three c\"\n##  2 \"e you are right, my love; it will be better \"\n##  3 \"ove far from that beloved spot was impossibl\"\n##  4 \"that he loved her daughter, and \"            \n##  5 \"rything amiable.  I love him already.\\\"\"     \n##  6 \"obation inferior to love.\\\"\"                 \n##  7 \"separate esteem and love.\\\"\"                 \n##  8 \"eive any symptom of love in his behaviour to\"\n##  9 \"\\\"My love, it will be scarcel\"               \n## 10 \"very amiable, and I love him tenderly.  But \"\n\n\n\nUsing the Starbucks data:\n\nFind all tweets containing “coffee”.\nExtract 20 characters of context around “coffee”.\nFind tweets containing mentions (@username) using regex.\nCreate a KWIC analysis for the word “Starbucks”.\n\n### Your workspace\n\n\n\n\n\n\n\n\nA Document-Term Matrix (DTM) is a mathematical representation of text where:\n\nRows represent documents (e.g., books, tweets, articles)\nColumns represent terms (words)\nValues indicate the frequency of each term in each document\n\nExample DTM:\n\n\n\n\nlove\nfamily\nmoney\nmarriage\nhappy\n\n\n\n\nBook 1\n45\n23\n12\n67\n34\n\n\nBook 2\n32\n45\n56\n23\n12\n\n\nBook 3\n67\n12\n8\n89\n45\n\n\n\nNote: A Term-Document Matrix (TDM) is simply the transpose - terms as rows, documents as columns. In R’s tidytext, we typically create DTMs.\n\n\n\n\nThe transformation from raw text to DTM follows these steps:\nRaw Text → Tokenize → Remove Stopwords → Count → DTM\nStep-by-step visualization:\n# STEP 1: Start with raw text\noriginal_books |&gt; \n  select(book, text) |&gt; \n  head(3)\n## # A tibble: 3 × 2\n##   book                text                   \n##   &lt;fct&gt;               &lt;chr&gt;                  \n## 1 Sense & Sensibility \"SENSE AND SENSIBILITY\"\n## 2 Sense & Sensibility \"\"                     \n## 3 Sense & Sensibility \"by Jane Austen\"\n# STEP 2: Tokenize (one word per row)\ntokenized &lt;- original_books |&gt; \n  unnest_tokens(word, text)\n\ntokenized |&gt; \n  select(book, word) |&gt; \n  head(10)\n## # A tibble: 10 × 2\n##    book                word       \n##    &lt;fct&gt;               &lt;chr&gt;      \n##  1 Sense & Sensibility sense      \n##  2 Sense & Sensibility and        \n##  3 Sense & Sensibility sensibility\n##  4 Sense & Sensibility by         \n##  5 Sense & Sensibility jane       \n##  6 Sense & Sensibility austen     \n##  7 Sense & Sensibility 1811       \n##  8 Sense & Sensibility chapter    \n##  9 Sense & Sensibility 1          \n## 10 Sense & Sensibility the\n# STEP 3: Remove stopwords\ncleaned &lt;- tokenized |&gt; \n  anti_join(stop_words, by = \"word\")\n\ncleaned |&gt; \n  select(book, word) |&gt; \n  head(10)\n## # A tibble: 10 × 2\n##    book                word       \n##    &lt;fct&gt;               &lt;chr&gt;      \n##  1 Sense & Sensibility sense      \n##  2 Sense & Sensibility sensibility\n##  3 Sense & Sensibility jane       \n##  4 Sense & Sensibility austen     \n##  5 Sense & Sensibility 1811       \n##  6 Sense & Sensibility chapter    \n##  7 Sense & Sensibility 1          \n##  8 Sense & Sensibility family     \n##  9 Sense & Sensibility dashwood   \n## 10 Sense & Sensibility settled\n# STEP 4: Count words per document\nword_counts &lt;- cleaned |&gt; \n  count(book, word, sort = TRUE)\n\nhead(word_counts, 10)\n## # A tibble: 10 × 3\n##    book                word          n\n##    &lt;fct&gt;               &lt;chr&gt;     &lt;int&gt;\n##  1 Mansfield Park      fanny       816\n##  2 Emma                emma        786\n##  3 Sense & Sensibility elinor      623\n##  4 Emma                miss        599\n##  5 Pride & Prejudice   elizabeth   597\n##  6 Mansfield Park      crawford    493\n##  7 Sense & Sensibility marianne    492\n##  8 Persuasion          anne        447\n##  9 Mansfield Park      miss        432\n## 10 Northanger Abbey    catherine   428\n\n\n\n\nUse cast_dtm() to convert tidy word counts to a DTM:\n# Create DTM\nbook_dtm &lt;- word_counts |&gt; \n  cast_dtm(document = book, term = word, value = n)\n\n# Inspect the DTM\nbook_dtm\n## &lt;&lt;DocumentTermMatrix (documents: 6, terms: 13914)&gt;&gt;\n## Non-/sparse entries: 37224/46260\n## Sparsity           : 55%\n## Maximal term length: 19\n## Weighting          : term frequency (tf)\n# View dimensions: documents x terms\ndim(book_dtm)\n## [1]     6 13914\nUnderstanding the output: - 6 documents (the 6 Jane Austen books) - 13,914 terms (unique words across all books) - 99% sparse means 99% of the cells are zeros (most words don’t appear in most books)\n\n\n\n\nLet’s visualize a small portion of the DTM to understand its structure:\n# Get top 10 words overall\ntop_10_words &lt;- word_counts |&gt; \n  group_by(word) |&gt; \n  summarize(total = sum(n)) |&gt; \n  slice_max(total, n = 10) |&gt; \n  pull(word)\n\n# Filter to just these words\ndtm_subset &lt;- word_counts |&gt; \n  filter(word %in% top_10_words)\n\n# Create heatmap\nggplot(dtm_subset, aes(x = word, y = book, fill = n)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = n), color = \"white\", size = 3) +\n  scale_fill_gradient(low = \"steelblue\", high = \"darkred\") +\n  labs(\n    title = \"Document-Term Matrix Heatmap\",\n    subtitle = \"Top 10 words across Jane Austen books\",\n    x = \"Term\",\n    y = \"Document\",\n    fill = \"Frequency\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nReading the heatmap: - Darker colors = higher frequency - Each row is a book (document) - Each column is a word (term) - The numbers show exact word counts\n\n\n\n\nTop words per book:\ntop_words &lt;- word_counts |&gt; \n  group_by(book) |&gt; \n  slice_max(n, n = 5) |&gt; \n  ungroup()\n\nhead(top_words, 12)\n## # A tibble: 12 × 3\n##    book                word          n\n##    &lt;fct&gt;               &lt;chr&gt;     &lt;int&gt;\n##  1 Sense & Sensibility elinor      623\n##  2 Sense & Sensibility marianne    492\n##  3 Sense & Sensibility time        239\n##  4 Sense & Sensibility dashwood    231\n##  5 Sense & Sensibility sister      229\n##  6 Pride & Prejudice   elizabeth   597\n##  7 Pride & Prejudice   darcy       373\n##  8 Pride & Prejudice   bennet      294\n##  9 Pride & Prejudice   miss        283\n## 10 Pride & Prejudice   jane        264\n## 11 Mansfield Park      fanny       816\n## 12 Mansfield Park      crawford    493\nBar chart visualization:\nggplot(top_words, aes(x = reorder(word, n), y = n, fill = book)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  facet_wrap(~ book, scales = \"free_y\") +\n  labs(\n    title = \"Top 5 Words in Each Jane Austen Book\",\n    x = \"Word\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nYou can convert a DTM back to tidy format using tidy():\n# Convert DTM back to tidy\ntidy_dtm &lt;- tidy(book_dtm)\n\nhead(tidy_dtm)\n## # A tibble: 6 × 3\n##   document            term   count\n##   &lt;chr&gt;               &lt;chr&gt;  &lt;dbl&gt;\n## 1 Sense & Sensibility fanny     42\n## 2 Mansfield Park      fanny    816\n## 3 Persuasion          fanny      4\n## 4 Emma                emma     786\n## 5 Persuasion          emma       1\n## 6 Sense & Sensibility elinor   623\nThis is useful when you receive a DTM from another package and want to use tidyverse tools.\n\n\n\n\n\n\n\nTF-IDF (Term Frequency - Inverse Document Frequency) measures how important a word is to a document within a collection.\n\nTF (Term Frequency): How often a term appears in a document\nIDF (Inverse Document Frequency): How rare a term is across all documents\n\n\\[\\text{TF-IDF} = \\text{TF} \\times \\log\\left(\\frac{\\text{Total Documents}}{\\text{Documents containing term}}\\right)\\]\nInterpretation:\n\nHigh TF-IDF: Word is frequent in this document but rare overall → important/distinctive\nLow TF-IDF: Word is common everywhere → less distinctive\n\n\n\n\n\nUse bind_tf_idf() from tidytext:\n# Calculate TF-IDF\nbook_tfidf &lt;- original_books |&gt; \n  unnest_tokens(word, text) |&gt; \n  count(book, word, sort = TRUE) |&gt; \n  bind_tf_idf(word, book, n)\n\nhead(book_tfidf)\n## # A tibble: 6 × 6\n##   book           word      n     tf   idf tf_idf\n##   &lt;fct&gt;          &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n## 1 Mansfield Park the    6206 0.0387     0      0\n## 2 Mansfield Park to     5475 0.0341     0      0\n## 3 Mansfield Park and    5438 0.0339     0      0\n## 4 Emma           to     5239 0.0325     0      0\n## 5 Emma           the    5201 0.0323     0      0\n## 6 Emma           and    4896 0.0304     0      0\nFind distinctive words for each book:\n# Top TF-IDF words per book\ntop_tfidf &lt;- book_tfidf |&gt; \n  group_by(book) |&gt; \n  slice_max(tf_idf, n = 5) |&gt; \n  ungroup()\n\nhead(top_tfidf, 12)\n## # A tibble: 12 × 6\n##    book                word           n      tf   idf  tf_idf\n##    &lt;fct&gt;               &lt;chr&gt;      &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n##  1 Sense & Sensibility elinor       623 0.00519 1.79  0.00931\n##  2 Sense & Sensibility marianne     492 0.00410 1.79  0.00735\n##  3 Sense & Sensibility dashwood     231 0.00193 1.79  0.00345\n##  4 Sense & Sensibility jennings     199 0.00166 1.79  0.00297\n##  5 Sense & Sensibility willoughby   181 0.00151 1.79  0.00270\n##  6 Pride & Prejudice   darcy        373 0.00305 1.79  0.00547\n##  7 Pride & Prejudice   bennet       294 0.00241 1.79  0.00431\n##  8 Pride & Prejudice   bingley      257 0.00210 1.79  0.00377\n##  9 Pride & Prejudice   elizabeth    597 0.00489 0.693 0.00339\n## 10 Pride & Prejudice   wickham      162 0.00133 1.79  0.00238\n## 11 Mansfield Park      crawford     493 0.00307 1.79  0.00551\n## 12 Mansfield Park      edmund       364 0.00227 1.79  0.00406\n\n\n\n\nggplot(top_tfidf, aes(x = reorder(word, tf_idf), y = tf_idf, fill = book)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  facet_wrap(~ book, scales = \"free\") +\n  labs(\n    title = \"Most Distinctive Words by TF-IDF\",\n    x = \"Word\",\n    y = \"TF-IDF Score\"\n  ) +\n  theme_minimal()\n\nNotice that TF-IDF highlights character names and distinctive terms for each book, rather than common words!\n\n\n\nUsing the Starbucks data:\n\nCreate word counts grouped by mention (the user being replied to).\nCalculate TF-IDF scores.\nFind the top 5 distinctive words for the 3 most active mentions.\nVisualize the results.\n\n### Your workspace\n\n\n\n\n\n\n\n\nWord clouds display words with size proportional to their frequency. Install the wordcloud2 package:\ninstall.packages(\"wordcloud2\")\nlibrary(wordcloud2)\n\n# Prepare word frequencies\nword_freq &lt;- original_books |&gt;\n  unnest_tokens(word, text) |&gt;\n  anti_join(stop_words, by = \"word\") |&gt;\n  count(word, sort = TRUE)\n\nhead(word_freq)\n## # A tibble: 6 × 2\n##   word      n\n##   &lt;chr&gt; &lt;int&gt;\n## 1 miss   1855\n## 2 time   1337\n## 3 fanny   862\n## 4 dear    822\n## 5 lady    817\n## 6 sir     806\nCreate a basic word cloud:\nwordcloud2(data = word_freq, size = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nDescription\nExample\n\n\n\n\nsize\nScale factor for word sizes\nsize = 0.5\n\n\ncolor\nColor scheme\ncolor = \"random-light\"\n\n\nbackgroundColor\nBackground color\nbackgroundColor = \"black\"\n\n\nshape\nShape of cloud\nshape = \"circle\" or \"star\"\n\n\nminRotation, maxRotation\nWord rotation angles\nminRotation = -pi/4\n\n\n\nCustom colors:\nwordcloud2(data = word_freq, size = 0.5, color = \"random-light\")\n\n\n\n\nChange shape and background:\nwordcloud2(data = word_freq, size = 0.4, shape = \"star\", backgroundColor = \"black\", color = \"random-light\")\n\n\n\n\nWith rotation:\nwordcloud2(data = word_freq, size = 0.5, \n           minRotation = -pi/4, maxRotation = pi/4, rotateRatio = 0.3)\n\n\n\n\n\n\n\nlibrary(htmlwidgets)\nlibrary(webshot)\nwebshot::install_phantomjs()\n\n# Create word cloud\nmy_cloud &lt;- wordcloud2(word_freq, size = 1)\n\n# Save as HTML\nsaveWidget(my_cloud, \"wordcloud.html\", selfcontained = FALSE)\n\n# Save as image\nwebshot(\"wordcloud.html\", \"wordcloud.png\", delay = 5)\n\n\n\n\n\n\n\n\nUsing either the Jane Austen or Starbucks dataset:\n\nWrite a function that takes a data frame and text column, then returns:\n\nTotal word count (after removing stopwords)\nTop 10 most frequent words\nA word cloud object\n\nUse a loop to analyze each book (or group) separately and store the results in a list.\nCreate a KWIC analysis for a keyword of your choice.\nCalculate TF-IDF and identify what makes each document/group distinctive.\nVisualize your findings with at least two different plot types.\n\n### Your workspace\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic\nDescription\nCode Example\n\n\n\n\nif-else\nConditional execution\nif (x &gt; 5) { ... } else { ... }\n\n\nfor loop\nFixed iterations\nfor (i in 1:10) { print(i) }\n\n\nwhile loop\nConditional iterations\nwhile (x &lt; 10) { x &lt;- x + 1 }\n\n\nseq_along()\nSafe sequence for loops\nfor (i in seq_along(vec)) { ... }\n\n\nbreak\nExit loop\nif (cond) break\n\n\nnext\nSkip iteration\nif (cond) next\n\n\nfunction()\nCreate function\nmy_func &lt;- function(arg) { ... }\n\n\nDefault arguments\nSet defaults\nfunction(x, y = 10) { ... }\n\n\nreturn()\nExplicit return\nreturn(result)\n\n\nunnest_tokens()\nTokenize text\nunnest_tokens(word, text)\n\n\nanti_join(stop_words)\nRemove stopwords\ndata |&gt; anti_join(stop_words)\n\n\nstr_detect()\nFind pattern\nfilter(str_detect(text, \"word\"))\n\n\nstr_extract()\nExtract pattern\nmutate(x = str_extract(text, \".{10}word.{10}\"))\n\n\nKWIC\nKeyword in context\nExtract surrounding text for keywords\n\n\ncast_dtm()\nCreate DTM from tidy\ncast_dtm(document, term, value)\n\n\ntidy()\nConvert DTM to tidy\ntidy(dtm_object)\n\n\ngeom_tile()\nCreate heatmap\ngeom_tile(aes(x, y, fill = value))\n\n\nbind_tf_idf()\nCalculate TF-IDF\nbind_tf_idf(word, document, n)\n\n\nwordcloud2()\nCreate word cloud\nwordcloud2(data = freq_df, size = 0.5)\n\n\ncount()\nCount occurrences\ncount(word, sort = TRUE)\n\n\nslice_max()\nTop n by value\nslice_max(n, n = 10)"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#lecture-4-table-of-contents",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#lecture-4-table-of-contents",
    "title": "Computational Research",
    "section": "",
    "text": "Section\nTopic\n\n\n\n\n1\nControl Structures\n\n\n1.1\nif-else Statements\n\n\n1.2\nfor Loops\n\n\n1.3\nLooping Over Vectors\n\n\n1.4\nNested Loops\n\n\n1.5\nwhile Loops\n\n\n1.6\nbreak and next\n\n\n2\nFunctions\n\n\n2.1\nWhy Write Functions?\n\n\n2.2\nCreating Your First Function\n\n\n2.3\nFunction Arguments\n\n\n2.4\nDefault Arguments\n\n\n2.5\nReturn Values\n\n\n2.6\nArgument Matching\n\n\n3\nText Analysis with TidyText\n\n\n3.1\nIntroduction to TidyText\n\n\n3.2\nTokenizing Text with unnest_tokens()\n\n\n3.3\nRemoving Stopwords\n\n\n4\nKeyword in Context (KWIC)\n\n\n4.1\nWhat is KWIC?\n\n\n4.2\nExtracting Keywords\n\n\n4.3\nExtracting Surrounding Context\n\n\n5\nDocument-Term Matrix (DTM)\n\n\n5.1\nWhat is a DTM?\n\n\n5.2\nFrom Text to DTM: The Pipeline\n\n\n5.3\nCreating the DTM\n\n\n5.4\nVisualizing the DTM as a Heatmap\n\n\n5.5\nVisualizing Top Words per Document\n\n\n5.6\nConverting DTM Back to Tidy Format\n\n\n6\nTF-IDF\n\n\n6.1\nUnderstanding TF-IDF\n\n\n6.2\nCalculating TF-IDF\n\n\n6.3\nVisualizing TF-IDF\n\n\n7\nWord Clouds\n\n\n7.1\nCreating Word Clouds\n\n\n7.2\nCustomizing Word Clouds\n\n\n\n\nALWAYS load our libraries first\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(ggplot2)"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#control-structures",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#control-structures",
    "title": "Computational Research",
    "section": "",
    "text": "Control structures allow you to control the flow of execution in your R code. Instead of running the same code every time, you can add logic to respond to different inputs or data conditions.\nCommon control structures include:\n\n\n\nStructure\nPurpose\n\n\n\n\nif, else\nTest a condition and act on it\n\n\nfor\nExecute a loop a fixed number of times\n\n\nwhile\nExecute a loop while a condition is true\n\n\nbreak\nExit a loop immediately\n\n\nnext\nSkip to the next iteration of a loop\n\n\n\n\n\n\nThe if-else combination tests a condition and executes different code depending on whether it’s TRUE or FALSE.\nBasic if statement:\nx &lt;- 7\n\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n}\n## [1] \"x is greater than 5\"\nif-else statement:\nx &lt;- 3\n\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n} else {\n  print(\"x is NOT greater than 5\")\n}\n## [1] \"x is NOT greater than 5\"\nMultiple conditions with else if:\nx &lt;- 5\n\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n} else if (x == 5) {\n  print(\"x is exactly 5\")\n} else {\n  print(\"x is less than 5\")\n}\n## [1] \"x is exactly 5\"\nCompact if-else (single line):\nx &lt;- 8\ny &lt;- if (x &gt; 5) \"big\" else \"small\"\nprint(y)\n## [1] \"big\"\n\n\n\n\nfor loops iterate over elements in a sequence (vector, list, etc.) and execute code for each element.\nBasic for loop:\nfor (i in 1:5) {\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\nLooping over a character vector:\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\")\n\nfor (fruit in fruits) {\n  print(paste(\"I like\", fruit))\n}\n## [1] \"I like apple\"\n## [1] \"I like banana\"\n## [1] \"I like cherry\"\n\n\n\n\nThere are multiple ways to loop over vectors. The seq_along() function is particularly useful:\ncolors &lt;- c(\"red\", \"green\", \"blue\", \"yellow\")\n\n# Method 1: Loop over indices\nfor (i in 1:length(colors)) {\n  print(paste(\"Color\", i, \"is\", colors[i]))\n}\n## [1] \"Color 1 is red\"\n## [1] \"Color 2 is green\"\n## [1] \"Color 3 is blue\"\n## [1] \"Color 4 is yellow\"\n# Method 2: Using seq_along() - safer!\nfor (i in seq_along(colors)) {\n  print(paste(\"Color\", i, \"is\", colors[i]))\n}\n## [1] \"Color 1 is red\"\n## [1] \"Color 2 is green\"\n## [1] \"Color 3 is blue\"\n## [1] \"Color 4 is yellow\"\n# Method 3: Loop directly over elements\nfor (color in colors) {\n  print(color)\n}\n## [1] \"red\"\n## [1] \"green\"\n## [1] \"blue\"\n## [1] \"yellow\"\nWhy use seq_along()? It’s safer because if the vector is empty, 1:length(x) would give 1:0 which creates c(1, 0), but seq_along(x) correctly returns an empty sequence.\n\n\n\n\nLoops can be nested inside each other. This is useful for working with matrices or multidimensional data:\n# Create a 3x3 matrix\nmat &lt;- matrix(1:9, nrow = 3, ncol = 3)\nprint(mat)\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n# Loop through rows and columns\nfor (i in 1:nrow(mat)) {\n  for (j in 1:ncol(mat)) {\n    print(paste(\"Row\", i, \"Col\", j, \"=\", mat[i, j]))\n  }\n}\n## [1] \"Row 1 Col 1 = 1\"\n## [1] \"Row 1 Col 2 = 4\"\n## [1] \"Row 1 Col 3 = 7\"\n## [1] \"Row 2 Col 1 = 2\"\n## [1] \"Row 2 Col 2 = 5\"\n## [1] \"Row 2 Col 3 = 8\"\n## [1] \"Row 3 Col 1 = 3\"\n## [1] \"Row 3 Col 2 = 6\"\n## [1] \"Row 3 Col 3 = 9\"\nWarning: Avoid nesting more than 2-3 levels deep. If you need more, consider using functions to break up the code.\n\n\n\n\nwhile loops execute as long as a condition is TRUE:\ncount &lt;- 1\n\nwhile (count &lt;= 5) {\n  print(paste(\"Count is:\", count))\n  count &lt;- count + 1\n}\n## [1] \"Count is: 1\"\n## [1] \"Count is: 2\"\n## [1] \"Count is: 3\"\n## [1] \"Count is: 4\"\n## [1] \"Count is: 5\"\nCaution: while loops can run forever if the condition never becomes FALSE. Always make sure your loop has a way to exit!\nExample with multiple conditions:\nset.seed(123)  # For reproducibility\nvalue &lt;- 5\n\nwhile (value &gt;= 2 && value &lt;= 8) {\n  # Random walk: add or subtract 1\n  coin &lt;- sample(c(-1, 1), 1)\n  value &lt;- value + coin\n  print(paste(\"Value is now:\", value))\n}\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 2\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 2\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 6\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 6\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 6\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 2\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 2\"\n## [1] \"Value is now: 1\"\n\n\n\n\n\nbreak: Exit the loop immediately\nnext: Skip the current iteration and continue to the next\n\nUsing break:\nfor (i in 1:10) {\n  if (i &gt; 5) {\n    print(\"Breaking out of loop!\")\n    break\n  }\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n## [1] \"Breaking out of loop!\"\nUsing next:\n# Print only odd numbers\nfor (i in 1:10) {\n  if (i %% 2 == 0) {  # If i is even, skip it\n    next\n  }\n  print(i)\n}\n## [1] 1\n## [1] 3\n## [1] 5\n## [1] 7\n## [1] 9\n\n\n\n\nWrite an if-else statement that checks if a number is positive, negative, or zero.\nCreate a for loop that prints the squares of numbers 1 through 10.\nWrite a while loop that starts at 100 and divides by 2 until the value is less than 1.\nUse a for loop with next to print only numbers divisible by 3 from 1 to 20.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#functions",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#functions",
    "title": "Computational Research",
    "section": "",
    "text": "Functions allow you to encapsulate code that you want to reuse. Instead of copying and pasting code, you write it once as a function and call it whenever needed.\n\n\n\n\nReusability: Write code once, use it many times\nReadability: Give meaningful names to complex operations\nMaintainability: Fix bugs in one place instead of many\nAbstraction: Hide implementation details from users\n\nRule of thumb: If you find yourself copying and pasting code more than twice, write a function!\n\n\n\n\nFunctions are created using the function() keyword:\n# A simple function that prints a greeting\nsay_hello &lt;- function() {\n  print(\"Hello, world!\")\n}\n\n# Call the function\nsay_hello()\n## [1] \"Hello, world!\"\nA function with a body that does computation:\n# Function to calculate the area of a circle\ncircle_area &lt;- function(radius) {\n  area &lt;- pi * radius^2\n  return(area)\n}\n\n# Use the function\ncircle_area(5)\n## [1] 78.53982\ncircle_area(10)\n## [1] 314.1593\n\n\n\n\nArguments are the inputs to your function. They let users customize the function’s behavior:\n# Function with multiple arguments\ngreet_person &lt;- function(name, greeting) {\n  message &lt;- paste(greeting, name)\n  print(message)\n}\n\ngreet_person(\"Alice\", \"Hello\")\n## [1] \"Hello Alice\"\ngreet_person(\"Bob\", \"Good morning\")\n## [1] \"Good morning Bob\"\n\n\n\n\nYou can set default values for arguments. This makes the function easier to use for common cases:\n# Function with default argument\ngreet_person &lt;- function(name, greeting = \"Hello\") {\n  message &lt;- paste(greeting, name)\n  print(message)\n}\n\n# Using default\ngreet_person(\"Alice\")\n## [1] \"Hello Alice\"\n# Overriding default\ngreet_person(\"Bob\", \"Good evening\")\n## [1] \"Good evening Bob\"\nAnother example:\n# Function to repeat a message\nrepeat_message &lt;- function(msg, times = 3) {\n  for (i in seq_len(times)) {\n    print(msg)\n  }\n}\n\nrepeat_message(\"R is fun!\")\n## [1] \"R is fun!\"\n## [1] \"R is fun!\"\n## [1] \"R is fun!\"\nrepeat_message(\"Learning loops!\", times = 2)\n## [1] \"Learning loops!\"\n## [1] \"Learning loops!\"\n\n\n\n\nFunctions return the last expression evaluated, or you can use return() explicitly:\n# Implicit return (last expression)\nadd_numbers &lt;- function(a, b) {\n  a + b  # This is returned\n}\n\nresult &lt;- add_numbers(3, 5)\nprint(result)\n## [1] 8\n# Explicit return\ncalculate_stats &lt;- function(numbers) {\n  if (length(numbers) == 0) {\n    return(NULL)  # Early return for edge case\n  }\n  \n  stats &lt;- list(\n    mean = mean(numbers),\n    sd = sd(numbers),\n    min = min(numbers),\n    max = max(numbers)\n  )\n  \n  return(stats)\n}\n\nmy_stats &lt;- calculate_stats(c(10, 20, 30, 40, 50))\nprint(my_stats)\n## $mean\n## [1] 30\n## \n## $sd\n## [1] 15.81139\n## \n## $min\n## [1] 10\n## \n## $max\n## [1] 50\n\n\n\n\nR matches arguments by position or by name:\n# Define a function\npower_calc &lt;- function(base, exponent) {\n  base^exponent\n}\n\n# Positional matching\npower_calc(2, 3)  # 2^3 = 8\n## [1] 8\n# Named matching\npower_calc(exponent = 3, base = 2)  # Same result\n## [1] 8\n# Mixed matching\npower_calc(2, exponent = 3)  # Same result\n## [1] 8\nTip: For functions with many arguments, use named arguments for clarity!\n\n\n\n\nWrite a function called fahrenheit_to_celsius that converts temperature from Fahrenheit to Celsius. Formula: C = (F - 32) * 5/9\nWrite a function called word_count that takes a text string and returns the number of words.\nWrite a function that takes a vector of numbers and returns a named list with the sum, mean, and length.\nModify your word_count function to have a default argument remove_punct = TRUE that removes punctuation before counting.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#text-analysis-with-tidytext",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#text-analysis-with-tidytext",
    "title": "Computational Research",
    "section": "",
    "text": "Now let’s apply what we’ve learned to text analysis! The tidytext package provides tools for working with text in a tidy data format.\n\n\n\nTidy text format means having one token per row. A token can be:\n\nA word\nA sentence\nAn n-gram (sequence of n words)\nA paragraph\n\nThis format works seamlessly with tidyverse tools like dplyr and ggplot2.\nLoad the Jane Austen books dataset:\n# install.packages(\"janeaustenr\")\nlibrary(janeaustenr)\n\n# Combine all books into a single dataframe\noriginal_books &lt;- austen_books() |&gt; \n  group_by(book) |&gt; \n  mutate(\n    linenumber = row_number(),\n    chapter = cumsum(str_detect(text, \n                                regex(\"^chapter [\\\\divxlc]\",\n                                      ignore_case = TRUE)))\n  ) |&gt; \n  ungroup()\n\nhead(original_books)\n## # A tibble: 6 × 4\n##   text                    book                linenumber chapter\n##   &lt;chr&gt;                   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt;\n## 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility          1       0\n## 2 \"\"                      Sense & Sensibility          2       0\n## 3 \"by Jane Austen\"        Sense & Sensibility          3       0\n## 4 \"\"                      Sense & Sensibility          4       0\n## 5 \"(1811)\"                Sense & Sensibility          5       0\n## 6 \"\"                      Sense & Sensibility          6       0\n\n\n\n\nunnest_tokens() breaks text into individual tokens (usually words):\nSyntax:\nunnest_tokens(tbl, output, input, token = \"words\", ...)\n\n\n\nArgument\nDescription\n\n\n\n\ntbl\nThe data frame\n\n\noutput\nName of the new column for tokens\n\n\ninput\nName of the column containing text\n\n\ntoken\nType: “words”, “sentences”, “ngrams”, etc.\n\n\n\nTokenize the Jane Austen books:\ntidy_books &lt;- original_books |&gt; \n  unnest_tokens(word, text)\n\nhead(tidy_books)\n## # A tibble: 6 × 4\n##   book                linenumber chapter word       \n##   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      \n## 1 Sense & Sensibility          1       0 sense      \n## 2 Sense & Sensibility          1       0 and        \n## 3 Sense & Sensibility          1       0 sensibility\n## 4 Sense & Sensibility          3       0 by         \n## 5 Sense & Sensibility          3       0 jane       \n## 6 Sense & Sensibility          3       0 austen\nNotice that: - Punctuation is removed - Text is converted to lowercase - Each word is now its own row\nCount words:\ntidy_books |&gt; \n  count(word, sort = TRUE)\n## # A tibble: 14,520 × 2\n##    word      n\n##    &lt;chr&gt; &lt;int&gt;\n##  1 the   26351\n##  2 to    24044\n##  3 and   22515\n##  4 of    21178\n##  5 a     13408\n##  6 her   13055\n##  7 i     12006\n##  8 in    11217\n##  9 was   11204\n## 10 it    10234\n## # ℹ 14,510 more rows\n\n\n\n\nStopwords are common words like “the”, “and”, “of” that don’t carry much meaning. The tidytext package includes a stopwords dataset:\n# View stopwords\nhead(stop_words)\n## # A tibble: 6 × 2\n##   word      lexicon\n##   &lt;chr&gt;     &lt;chr&gt;  \n## 1 a         SMART  \n## 2 a's       SMART  \n## 3 able      SMART  \n## 4 about     SMART  \n## 5 above     SMART  \n## 6 according SMART\n# How many stopwords?\nnrow(stop_words)\n## [1] 1149\nRemove stopwords using anti_join():\ntidy_books_clean &lt;- tidy_books |&gt; \n  anti_join(stop_words, by = \"word\")\n\n# Compare counts\nnrow(tidy_books)        # Before\n## [1] 725055\nnrow(tidy_books_clean)  # After\n## [1] 217609\n# Most common words without stopwords\ntidy_books_clean |&gt; \n  count(word, sort = TRUE) |&gt; \n  head(15)\n## # A tibble: 15 × 2\n##    word          n\n##    &lt;chr&gt;     &lt;int&gt;\n##  1 miss       1855\n##  2 time       1337\n##  3 fanny       862\n##  4 dear        822\n##  5 lady        817\n##  6 sir         806\n##  7 day         797\n##  8 emma        787\n##  9 sister      727\n## 10 house       699\n## 11 elizabeth   687\n## 12 elinor      623\n## 13 hope        601\n## 14 friend      593\n## 15 family      578\n\n\n\nLoad the Starbucks Twitter data and practice tokenization:\nlibrary(readr)\n\nurl &lt;- \"https://raw.githubusercontent.com/aysedeniz09/Social_Media_Listening/refs/heads/main/MSC_social_media_list_data/Starbucks_User_Data.csv\"\nstarbucks_data &lt;- read_csv(url)\n\nhead(starbucks_data)\n## # A tibble: 6 × 16\n##   author_id conversation_id created_at          hashtag lang  like_count mention\n##       &lt;dbl&gt;           &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;  \n## 1     30973         1.61e18 2022-12-27 15:43:16 &lt;NA&gt;    en            10 &lt;NA&gt;   \n## 2     30973         1.60e18 2022-11-29 05:23:55 &lt;NA&gt;    en             9 Mo_sha…\n## 3     30973         1.59e18 2022-11-28 20:14:09 &lt;NA&gt;    en             2 Mixxed…\n## 4     30973         1.60e18 2022-11-28 12:51:28 &lt;NA&gt;    en             0 BihhKa…\n## 5     30973         1.60e18 2022-11-27 15:14:26 &lt;NA&gt;    en             0 BihhKa…\n## 6     30973         1.60e18 2022-11-24 17:47:24 &lt;NA&gt;    en             1 therea…\n## # ℹ 9 more variables: quote_count &lt;dbl&gt;, referenced_status_id &lt;dbl&gt;,\n## #   referenced_user_id &lt;dbl&gt;, reply_count &lt;dbl&gt;, retweet_count &lt;dbl&gt;,\n## #   row_id &lt;dbl&gt;, status_id &lt;dbl&gt;, text &lt;chr&gt;, type &lt;chr&gt;\n\nTokenize the text column into words.\nRemove stopwords.\nFind the 20 most common words.\nHow many total words are there before and after removing stopwords?\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#keyword-in-context-kwic",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#keyword-in-context-kwic",
    "title": "Computational Research",
    "section": "",
    "text": "Keyword in Context (KWIC) extracts and analyzes how specific keywords appear in text along with their surrounding context.\nWhy use KWIC?\n\nIdentify patterns in how words are used\nUnderstand the context around specific terms\nUseful for sentiment analysis and content understanding\n\n\n\n\n\nUse str_detect() to filter text containing a keyword:\n# Find all lines containing \"family\"\nfiltered_text &lt;- original_books |&gt; \n  filter(str_detect(text, \"family\"))\n\nhead(filtered_text)\n## # A tibble: 6 × 4\n##   text                                                  book  linenumber chapter\n##   &lt;chr&gt;                                                 &lt;fct&gt;      &lt;int&gt;   &lt;int&gt;\n## 1 The family of Dashwood had long been settled in Suss… Sens…         13       1\n## 2 into his house the family of his nephew Mr. Henry Da… Sens…         22       1\n## 3 family; but he was affected by a recommendation of s… Sens…         79       1\n## 4 any of her husband's family; but she had had no oppo… Sens…        116       1\n## 5 large a sum was parted with.  If he should have a nu… Sens…        223       2\n## 6 of her character, which half a year's residence in h… Sens…        398       3\nnrow(filtered_text)\n## [1] 572\n\n\n\n\nUse regex to extract characters before and after a keyword:\n# Extract 15 characters before and after \"family\"\ncontext &lt;- original_books |&gt;\n  filter(str_detect(text, \"family\")) |&gt;\n  mutate(context = str_extract(text, \".{0,15}family.{0,15}\"))\n\nhead(context$context, 10)\n##  [1] \"The family of Dashwood ha\"           \n##  [2] \" his house the family of his nephew \"\n##  [3] \"family; but he was af\"               \n##  [4] \" her husband's family; but she had h\"\n##  [5] \"ave a numerous family, for\"          \n##  [6] \"sidence in her family afforded;\"     \n##  [7] \" small for our family,\"              \n##  [8] \"terms with his family, and pressed\"  \n##  [9] \"than any other family in the neighbo\"\n## [10] \"rival of a new family in the country\"\nCreate a function for KWIC:\n# KWIC function\nkwic &lt;- function(data, text_col, keyword, window = 10) {\n  pattern &lt;- paste0(\".{0,\", window, \"}\", keyword, \".{0,\", window, \"}\")\n  \n  data |&gt;\n    filter(str_detect({{ text_col }}, keyword)) |&gt;\n    mutate(context = str_extract({{ text_col }}, pattern)) |&gt;\n    select(context)\n}\n\n# Use the function\nkwic(original_books, text, \"love\", window = 20) |&gt; \n  head(10)\n## # A tibble: 10 × 1\n##    context                                       \n##    &lt;chr&gt;                                         \n##  1 \" and her own tender love for all her three c\"\n##  2 \"e you are right, my love; it will be better \"\n##  3 \"ove far from that beloved spot was impossibl\"\n##  4 \"that he loved her daughter, and \"            \n##  5 \"rything amiable.  I love him already.\\\"\"     \n##  6 \"obation inferior to love.\\\"\"                 \n##  7 \"separate esteem and love.\\\"\"                 \n##  8 \"eive any symptom of love in his behaviour to\"\n##  9 \"\\\"My love, it will be scarcel\"               \n## 10 \"very amiable, and I love him tenderly.  But \"\n\n\n\nUsing the Starbucks data:\n\nFind all tweets containing “coffee”.\nExtract 20 characters of context around “coffee”.\nFind tweets containing mentions (@username) using regex.\nCreate a KWIC analysis for the word “Starbucks”.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#document-term-matrix-dtm",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#document-term-matrix-dtm",
    "title": "Computational Research",
    "section": "",
    "text": "A Document-Term Matrix (DTM) is a mathematical representation of text where:\n\nRows represent documents (e.g., books, tweets, articles)\nColumns represent terms (words)\nValues indicate the frequency of each term in each document\n\nExample DTM:\n\n\n\n\nlove\nfamily\nmoney\nmarriage\nhappy\n\n\n\n\nBook 1\n45\n23\n12\n67\n34\n\n\nBook 2\n32\n45\n56\n23\n12\n\n\nBook 3\n67\n12\n8\n89\n45\n\n\n\nNote: A Term-Document Matrix (TDM) is simply the transpose - terms as rows, documents as columns. In R’s tidytext, we typically create DTMs.\n\n\n\n\nThe transformation from raw text to DTM follows these steps:\nRaw Text → Tokenize → Remove Stopwords → Count → DTM\nStep-by-step visualization:\n# STEP 1: Start with raw text\noriginal_books |&gt; \n  select(book, text) |&gt; \n  head(3)\n## # A tibble: 3 × 2\n##   book                text                   \n##   &lt;fct&gt;               &lt;chr&gt;                  \n## 1 Sense & Sensibility \"SENSE AND SENSIBILITY\"\n## 2 Sense & Sensibility \"\"                     \n## 3 Sense & Sensibility \"by Jane Austen\"\n# STEP 2: Tokenize (one word per row)\ntokenized &lt;- original_books |&gt; \n  unnest_tokens(word, text)\n\ntokenized |&gt; \n  select(book, word) |&gt; \n  head(10)\n## # A tibble: 10 × 2\n##    book                word       \n##    &lt;fct&gt;               &lt;chr&gt;      \n##  1 Sense & Sensibility sense      \n##  2 Sense & Sensibility and        \n##  3 Sense & Sensibility sensibility\n##  4 Sense & Sensibility by         \n##  5 Sense & Sensibility jane       \n##  6 Sense & Sensibility austen     \n##  7 Sense & Sensibility 1811       \n##  8 Sense & Sensibility chapter    \n##  9 Sense & Sensibility 1          \n## 10 Sense & Sensibility the\n# STEP 3: Remove stopwords\ncleaned &lt;- tokenized |&gt; \n  anti_join(stop_words, by = \"word\")\n\ncleaned |&gt; \n  select(book, word) |&gt; \n  head(10)\n## # A tibble: 10 × 2\n##    book                word       \n##    &lt;fct&gt;               &lt;chr&gt;      \n##  1 Sense & Sensibility sense      \n##  2 Sense & Sensibility sensibility\n##  3 Sense & Sensibility jane       \n##  4 Sense & Sensibility austen     \n##  5 Sense & Sensibility 1811       \n##  6 Sense & Sensibility chapter    \n##  7 Sense & Sensibility 1          \n##  8 Sense & Sensibility family     \n##  9 Sense & Sensibility dashwood   \n## 10 Sense & Sensibility settled\n# STEP 4: Count words per document\nword_counts &lt;- cleaned |&gt; \n  count(book, word, sort = TRUE)\n\nhead(word_counts, 10)\n## # A tibble: 10 × 3\n##    book                word          n\n##    &lt;fct&gt;               &lt;chr&gt;     &lt;int&gt;\n##  1 Mansfield Park      fanny       816\n##  2 Emma                emma        786\n##  3 Sense & Sensibility elinor      623\n##  4 Emma                miss        599\n##  5 Pride & Prejudice   elizabeth   597\n##  6 Mansfield Park      crawford    493\n##  7 Sense & Sensibility marianne    492\n##  8 Persuasion          anne        447\n##  9 Mansfield Park      miss        432\n## 10 Northanger Abbey    catherine   428\n\n\n\n\nUse cast_dtm() to convert tidy word counts to a DTM:\n# Create DTM\nbook_dtm &lt;- word_counts |&gt; \n  cast_dtm(document = book, term = word, value = n)\n\n# Inspect the DTM\nbook_dtm\n## &lt;&lt;DocumentTermMatrix (documents: 6, terms: 13914)&gt;&gt;\n## Non-/sparse entries: 37224/46260\n## Sparsity           : 55%\n## Maximal term length: 19\n## Weighting          : term frequency (tf)\n# View dimensions: documents x terms\ndim(book_dtm)\n## [1]     6 13914\nUnderstanding the output: - 6 documents (the 6 Jane Austen books) - 13,914 terms (unique words across all books) - 99% sparse means 99% of the cells are zeros (most words don’t appear in most books)\n\n\n\n\nLet’s visualize a small portion of the DTM to understand its structure:\n# Get top 10 words overall\ntop_10_words &lt;- word_counts |&gt; \n  group_by(word) |&gt; \n  summarize(total = sum(n)) |&gt; \n  slice_max(total, n = 10) |&gt; \n  pull(word)\n\n# Filter to just these words\ndtm_subset &lt;- word_counts |&gt; \n  filter(word %in% top_10_words)\n\n# Create heatmap\nggplot(dtm_subset, aes(x = word, y = book, fill = n)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = n), color = \"white\", size = 3) +\n  scale_fill_gradient(low = \"steelblue\", high = \"darkred\") +\n  labs(\n    title = \"Document-Term Matrix Heatmap\",\n    subtitle = \"Top 10 words across Jane Austen books\",\n    x = \"Term\",\n    y = \"Document\",\n    fill = \"Frequency\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nReading the heatmap: - Darker colors = higher frequency - Each row is a book (document) - Each column is a word (term) - The numbers show exact word counts\n\n\n\n\nTop words per book:\ntop_words &lt;- word_counts |&gt; \n  group_by(book) |&gt; \n  slice_max(n, n = 5) |&gt; \n  ungroup()\n\nhead(top_words, 12)\n## # A tibble: 12 × 3\n##    book                word          n\n##    &lt;fct&gt;               &lt;chr&gt;     &lt;int&gt;\n##  1 Sense & Sensibility elinor      623\n##  2 Sense & Sensibility marianne    492\n##  3 Sense & Sensibility time        239\n##  4 Sense & Sensibility dashwood    231\n##  5 Sense & Sensibility sister      229\n##  6 Pride & Prejudice   elizabeth   597\n##  7 Pride & Prejudice   darcy       373\n##  8 Pride & Prejudice   bennet      294\n##  9 Pride & Prejudice   miss        283\n## 10 Pride & Prejudice   jane        264\n## 11 Mansfield Park      fanny       816\n## 12 Mansfield Park      crawford    493\nBar chart visualization:\nggplot(top_words, aes(x = reorder(word, n), y = n, fill = book)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  facet_wrap(~ book, scales = \"free_y\") +\n  labs(\n    title = \"Top 5 Words in Each Jane Austen Book\",\n    x = \"Word\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nYou can convert a DTM back to tidy format using tidy():\n# Convert DTM back to tidy\ntidy_dtm &lt;- tidy(book_dtm)\n\nhead(tidy_dtm)\n## # A tibble: 6 × 3\n##   document            term   count\n##   &lt;chr&gt;               &lt;chr&gt;  &lt;dbl&gt;\n## 1 Sense & Sensibility fanny     42\n## 2 Mansfield Park      fanny    816\n## 3 Persuasion          fanny      4\n## 4 Emma                emma     786\n## 5 Persuasion          emma       1\n## 6 Sense & Sensibility elinor   623\nThis is useful when you receive a DTM from another package and want to use tidyverse tools."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#tf-idf",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#tf-idf",
    "title": "Computational Research",
    "section": "",
    "text": "TF-IDF (Term Frequency - Inverse Document Frequency) measures how important a word is to a document within a collection.\n\nTF (Term Frequency): How often a term appears in a document\nIDF (Inverse Document Frequency): How rare a term is across all documents\n\n\\[\\text{TF-IDF} = \\text{TF} \\times \\log\\left(\\frac{\\text{Total Documents}}{\\text{Documents containing term}}\\right)\\]\nInterpretation:\n\nHigh TF-IDF: Word is frequent in this document but rare overall → important/distinctive\nLow TF-IDF: Word is common everywhere → less distinctive\n\n\n\n\n\nUse bind_tf_idf() from tidytext:\n# Calculate TF-IDF\nbook_tfidf &lt;- original_books |&gt; \n  unnest_tokens(word, text) |&gt; \n  count(book, word, sort = TRUE) |&gt; \n  bind_tf_idf(word, book, n)\n\nhead(book_tfidf)\n## # A tibble: 6 × 6\n##   book           word      n     tf   idf tf_idf\n##   &lt;fct&gt;          &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n## 1 Mansfield Park the    6206 0.0387     0      0\n## 2 Mansfield Park to     5475 0.0341     0      0\n## 3 Mansfield Park and    5438 0.0339     0      0\n## 4 Emma           to     5239 0.0325     0      0\n## 5 Emma           the    5201 0.0323     0      0\n## 6 Emma           and    4896 0.0304     0      0\nFind distinctive words for each book:\n# Top TF-IDF words per book\ntop_tfidf &lt;- book_tfidf |&gt; \n  group_by(book) |&gt; \n  slice_max(tf_idf, n = 5) |&gt; \n  ungroup()\n\nhead(top_tfidf, 12)\n## # A tibble: 12 × 6\n##    book                word           n      tf   idf  tf_idf\n##    &lt;fct&gt;               &lt;chr&gt;      &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n##  1 Sense & Sensibility elinor       623 0.00519 1.79  0.00931\n##  2 Sense & Sensibility marianne     492 0.00410 1.79  0.00735\n##  3 Sense & Sensibility dashwood     231 0.00193 1.79  0.00345\n##  4 Sense & Sensibility jennings     199 0.00166 1.79  0.00297\n##  5 Sense & Sensibility willoughby   181 0.00151 1.79  0.00270\n##  6 Pride & Prejudice   darcy        373 0.00305 1.79  0.00547\n##  7 Pride & Prejudice   bennet       294 0.00241 1.79  0.00431\n##  8 Pride & Prejudice   bingley      257 0.00210 1.79  0.00377\n##  9 Pride & Prejudice   elizabeth    597 0.00489 0.693 0.00339\n## 10 Pride & Prejudice   wickham      162 0.00133 1.79  0.00238\n## 11 Mansfield Park      crawford     493 0.00307 1.79  0.00551\n## 12 Mansfield Park      edmund       364 0.00227 1.79  0.00406\n\n\n\n\nggplot(top_tfidf, aes(x = reorder(word, tf_idf), y = tf_idf, fill = book)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  facet_wrap(~ book, scales = \"free\") +\n  labs(\n    title = \"Most Distinctive Words by TF-IDF\",\n    x = \"Word\",\n    y = \"TF-IDF Score\"\n  ) +\n  theme_minimal()\n\nNotice that TF-IDF highlights character names and distinctive terms for each book, rather than common words!\n\n\n\nUsing the Starbucks data:\n\nCreate word counts grouped by mention (the user being replied to).\nCalculate TF-IDF scores.\nFind the top 5 distinctive words for the 3 most active mentions.\nVisualize the results.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#word-clouds",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#word-clouds",
    "title": "Computational Research",
    "section": "",
    "text": "Word clouds display words with size proportional to their frequency. Install the wordcloud2 package:\ninstall.packages(\"wordcloud2\")\nlibrary(wordcloud2)\n\n# Prepare word frequencies\nword_freq &lt;- original_books |&gt;\n  unnest_tokens(word, text) |&gt;\n  anti_join(stop_words, by = \"word\") |&gt;\n  count(word, sort = TRUE)\n\nhead(word_freq)\n## # A tibble: 6 × 2\n##   word      n\n##   &lt;chr&gt; &lt;int&gt;\n## 1 miss   1855\n## 2 time   1337\n## 3 fanny   862\n## 4 dear    822\n## 5 lady    817\n## 6 sir     806\nCreate a basic word cloud:\nwordcloud2(data = word_freq, size = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nDescription\nExample\n\n\n\n\nsize\nScale factor for word sizes\nsize = 0.5\n\n\ncolor\nColor scheme\ncolor = \"random-light\"\n\n\nbackgroundColor\nBackground color\nbackgroundColor = \"black\"\n\n\nshape\nShape of cloud\nshape = \"circle\" or \"star\"\n\n\nminRotation, maxRotation\nWord rotation angles\nminRotation = -pi/4\n\n\n\nCustom colors:\nwordcloud2(data = word_freq, size = 0.5, color = \"random-light\")\n\n\n\n\nChange shape and background:\nwordcloud2(data = word_freq, size = 0.4, shape = \"star\", backgroundColor = \"black\", color = \"random-light\")\n\n\n\n\nWith rotation:\nwordcloud2(data = word_freq, size = 0.5, \n           minRotation = -pi/4, maxRotation = pi/4, rotateRatio = 0.3)\n\n\n\n\n\n\n\nlibrary(htmlwidgets)\nlibrary(webshot)\nwebshot::install_phantomjs()\n\n# Create word cloud\nmy_cloud &lt;- wordcloud2(word_freq, size = 1)\n\n# Save as HTML\nsaveWidget(my_cloud, \"wordcloud.html\", selfcontained = FALSE)\n\n# Save as image\nwebshot(\"wordcloud.html\", \"wordcloud.png\", delay = 5)"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#class-exercises-putting-it-all-together",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#class-exercises-putting-it-all-together",
    "title": "Computational Research",
    "section": "",
    "text": "Using either the Jane Austen or Starbucks dataset:\n\nWrite a function that takes a data frame and text column, then returns:\n\nTotal word count (after removing stopwords)\nTop 10 most frequent words\nA word cloud object\n\nUse a loop to analyze each book (or group) separately and store the results in a list.\nCreate a KWIC analysis for a keyword of your choice.\nCalculate TF-IDF and identify what makes each document/group distinctive.\nVisualize your findings with at least two different plot types.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#lecture-4-cheat-sheet",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#lecture-4-cheat-sheet",
    "title": "Computational Research",
    "section": "",
    "text": "Topic\nDescription\nCode Example\n\n\n\n\nif-else\nConditional execution\nif (x &gt; 5) { ... } else { ... }\n\n\nfor loop\nFixed iterations\nfor (i in 1:10) { print(i) }\n\n\nwhile loop\nConditional iterations\nwhile (x &lt; 10) { x &lt;- x + 1 }\n\n\nseq_along()\nSafe sequence for loops\nfor (i in seq_along(vec)) { ... }\n\n\nbreak\nExit loop\nif (cond) break\n\n\nnext\nSkip iteration\nif (cond) next\n\n\nfunction()\nCreate function\nmy_func &lt;- function(arg) { ... }\n\n\nDefault arguments\nSet defaults\nfunction(x, y = 10) { ... }\n\n\nreturn()\nExplicit return\nreturn(result)\n\n\nunnest_tokens()\nTokenize text\nunnest_tokens(word, text)\n\n\nanti_join(stop_words)\nRemove stopwords\ndata |&gt; anti_join(stop_words)\n\n\nstr_detect()\nFind pattern\nfilter(str_detect(text, \"word\"))\n\n\nstr_extract()\nExtract pattern\nmutate(x = str_extract(text, \".{10}word.{10}\"))\n\n\nKWIC\nKeyword in context\nExtract surrounding text for keywords\n\n\ncast_dtm()\nCreate DTM from tidy\ncast_dtm(document, term, value)\n\n\ntidy()\nConvert DTM to tidy\ntidy(dtm_object)\n\n\ngeom_tile()\nCreate heatmap\ngeom_tile(aes(x, y, fill = value))\n\n\nbind_tf_idf()\nCalculate TF-IDF\nbind_tf_idf(word, document, n)\n\n\nwordcloud2()\nCreate word cloud\nwordcloud2(data = freq_df, size = 0.5)\n\n\ncount()\nCount occurrences\ncount(word, sort = TRUE)\n\n\nslice_max()\nTop n by value\nslice_max(n, n = 10)"
  },
  {
    "objectID": "install.html",
    "href": "install.html",
    "title": "How to Install R Studio",
    "section": "",
    "text": "It is very important to install in order: first R, then R Studio\n\nInstall R from the download link. You can pick which mirror to choose. For example, Duke Mirror: https://archive.linux.duke.edu/cran/\nInstall R Studio (free desktop version) AFTER you installed R, from the link. You will only use R Studio but need to have both on your computer.",
    "crumbs": [
      "Home",
      "Getting Started",
      "How to Install R Studio"
    ]
  },
  {
    "objectID": "install.html#installation-steps",
    "href": "install.html#installation-steps",
    "title": "How to Install R Studio",
    "section": "",
    "text": "It is very important to install in order: first R, then R Studio\n\nInstall R from the download link. You can pick which mirror to choose. For example, Duke Mirror: https://archive.linux.duke.edu/cran/\nInstall R Studio (free desktop version) AFTER you installed R, from the link. You will only use R Studio but need to have both on your computer.",
    "crumbs": [
      "Home",
      "Getting Started",
      "How to Install R Studio"
    ]
  },
  {
    "objectID": "install.html#how-to-use-r-studio",
    "href": "install.html#how-to-use-r-studio",
    "title": "How to Install R Studio",
    "section": "How to Use R Studio",
    "text": "How to Use R Studio\n\nStep 1: Open R Studio from applications folder\n\n\n\nStep 2: Create a new project\n\n\n\nStep 3: Click on “File/New File/R script”\n\n\n\nStep 4: Running commands\nCommands in R Studio can be written directly into the console area (bottom left), or in the script. To run a completed command press Control + Enter (Windows/Linux) or Command + Enter (Mac).\n\n\nStep 5: Install Required Packages\nType the following code in the script and run it:\ninstall.packages(\"tidyverse\")    # data wrangling\ninstall.packages(\"tidyr\")        # data wrangling\ninstall.packages(\"tidytext\")     # text analysis\ninstall.packages(\"dplyr\")        # data wrangling\ninstall.packages(\"tm\")           # topic modeling\ninstall.packages(\"topicmodels\")  # topic modeling\ninstall.packages(\"quanteda\")     # text analysis\ninstall.packages(\"lubridate\")    # dates\ninstall.packages(\"ggplot2\")      # visualizations\ninstall.packages(\"ggthemes\")     # visualizations\ninstall.packages(\"scales\")       # visualizations\ninstall.packages(\"wesanderson\")  # visualizations\n\n\nStep 6: Load Packages into Library\nAfter installation, load the packages before using them:\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(tm)\nlibrary(topicmodels)\nlibrary(quanteda)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(scales)\nlibrary(wesanderson)\n\n\nStep 7: Getting Help\nFor help with any command, type ? followed by the command name and run:\n?merge\n?ggplot\n?install.packages\nFor keyboard shortcuts in R Studio, see the official documentation.\nYou’re Ready!\nNow you are ready to work in R Studio! If you have any issues with installation, please email the instructor or TA, or come to office hours.",
    "crumbs": [
      "Home",
      "Getting Started",
      "How to Install R Studio"
    ]
  },
  {
    "objectID": "install.html#troubleshooting",
    "href": "install.html#troubleshooting",
    "title": "How to Install R Studio",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nProblem: Package installation fails\n\nMake sure you’re connected to the internet\nTry selecting a different CRAN mirror\nCheck if you have write permissions\n\nProblem: R Studio won’t open\n\nMake sure R is installed first\nTry reinstalling R Studio\nCheck system requirements\n\nProblem: Can’t load a package\n\nMake sure the package is installed first using install.packages()\nCheck the package name spelling\nTry reinstalling the package",
    "crumbs": [
      "Home",
      "Getting Started",
      "How to Install R Studio"
    ]
  },
  {
    "objectID": "install.html#additional-resources",
    "href": "install.html#additional-resources",
    "title": "How to Install R Studio",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nRStudio Cheatsheets\nStack Overflow R Questions",
    "crumbs": [
      "Home",
      "Getting Started",
      "How to Install R Studio"
    ]
  },
  {
    "objectID": "resources/datasets.html",
    "href": "resources/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "List of datasets used in this course.",
    "crumbs": [
      "Home",
      "Resources",
      "Datasets"
    ]
  },
  {
    "objectID": "resources/datasets.html#course-datasets",
    "href": "resources/datasets.html#course-datasets",
    "title": "Datasets",
    "section": "",
    "text": "List of datasets used in this course.",
    "crumbs": [
      "Home",
      "Resources",
      "Datasets"
    ]
  },
  {
    "objectID": "resources/datasets.html#where-to-find-data",
    "href": "resources/datasets.html#where-to-find-data",
    "title": "Datasets",
    "section": "Where to Find Data",
    "text": "Where to Find Data\nResources for finding data for your projects.",
    "crumbs": [
      "Home",
      "Resources",
      "Datasets"
    ]
  },
  {
    "objectID": "schedule_phd.html",
    "href": "schedule_phd.html",
    "title": "Course Schedule - PhD",
    "section": "",
    "text": "Session\nDate\nTopic\nAssignment\nReadings\n\n\n\n\n1\nJan 21\nWhy Computational Social Science?\n\nView\n\n\n2\nJan 28\nBig Data: Data Collection and Wrangling\n\nView\n\n\n3\nFeb 4\nComputational Research\nBenchmark 1 Due\nView\n\n\n4\nFeb 11\nData Cleaning & Wrangling, GGPlot\n\nView\n\n\n5\nFeb 18\nStatistics\n\nView\n\n\n6\nFeb 25\nOptional: AI API, Images\n\n\n\n\n7\nMar 4\nText as Data: Dictionary Methods\nBenchmark 2 Due\nView\n\n\n8\nMar 18\nNo Class\n\n\n\n\n9\nMar 25\nText as Data: Topic Modeling\n\nView\n\n\n10\nApr 1\nUnsupervised Machine Learning\n\nView\n\n\n11\nApr 8\nNetworks\n\nView\n\n\n12\nApr 15\nOnline Social Movements\n\nView\n\n\n13\nApr 29\nFrom Data to Conclusions\n\nView\n\n\n\nMay 4\n\nFinal Paper Due",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - PhD"
    ]
  },
  {
    "objectID": "schedule_phd.html#mondays-230-515-pm",
    "href": "schedule_phd.html#mondays-230-515-pm",
    "title": "Course Schedule - PhD",
    "section": "",
    "text": "Session\nDate\nTopic\nAssignment\nReadings\n\n\n\n\n1\nJan 21\nWhy Computational Social Science?\n\nView\n\n\n2\nJan 28\nBig Data: Data Collection and Wrangling\n\nView\n\n\n3\nFeb 4\nComputational Research\nBenchmark 1 Due\nView\n\n\n4\nFeb 11\nData Cleaning & Wrangling, GGPlot\n\nView\n\n\n5\nFeb 18\nStatistics\n\nView\n\n\n6\nFeb 25\nOptional: AI API, Images\n\n\n\n\n7\nMar 4\nText as Data: Dictionary Methods\nBenchmark 2 Due\nView\n\n\n8\nMar 18\nNo Class\n\n\n\n\n9\nMar 25\nText as Data: Topic Modeling\n\nView\n\n\n10\nApr 1\nUnsupervised Machine Learning\n\nView\n\n\n11\nApr 8\nNetworks\n\nView\n\n\n12\nApr 15\nOnline Social Movements\n\nView\n\n\n13\nApr 29\nFrom Data to Conclusions\n\nView\n\n\n\nMay 4\n\nFinal Paper Due",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - PhD"
    ]
  },
  {
    "objectID": "schedule_phd.html#required-readings",
    "href": "schedule_phd.html#required-readings",
    "title": "Course Schedule - PhD",
    "section": "Required Readings",
    "text": "Required Readings\n\nSession 1: Why Computational Social Science?\n\nHofman, J. M., Watts, D. J., Athey, S., Garip, F., Griffiths, T. L., Kleinberg, J., Margetts, H., Mullainathan, S., Salganik, M. J., Vazire, S., Vespignani, A., & Yarkoni, T. (2021). Integrating explanation and prediction in computational social science. Nature, 595(7866), 181-188. https://doi.org/10.1038/s41586-021-03659-0\nLazer, D. M. J., Pentland, A., Watts, D. J., Aral, S., Athey, S., Contractor, N., Freelon, D., Gonzalez-Bailon, S., King, G., Margetts, H., Nelson, A., Salganik, M. J., Strohmaier, M., Vespignani, A., & Wagner, C. (2020). Computational social science: Obstacles and opportunities. Science, 369(6507), 1060-1062. https://doi.org/10.1126/science.aaz8170\nPeng, Y., Lu, Y., & Shen, C. (2023). An Agenda for Studying Credibility Perceptions of Visual Misinformation. Political Communication, 40(2), 225-237. https://doi.org/10.1080/10584609.2023.2175398\nPetchler, R., & González-Bailón, S. (2015). Automated content analysis of online political communication. In S. Coleman & D. Freelon (Eds.), Handbook of Digital Politics (pp. 433-450). Edward Elgar Publishing. https://doi.org/10.4337/9781782548768\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for Data Science - Whole Section. Available at: https://r4ds.hadley.nz/\n\nBack to Schedule\n\n\n\nSession 2: Big Data - Data Collection and Wrangling\n\nCappella, J. N. (2017). Vectors into the Future of Mass and Interpersonal Communication Research: Big Data, Social Media, and Computational Social Science. Human Communication Research, 43(4), 545-558. https://doi.org/10.1111/hcre.12114\nFreelon, D. (2018). Computational Research in the Post-API Age. Political Communication, 35(4), 665-668. https://doi.org/10.1080/10584609.2018.1477506\nHealy, K. (2018). Data Visualization: A Practical Introduction - Chapter 2: Make a Plot.\nTheocharis, Y., & Jungherr, A. (2021). Computational Social Science and the Study of Political Communication. Political Communication, 38(1-2), 1-22. https://doi.org/10.1080/10584609.2020.1833121\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for Data Science - Section: Visualize. Available at: https://r4ds.hadley.nz/\n\nBack to Schedule\n\n\n\nSession 3: Computational Research\n\nPeng, R. D. (2022). R Programming for Data Science - Chapters 13 & 14. Available at: https://bookdown.org/rdpeng/rprogdatascience/functions.html\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for Data Science - Section: Transform. Available at: https://r4ds.hadley.nz/\n\nBack to Schedule\n\n\n\nSession 4: Data Cleaning & Wrangling\n\nThulin, M. (2024). Modern Statistics with R - Chapter 5: Dealing with messy data. Available at: https://www.modernstatisticswithr.com/messychapter.html\n\nBack to Schedule\n\n\n\nSession 5: Statistics\n\nCernat, A. (2014). Introduction to Regression Models. In Longitudinal Data Analysis Using R.\nHealy, K. (2018). Data Visualization: A Practical Introduction - Section 6: Work with models.\nThulin, M. (2024). Modern Statistics with R - Chapter 8: Regression models. Available at: https://www.modernstatisticswithr.com/\n\nBack to Schedule\n\n\n\nSession 7: Text as Data - Dictionary Methods\n\nCarley, K. (1994). Extracting culture through textual analysis. Poetics, 22(4), 291-312. https://doi.org/10.1016/0304-422X(94)90011-6\nFraser, K. C., Zeller, F., Smith, D. H., Mohammad, S., & Rudzicz, F. (2019). How do we feel when a robot dies? Emotions expressed on Twitter before and after hitch. Proceedings of the Tenth Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, 62-71. https://doi.org/10.18653/v1/W19-1308\nHart, R. P. (2013). The Rhetoric of Political Comedy: A Tragedy?\nMora, M., Dupas de Matos, A., Fernández-Ruiz, V., Briz, T., & Chaya, C. (2020). Comparison of methods to develop an emotional lexicon of wine: Conventional vs rapid-method approach. Food Quality and Preference, 83, 103920. https://doi.org/10.1016/j.foodqual.2020.103920\nOphir, Y., & Walter, D. (2023). Computational Sentiment Analysis. In R. L. Nabi & J. G. Myrick, Emotions in the Digital World: Exploring Affective Experience and Expression in Online Interactions (pp. 114-133). Oxford University Press.\nPang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up? Sentiment Classification using Machine Learning Techniques. arXiv. http://arxiv.org/abs/cs/0205070\n\nBack to Schedule\n\n\n\nSession 9: Text as Data - Topic Modeling\n\nDiMaggio, P., Nag, M., & Blei, D. (2013). Exploiting affinities between topic modeling and the sociological perspective on culture: Application to newspaper coverage of U.S. government arts funding. Poetics, 41(6), 570-606. https://doi.org/10.1016/j.poetic.2013.08.004\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267-297. https://doi.org/10.1093/pan/mps028\nRudkowsky, E., Haselmayer, M., Wastian, M., Jenny, M., Emrich, Š., & Sedlmair, M. (2018). More than Bags of Words: Sentiment Analysis with Word Embeddings. Communication Methods and Measures, 12(2-3), 140-157. https://doi.org/10.1080/19312458.2018.1455817\nStracqualursi, L., & Agati, P. (2022). Tweet topics and sentiments relating to distance learning among Italian Twitter users. Scientific Reports, 12(1). https://doi.org/10.1038/s41598-022-12915-w\n\nBack to Schedule\n\n\n\nSession 10: Unsupervised Machine Learning\nNo assigned readings for this session.\nBack to Schedule\n\n\n\nSession 11: Networks\n\nBreiger, R. L. (1974). The Duality of Persons and Groups. Social Forces, 53(2), 181. https://doi.org/10.2307/2576011\nOphir, Y., Walter, D., Arnon, D., Lokmanoglu, A. D., Tizzoni, M., Carota, J., D’Antiga, L., & Nicastro, E. (2021). The Framing of COVID-19 in Italian Media and Its Relationship with Community Mobility: A Mixed-Method Approach. Journal of Health Communication, 26(3), 161-173. https://doi.org/10.1080/10810730.2021.1899344\nTorres, M., & Cantú, F. (2022). Learning to See: Convolutional Neural Networks for the Analysis of Social Science Data. Political Analysis, 30(1), 113-131. https://doi.org/10.1017/pan.2021.9\n\nBack to Schedule\n\n\n\nSession 12: Online Social Movements\n\nKraidy, M. (2017). Burning Man. In Naked Blogger of Cairo: Creative Insurgency in the Arab World (pp. 23-52). Harvard University Press.\nCasas, A., & Williams, N. W. (2019). Images that Matter: Online Protests and the Mobilizing Role of Pictures. Political Research Quarterly, 72(2), 360-375. https://doi.org/10.1177/1065912918786805\nFreelon, D. (2018). Computational Research in the Post-API Age. Political Communication, 35(4), 665-668. https://doi.org/10.1080/10584609.2018.1477506\nJackson, S. J., Bailey, M., & Welles, B. F. (2020). From #Ferguson to #FalconHeights: The Networked Case of Black Lives. In #hashtagactivism: Networks of race and gender justice (pp. 123-152). The MIT Press.\nThulin, M. (2024). Modern Statistics with R - Chapter 11: Predictive modelling and machine learning. Available at: https://www.modernstatisticswithr.com/\nTufekci, Z. (2017). Movement of Cultures. In Twitter and tear gas: The power and fragility of networked protest (pp. 83-114). Yale University Press.\n\nBack to Schedule\n\n\n\nSession 13: From Data to Conclusions\n\nBower, B. (2018, July 27). ‘Replication crisis’ spurs reforms in how science studies are done. Science News. https://www.sciencenews.org/blog/science-the-public/replication-crisis-psychology-science-studies-statistics\nBrown, M. A., Gruen, A., Maldoff, G., Messing, S., Sanderson, Z., & Zimmer, M. (2024). Web Scraping for Research: Legal, Ethical, Institutional, and Scientific Considerations. arXiv. https://doi.org/10.48550/arXiv.2410.23432\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267-297. https://doi.org/10.1093/pan/mps028\nKasy, M. (2019). No Data in the Void: Values and Distributional Conflicts in Empirical Policy Research and Artificial Intelligence. Economics for Inclusive Prosperity. https://econfip.org/policy-briefs/no-data-in-the-void-values-and-distributional-conflicts-in-empirical-policy-research-and-artificial-intelligence/\nLazer, D., Kennedy, R., King, G., & Vespignani, A. (2014). The Parable of Google Flu: Traps in Big Data Analysis. Science, 343(6176), 1203-1205. https://doi.org/10.1126/science.1248506\n\nBack to Schedule",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - PhD"
    ]
  },
  {
    "objectID": "schedule_phd.html#notes",
    "href": "schedule_phd.html#notes",
    "title": "Course Schedule - PhD",
    "section": "Notes",
    "text": "Notes\nAll readings are available on Blackboard under the “Readings” folder.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - PhD"
    ]
  },
  {
    "objectID": "sessions/02/session.html",
    "href": "sessions/02/session.html",
    "title": "Session 02 - Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select “Save Link As”\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n← Session 01\n\n\nSchedule\n\n\nSession 03 →",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 2 - Data Collection and Wrangling"
    ]
  },
  {
    "objectID": "sessions/04/session.html",
    "href": "sessions/04/session.html",
    "title": "Session 04 - Computational Research",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select “Save Link As”\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n← Session 03\n\n\nSchedule\n\n\nSession 05 →",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 4 - Computational Research"
    ]
  },
  {
    "objectID": "sessions/06/session.html",
    "href": "sessions/06/session.html",
    "title": "Session 06 - Statistics",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select “Save Link As”\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n← Session 05\n\n\nSchedule\n\n\nSession 07 →",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 6 - Statistics"
    ]
  },
  {
    "objectID": "sessions/08/session.html",
    "href": "sessions/08/session.html",
    "title": "Session 08 - MIDTERM EXAM",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select “Save Link As”\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n← Session 07\n\n\nSchedule\n\n\nSession 09 →",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 8 - Midterm Exam"
    ]
  },
  {
    "objectID": "sessions/10/session.html",
    "href": "sessions/10/session.html",
    "title": "Session 10 - Unsupervised Machine Learning",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select “Save Link As”\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n← Session 09\n\n\nSchedule\n\n\nSession 11 →",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 10 - Unsupervised Machine Learning"
    ]
  },
  {
    "objectID": "sessions/12/session.html",
    "href": "sessions/12/session.html",
    "title": "Session 12 - Online Social Movements",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select “Save Link As”\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n← Session 11\n\n\nSchedule\n\n\nSession 13 →",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 12 - Online Social Movements"
    ]
  }
]