[
  {
    "objectID": "sessions/13/session.html",
    "href": "sessions/13/session.html",
    "title": "Session 13 - From Data to Conclusions",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select ‚ÄúSave Link As‚Äù\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n‚Üê Session 12\n\n\nSchedule\n\n\n[End of Course] ‚Üí",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 13 - From Data to Conclusions"
    ]
  },
  {
    "objectID": "sessions/11/session.html",
    "href": "sessions/11/session.html",
    "title": "Session 11 - Networks",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select ‚ÄúSave Link As‚Äù\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n‚Üê Session 10\n\n\nSchedule\n\n\nSession 12 ‚Üí",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 11 - Networks"
    ]
  },
  {
    "objectID": "sessions/09/session.html",
    "href": "sessions/09/session.html",
    "title": "Session 09 - Text as Data: Topic Modeling",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select ‚ÄúSave Link As‚Äù\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n‚Üê Session 08\n\n\nSchedule\n\n\nSession 10 ‚Üí",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 9 - Topic Modeling"
    ]
  },
  {
    "objectID": "sessions/07/session.html",
    "href": "sessions/07/session.html",
    "title": "Session 07 - Text as Data: Dictionary Methods and Word Embeddings",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select ‚ÄúSave Link As‚Äù\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n‚Üê Session 06\n\n\nSchedule\n\n\nSession 08 ‚Üí",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 7 - Dictionary Methods & Word Embeddings"
    ]
  },
  {
    "objectID": "sessions/05/session.html",
    "href": "sessions/05/session.html",
    "title": "Session 05 - Descriptive Data",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select ‚ÄúSave Link As‚Äù\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n‚Üê Session 04\n\n\nSchedule\n\n\nSession 06 ‚Üí",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 5 - Descriptive Data"
    ]
  },
  {
    "objectID": "sessions/03/session.html",
    "href": "sessions/03/session.html",
    "title": "Session 03 - Big Data: GGPlot and Visualizations",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select ‚ÄúSave Link As‚Äù\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n‚Üê Session 02\n\n\nSchedule\n\n\nSession 04 ‚Üí",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 3 - GGPlot and Visualizations"
    ]
  },
  {
    "objectID": "sessions/01/session.html",
    "href": "sessions/01/session.html",
    "title": "Session 01 - Why Computational Social Science?",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select ‚ÄúSave Link As‚Äù\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n‚Üê Home\n\n\nSchedule\n\n\nSession 02 ‚Üí",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 1 - Why Computational Social Science?"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Course Schedule - Masters",
    "section": "",
    "text": "Session\nDate\nTopic\nAssignment\nReadings\n\n\n\n\n1\nJan 26\nWhy Computational Social Science?\n\nView\n\n\n2\nFeb 2\nBig Data: Data Collection and Wrangling\n\nView\n\n\n3\nFeb 9\nBig Data: GGPlot and Visualizations\n\nView\n\n\n4\nFeb 17*\nComputational Research\n\nView\n\n\n5\nFeb 23\nDescriptive Data\n\nView\n\n\n6\nMar 2\nStatistics\nBenchmark 1 Due\nView\n\n\n7\nMar 16\nText as Data: Dictionary Methods\n\nView\n\n\n8\nMar 23\nMidterm Exam\nMidterm\n\n\n\n9\nMar 30\nText as Data: Topic Modeling\n\nView\n\n\n10\nApr 6\nUnsupervised Machine Learning\n\nView\n\n\n11\nApr 13\nNetworks\n\nView\n\n\n12\nApr 22*\nOnline Social Movements\n\nView\n\n\n13\nApr 27\nFrom Data to Conclusions\n\nView\n\n\n\nMay 4\n\nFinal Papers Due\n\n\n\n\n*Monday schedule (university holiday adjustment)",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - Masters"
    ]
  },
  {
    "objectID": "schedule.html#section-a---mondays-230-515-pm",
    "href": "schedule.html#section-a---mondays-230-515-pm",
    "title": "Course Schedule - Masters",
    "section": "",
    "text": "Session\nDate\nTopic\nAssignment\nReadings\n\n\n\n\n1\nJan 26\nWhy Computational Social Science?\n\nView\n\n\n2\nFeb 2\nBig Data: Data Collection and Wrangling\n\nView\n\n\n3\nFeb 9\nBig Data: GGPlot and Visualizations\n\nView\n\n\n4\nFeb 17*\nComputational Research\n\nView\n\n\n5\nFeb 23\nDescriptive Data\n\nView\n\n\n6\nMar 2\nStatistics\nBenchmark 1 Due\nView\n\n\n7\nMar 16\nText as Data: Dictionary Methods\n\nView\n\n\n8\nMar 23\nMidterm Exam\nMidterm\n\n\n\n9\nMar 30\nText as Data: Topic Modeling\n\nView\n\n\n10\nApr 6\nUnsupervised Machine Learning\n\nView\n\n\n11\nApr 13\nNetworks\n\nView\n\n\n12\nApr 22*\nOnline Social Movements\n\nView\n\n\n13\nApr 27\nFrom Data to Conclusions\n\nView\n\n\n\nMay 4\n\nFinal Papers Due\n\n\n\n\n*Monday schedule (university holiday adjustment)",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - Masters"
    ]
  },
  {
    "objectID": "schedule.html#section-b---wednesdays-230-515-pm",
    "href": "schedule.html#section-b---wednesdays-230-515-pm",
    "title": "Course Schedule - Masters",
    "section": "Section B - Wednesdays 2:30-5:15 PM",
    "text": "Section B - Wednesdays 2:30-5:15 PM\n\n\n\nSession\nDate\nTopic\nAssignment\nReadings\n\n\n\n\n1\nJan 21\nWhy Computational Social Science?\n\nView\n\n\n2\nJan 28\nBig Data: Data Collection and Wrangling\n\nView\n\n\n3\nFeb 4\nBig Data: GGPlot and Visualizations\n\nView\n\n\n4\nFeb 11\nComputational Research\n\nView\n\n\n5\nFeb 18\nDescriptive Data\n\nView\n\n\n6\nFeb 25\nStatistics\nBenchmark 1 Due\nView\n\n\n7\nMar 4\nText as Data: Dictionary Methods\n\nView\n\n\n8\nMar 18\nMidterm Exam\nMidterm\n\n\n\n9\nMar 25\nText as Data: Topic Modeling\n\nView\n\n\n10\nApr 1\nUnsupervised Machine Learning\n\nView\n\n\n11\nApr 8\nNetworks\n\nView\n\n\n12\nApr 15\nOnline Social Movements\n\nView\n\n\n13\nApr 29\nFrom Data to Conclusions\n\nView\n\n\n\nMay 4\n\nFinal Papers Due",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - Masters"
    ]
  },
  {
    "objectID": "schedule.html#required-readings",
    "href": "schedule.html#required-readings",
    "title": "Course Schedule - Masters",
    "section": "Required Readings",
    "text": "Required Readings\n\nSession 1: Why Computational Social Science?\n\nHofman, J. M., Watts, D. J., Athey, S., Garip, F., Griffiths, T. L., Kleinberg, J., Margetts, H., Mullainathan, S., Salganik, M. J., Vazire, S., Vespignani, A., & Yarkoni, T. (2021). Integrating explanation and prediction in computational social science. Nature, 595(7866), 181-188.\nLazer, D. M. J., Pentland, A., Watts, D. J., Aral, S., Athey, S., Contractor, N., Freelon, D., Gonzalez-Bailon, S., King, G., Margetts, H., Nelson, A., Salganik, M. J., Strohmaier, M., Vespignani, A., & Wagner, C. (2020). Computational social science: Obstacles and opportunities. Science, 369(6507), 1060-1062.\nWickham, H., √áetinkaya-Rundel, M., & Grolemund, G. (2023). R for Data Science - Whole Game: Chapters 1-8. Available at: https://r4ds.hadley.nz/\n\nBack to Schedule\n\n\n\nSession 2: Big Data - Data Collection and Wrangling\n\nCappella, J. N. (2017). Vectors into the Future of Mass and Interpersonal Communication Research: Big Data, Social Media, and Computational Social Science. Human Communication Research, 43(4), 545-558.\nHealy, K. (2018). Data Visualization: A Practical Introduction - Chapter 2: Make a Plot.\nWickham, H., √áetinkaya-Rundel, M., & Grolemund, G. (2023). R for Data Science - Section: Visualize. Available at: https://r4ds.hadley.nz/\n\nBack to Schedule\n\n\n\nSession 3: Big Data - GGPlot and Visualizations\n\nThulin, M. (2024). Modern Statistics with R - Chapter 5: Dealing with messy data. Available at: https://www.modernstatisticswithr.com/\nWickham, H., √áetinkaya-Rundel, M., & Grolemund, G. (2023). R for Data Science - Section: Transform. Available at: https://r4ds.hadley.nz/\n\nBack to Schedule\n\n\n\nSession 4: Computational Research\n\nPeng, R. D. (2022). R Programming for Data Science - Chapters 13 & 14. Available at: https://bookdown.org/rdpeng/rprogdatascience/\n\nBack to Schedule\n\n\n\nSession 5: Descriptive Data\n\nCernat, A. (2014). Introduction to Regression Models. In Longitudinal Data Analysis Using R.\nHealy, K. (2018). Data Visualization: A Practical Introduction - Section 6: Work with models.\n\nBack to Schedule\n\n\n\nSession 6: Statistics\n\nThulin, M. (2024). Modern Statistics with R - Chapter 8: Regression models. Available at: https://www.modernstatisticswithr.com/\n\nBack to Schedule\n\n\n\nSession 7: Text as Data - Dictionary Methods\n\nCarley, K. (1994). Extracting culture through textual analysis. Poetics, 22(4), 291-312.\nPang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up? Sentiment Classification using Machine Learning Techniques. arXiv. http://arxiv.org/abs/cs/0205070\nStracqualursi, L., & Agati, P. (2022). Tweet topics and sentiments relating to distance learning among Italian Twitter users. Scientific Reports, 12(1).\n\nBack to Schedule\n\n\n\nSession 9: Text as Data - Topic Modeling\n\nDiMaggio, P., Nag, M., & Blei, D. (2013). Exploiting affinities between topic modeling and the sociological perspective on culture: Application to newspaper coverage of U.S. government arts funding. Poetics, 41(6), 570-606.\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267-297.\n\nBack to Schedule\n\n\n\nSession 10: Unsupervised Machine Learning\nNo assigned readings for this session.\nBack to Schedule\n\n\n\nSession 11: Networks\n\nBreiger, R. L. (1974). The Duality of Persons and Groups. Social Forces, 53(2), 181.\nOphir, Y., Walter, D., Arnon, D., Lokmanoglu, A. D., Tizzoni, M., Carota, J., D‚ÄôAntiga, L., & Nicastro, E. (2021). The Framing of COVID-19 in Italian Media and Its Relationship with Community Mobility: A Mixed-Method Approach. Journal of Health Communication, 26(3), 161-173.\nTorres, M., & Cant√∫, F. (2022). Learning to See: Convolutional Neural Networks for the Analysis of Social Science Data. Political Analysis, 30(1), 113-131.\n\nBack to Schedule\n\n\n\nSession 12: Online Social Movements\n\nThulin, M. (2024). Modern Statistics with R - Chapter 11: Predictive modelling and machine learning. Available at: https://www.modernstatisticswithr.com/\n\nBack to Schedule\n\n\n\nSession 13: From Data to Conclusions\n\nBower, B. (2018, July 27). ‚ÄòReplication crisis‚Äô spurs reforms in how science studies are done. Science News. https://www.sciencenews.org/blog/science-the-public/replication-crisis-psychology-science-studies-statistics\nLazer, D., Kennedy, R., King, G., & Vespignani, A. (2014). The Parable of Google Flu: Traps in Big Data Analysis. Science, 343(6176), 1203-1205.\n\nBack to Schedule",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - Masters"
    ]
  },
  {
    "objectID": "schedule.html#notes",
    "href": "schedule.html#notes",
    "title": "Course Schedule - Masters",
    "section": "Notes",
    "text": "Notes\nAll readings are available on Blackboard under the ‚ÄúReadings‚Äù folder.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - Masters"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html",
    "href": "resources/cheatsheets.html",
    "title": "R Cheatsheets",
    "section": "",
    "text": "Collection of helpful R cheatsheets for the course.\n\nR Markdown\ntidyr\ndata import\nRegular Expression\nLubridate\nggplot2",
    "crumbs": [
      "Home",
      "Resources",
      "R Cheatsheets"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html#essential-r-cheatsheets",
    "href": "resources/cheatsheets.html#essential-r-cheatsheets",
    "title": "R Cheatsheets",
    "section": "",
    "text": "Collection of helpful R cheatsheets for the course.\n\nR Markdown\ntidyr\ndata import\nRegular Expression\nLubridate\nggplot2",
    "crumbs": [
      "Home",
      "Resources",
      "R Cheatsheets"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html#external-resources",
    "href": "resources/cheatsheets.html#external-resources",
    "title": "R Cheatsheets",
    "section": "External Resources",
    "text": "External Resources\n\nRStudio Cheatsheets",
    "crumbs": [
      "Home",
      "Resources",
      "R Cheatsheets"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Data Analysis and Visualization",
    "section": "",
    "text": "Welcome to Social Data Analysis and Visualization (Masters) and Computer-Assisted Text Analysis (PhD)!\nAs researchers, we now have various big data and tools to learn about human behavior and nature. With the growth of electronic sources such as cell phones, online messaging platforms, and applications, every second new data is being added globally. This course provides a hands-on tutorial on introductory methods in Computational Social Science, which can be used in academic research and the non-academic industry specializing in data science.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Social Data Analysis and Visualization",
    "section": "",
    "text": "Welcome to Social Data Analysis and Visualization (Masters) and Computer-Assisted Text Analysis (PhD)!\nAs researchers, we now have various big data and tools to learn about human behavior and nature. With the growth of electronic sources such as cell phones, online messaging platforms, and applications, every second new data is being added globally. This course provides a hands-on tutorial on introductory methods in Computational Social Science, which can be used in academic research and the non-academic industry specializing in data science.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "Social Data Analysis and Visualization",
    "section": "Course Information",
    "text": "Course Information\nInstructor: Dr.¬†Ayse D. Lokmanoglu\nEmail: alokman@bu.edu\nOffice Hours: Wednesdays 10:00 am - 1:30 pm\nOffice Hours Sign-up Link\nTeaching Assistant: Max Wong, maxwong@bu.edu",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#course-goals",
    "href": "index.html#course-goals",
    "title": "Social Data Analysis and Visualization",
    "section": "Course Goals",
    "text": "Course Goals\nThe course has three main goals:\n\nTechnical Skills: Introduce students to R Studio and methods of text-mining and analysis\nCritical Thinking: Connect theory and methods while discussing ethical and practical concerns\nIndependent Learning: Prepare students with skills to adapt to evolving methodologies and programming languages",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Social Data Analysis and Visualization",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the course, students will be able to:\n\nUnderstand various computational methods available to social scientists\nImplement these methods in social science research\nBe aware of ethical and practical concerns with these methods\nUse these methods in current and future research\nDevelop independent learning and problem-solving skills",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "Social Data Analysis and Visualization",
    "section": "Quick Links",
    "text": "Quick Links\n\n\nCourse Materials\n\nSyllabus on Blackboard\nSchedule\nInstallation Guide\n\n\n\nResources\n\nR Cheatsheets\nDatasets\nBlackboard\n\n\n\nExternal Links\n\nGitHub Repository",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Social Data Analysis and Visualization",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\n\n\n\nImportantBefore the First Class\n\n\n\n\nInstall R and RStudio - Follow our installation guide\nJoin Blackboard - Access course materials and readings\nReview the syllabus - Familiarize yourself with course policies\nCheck the schedule - See upcoming topics and due dates",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#what-youll-learn",
    "href": "index.html#what-youll-learn",
    "title": "Social Data Analysis and Visualization",
    "section": "What You‚Äôll Learn",
    "text": "What You‚Äôll Learn\nThis course covers the following computational methods:\n\n\n\n\n\n\n\n\nWeek\nTopic\nMethods\n\n\n\n\n1-3\nR Fundamentals\nR Studio basics, data wrangling, visualization\n\n\n4-6\nStatistical Analysis\nDescriptive statistics, regression models, trend analysis\n\n\n7-10\nText Analysis\nDictionary methods, sentiment analysis, topic modeling\n\n\n11-12\nAdvanced Methods\nNetwork analysis, machine learning, predictive modeling\n\n\n13\nEthics & Best Practices\nData statements, reproducibility, ethical considerations",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#required-materials",
    "href": "index.html#required-materials",
    "title": "Social Data Analysis and Visualization",
    "section": "Required Materials",
    "text": "Required Materials\n\nSoftware: R and RStudio (free, open-source)\nTextbooks: All readings provided on Blackboard\nComputer: Reliable laptop with internet access (contact IT if needed)",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "index.html#support-resources",
    "href": "index.html#support-resources",
    "title": "Social Data Analysis and Visualization",
    "section": "Support Resources",
    "text": "Support Resources\nNeed help? Check out these resources:\n\nCOM Writing Center: Writing assistance for all assignments\nBU IT Support: Technical help with software and computers\nOffice of Disability Services: Accommodations for students with disabilities\nDean of Students: Support for food/housing insecurity or other challenges\n\nSee syllabus at blackboard for complete contact information.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Social Data Analysis and Visualization"
    ]
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L7-github.html",
    "href": "Spring2026/LectureSlides/bigdata_L7-github.html",
    "title": "Text as Data: Dictionary Methods and Word Embeddings",
    "section": "",
    "text": "Dr.¬†Ayse D. Lokmanoglu Lecture 7, (B) March 16, (A) March 4"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L7-github.html#lecture-7-table-of-contents",
    "href": "Spring2026/LectureSlides/bigdata_L7-github.html#lecture-7-table-of-contents",
    "title": "Text as Data: Dictionary Methods and Word Embeddings",
    "section": "Lecture 7 Table of Contents",
    "text": "Lecture 7 Table of Contents\n\n\n\nSection\nTopic\n\n\n\n\n1\nIntroduction to Text as Data\n\n\n2\nDictionary Methods\n\n\n2.1\nOnline Dataset: Twitter Data\n\n\n2.2\nText Preprocessing\n\n\n2.3\nSentiment Analysis with Dictionary Methods\n\n\n2.4\nVisualizing and Comparing Sentiment Analysis Results\n\n\n2.5\nMost common positive and negative words\n\n\n2.6\nNormalize sentiment scores\n\n\n3\nWord Embeddings\n\n\n3.1\nIntroduction to Word Embeddings\n\n\n3.1.1\nContinuous Bag of Words (CBOW)\n\n\n3.1.2\nSkip-Gram Model\n\n\n3.2\nApplying Word Embeddings in R\n\n\n3.2.1\nTraining Word2Vec with CBOW\n\n\n3.2.2\nVisualize CBOW\n\n\n3.2.3\nTraining Word2Vec with Skip Gram\n\n\n4\nClass Exercises: Sentiment Analysis and Word Embeddings\n\n\n\n\nALWAYS Let‚Äôs load our libraries\nlibrary(tidyverse)   # Data manipulation and visualization (includes dplyr, ggplot2, tidyr, stringr)\nlibrary(tidytext)    # Text mining using tidy data principles\nlibrary(ggplot2)     # Creating visualizations and plots\nlibrary(stopwords)   # Access to stopword lists in multiple languages\nlibrary(word2vec)    # Training word embedding models (CBOW and Skip-Gram)\nlibrary(umap)        # Dimensionality reduction for visualizing high-dimensional data\nlibrary(wordcloud2)  # Creating interactive word clouds\nlibrary(plotly)      # Creating interactive plots and visualizations\nlibrary(htmlwidgets)"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L7-github.html#introduction-to-text-as-data",
    "href": "Spring2026/LectureSlides/bigdata_L7-github.html#introduction-to-text-as-data",
    "title": "Text as Data: Dictionary Methods and Word Embeddings",
    "section": "1. Introduction to Text as Data",
    "text": "1. Introduction to Text as Data\nText data:\n\nis unstructured,\nrequires preprocessing to be analyzed.\n\n source: https://media.geeksforgeeks.org/wp-content/uploads/20210526142713/BlockDigramofTextMining.png\n\n\n\n\n\n\n\n\n\n\nPhase\nTechnique\nCore Question\nPurpose\nMethods & R Packages\n\n\n\n\nText Preprocessing\nTokenization\nHow can we segment text into meaningful units?\nConvert text into individual words or phrases.\ntidytext (unnest_tokens()), stringr (str_split())\n\n\n\nStopword Removal\nHow can we remove redundant words?\nEliminate common words that add little meaning.\ntidytext (stop_words), stopwords\n\n\n\nLemmatization & Stemming\nHow can we reduce word variations?\nStandardize words to their root forms.\ntextstem (lemmatize_words()), SnowballC (wordStem())\n\n\nFeature Engineering\nN-grams\nHow can we capture word sequences?\nIdentify multi-word expressions and patterns.\ntidytext (unnest_tokens(ngrams = 2)), text2vec\n\n\n\nPart-of-Speech Tagging\nHow can we recognize word functions?\nAssign grammatical categories to words.\nudpipe (udpipe_annotate()), spacyr\n\n\nContent Analysis\nDictionary-Based Analysis\nHow can we quantify meaning in text?\nDetect linguistic, psychological, or topical patterns.\ntidytext (get_sentiments()), quanteda (dfm_lookup())\n\n\nMachine Learning\nSupervised Classification\nHow can we predict categories from text?\nAssign labels based on prior training examples.\ncaret, textrecipes, tidymodels\n\n\n\nUnsupervised Clustering\nHow can we discover hidden patterns?\nGroup similar documents or topics automatically.\ntopicmodels (LDA), quanteda (k-means clustering), text2vec (word embeddings)\n\n\n\nWe will learn 2 methods today:\n\nDictionary Methods - Using predefined word lists to categorize text (e.g., sentiment analysis with lexicons).\nWord Embeddings - Representing words as numerical vectors to capture semantic relationships and similarities."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L7-github.html#dictionary-methods",
    "href": "Spring2026/LectureSlides/bigdata_L7-github.html#dictionary-methods",
    "title": "Text as Data: Dictionary Methods and Word Embeddings",
    "section": "2. Dictionary Methods",
    "text": "2. Dictionary Methods\nDictionary-based methods assign predefined categories to words.\n\n2.1 Online Dataset: Amazon Sales Data\nDataset Citation: Karkavel Raja, J. (2023). Amazon sales dataset [Data set]. Kaggle. https://www.kaggle.com/datasets/karkavelrajaj/amazon-sales-dataset\nNote: This dataset is raw and unfiltered, meaning it may contain explicit language, including swear words. Please proceed with awareness and discretion.\nWe will use a publicly available Amazon Sales Review Dataset, which contains tweets labeled as positive, neutral, or negative.\namazon_data &lt;- read_csv(\"https://raw.githubusercontent.com/aysedeniz09/IntroCSS/refs/heads/main/data/amazon.csv\")\n\ncolnames(amazon_data)\n##  [1] \"product_id\"          \"product_name\"        \"category\"           \n##  [4] \"discounted_price\"    \"actual_price\"        \"discount_percentage\"\n##  [7] \"rating\"              \"rating_count\"        \"about_product\"      \n## [10] \"user_id\"             \"user_name\"           \"review_id\"          \n## [13] \"review_title\"        \"review_content\"      \"img_link\"           \n## [16] \"product_link\"\n\n\n\n2.2 Text Preprocessing\nSince we are working with the review_content column from the Amazon sales dataset, we need to ensure proper formatting before tokenization. We will create a new text column, remove unnecessary whitespace, convert text to lowercase, remove URLs and numbers, and maintain consistency across all reviews. We‚Äôll also add an index column to help with merging data later in our analysis.\n# Ensure text is properly formatted\namazon_data &lt;- amazon_data |&gt;\n  mutate(textBU = review_content,   ### created a backup column so we always have the OG review\n    text = str_squish(review_content)) |&gt;\n  filter(!is.na(text)) |&gt;\n  mutate(text = str_to_lower(text)) |&gt; # Convert to lowercase\n  mutate(text = str_remove_all(text, \"https?://\\\\S+\")) |&gt; # Remove URLs\n  mutate(text = str_remove_all(text, \"\\\\d+\")) |&gt;  # Remove numbers\n  mutate(review_index = seq_len(nrow(amazon_data))) |&gt; ### creating an index\n  mutate(nwords = str_count(text, \"\\\\w+\")) ### counting number of words\n\nhead(amazon_data$text)\n## [1] \"looks durable charging is fine toono complains,charging is really fast, good product.,till now satisfied with the quality.,this is a good product . the charging speed is slower than the original iphone cable,good quality, would recommend, had worked well till date and was having no issue.cable is also sturdy enough...have asked for replacement and company is doing the same...,value for money\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n## [2] \"i ordered this cable to connect my phone to android auto of car. the cable is really strong and the connection ports are really well made. i already has a micro usb cable from ambrane and it's still in good shape. i connected my phone to the car using the cable and it got connected well and no issues. i also connected it to the charging port and yes it has fast charging support.,it quality is good at this price and the main thing is that i didn't ever thought that this cable will be so long it's good one and charging power is too good and also supports fast charging,value for money, with extra lengthüëç,good, working fine,product quality is good,good,very good,bought for my daughter's old phone.brand new cable it was not charging, i already repacked and requested for replacement.i checked again, and there was some green colour paste/fungus inside the micro usb connector. i cleaned with an alcoholic and starts working again.checked the ampere of charging speed got around ma-ma - not bad, came with braided .m long cable, pretty impressive for the price.can't blame the manufacturer.but quality issues by the distributor, they might have stored in very humid place.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n## [3] \"not quite durable and sturdy, good, nice product,working well,it's a really nice product\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n## [4] \"good product,long wire,charges good,nice,i bought this cable for rs. worthy product for this price, i tested it in various charger adapters w and w it supports fast charging as well.,good,ok,i had got this at good price on sale on amazon and product is useful with warranty but for warranty you need to go very far not practical for such a cost and mine micro to type c connector stopped working after few days only.,i like this product\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n## [5] \"bought this instead of original apple, does the work for rs, not as fast as apple charger but its a good option if you want cheap and good product, bought it for ipad pro . and it's working flawlessly, build quality is ok, its not like i am gonna hang my clothes on it and i want a very strong cable, even a braided cable stop working after a year, i have used both anker and apple store strong braided cable they all stop working after a year so please don't buy high end cables just for that instead choose a this one and even if it's stops working withing a year you only loose rs compares to rs.update------------------------------------pin has stopped charging from one side, now i have to slip the pin to charge from other side, but i will update and let know for how long does it work,,it‚Äôs good. not sure about durability as the pin area feels a bit fragile,does not support apple carplayso was little disappointed about thatother than that cable is made up of very good quality,best to buy,% not fathful,writing this review post  months and  orders of the same product.honestly portronics konnect l lightning cable works like magic with the original apple charging brick.seeing the price of the cable i initially hesitated buying as it was as low as ‚Çπ/- with the offers and so i wasn‚Äôt sure if it would work well with my iphone  or whether it would impact my iphone‚Äôs battery health because all the other lightning cable brands were costing over ‚Çπ/- like wayona, amazon basics, etc.earlier i was using wayona brand lightning cable with eventually frayed and stopped working.charging speed:charges my iphone fast enough almost similar compared to the original cable level when used with w original apple power adapter.quality and durability:great quality braided cable and doesn‚Äôt tangle easily and can withstand day-to-day usage.l-shaped pin:this is very innovative by portronics and it makes sure the cable doesn‚Äôt get damaged even if used while charging.carplay and data sync:works smoothly with carplay and syncs data effortlessly.ps: i have used this cable only with the original apple charging brick and extremely satisfied with its performance.,better than i expect the product i like that quality and i plan to buy same type cable come with usb c to lighting cable for emergency purpose that much i love this cable. buy for this cable only emergency uses only since good one,good product and value for money\"\n## [6] \"it's a good product.,like,very good item strong and useful usb cablevalue for moneythanks to amazon and producer, product and useful product,-,sturdy but does not support w charging\"\nBefore applying dictionary methods, we need to clean the text by:\n\nTokenizing the reviews into individual words\nRemoving stop words (common words like ‚Äúthe‚Äù, ‚Äúand‚Äù, ‚Äúis‚Äù that don‚Äôt carry much sentiment)\nRemoving unnecessary characters\n\n# Tokenize text\namazon_tokens &lt;- amazon_data |&gt; \n  unnest_tokens(word, text) |&gt;\n  anti_join(stop_words, by = \"word\") ## removing stopwords\n\n# View tokenized words\nhead(amazon_tokens$word)\n## [1] \"durable\"   \"charging\"  \"fine\"      \"toono\"     \"complains\" \"charging\"\n\n\n\n2.3 Sentiment Analysis with Dictionary Methods\nTo understand how different sentiment analysis lexicons classify text, we will compare results from multiple dictionaries, including Bing, AFINN, and NRC. Each lexicon provides different insights:\n\nBing: Binary classification (positive/negative sentiment).\nAFINN: Numeric scores for sentiment intensity.\nNRC: Categorizes words into emotional dimensions (anger, joy, fear, etc.).\n\nNote: For AFINN and NRC you need to select 1 in your console when prompted\nget_sentiments(\"afinn\")\n## # A tibble: 2,477 √ó 2\n##    word       value\n##    &lt;chr&gt;      &lt;dbl&gt;\n##  1 abandon       -2\n##  2 abandoned     -2\n##  3 abandons      -2\n##  4 abducted      -2\n##  5 abduction     -2\n##  6 abductions    -2\n##  7 abhor         -3\n##  8 abhorred      -3\n##  9 abhorrent     -3\n## 10 abhors        -3\n## # ‚Ñπ 2,467 more rows\nget_sentiments(\"bing\")\n## # A tibble: 6,786 √ó 2\n##    word        sentiment\n##    &lt;chr&gt;       &lt;chr&gt;    \n##  1 2-faces     negative \n##  2 abnormal    negative \n##  3 abolish     negative \n##  4 abominable  negative \n##  5 abominably  negative \n##  6 abominate   negative \n##  7 abomination negative \n##  8 abort       negative \n##  9 aborted     negative \n## 10 aborts      negative \n## # ‚Ñπ 6,776 more rows\nget_sentiments(\"nrc\")\n## # A tibble: 13,872 √ó 2\n##    word        sentiment\n##    &lt;chr&gt;       &lt;chr&gt;    \n##  1 abacus      trust    \n##  2 abandon     fear     \n##  3 abandon     negative \n##  4 abandon     sadness  \n##  5 abandoned   anger    \n##  6 abandoned   fear     \n##  7 abandoned   negative \n##  8 abandoned   sadness  \n##  9 abandonment anger    \n## 10 abandonment fear     \n## # ‚Ñπ 13,862 more rows\nLet‚Äôs now see how is it in our dataset\n# Apply Bing sentiment lexicon\n## Step 1:\nbing_sentiments_S1 &lt;- amazon_tokens |&gt;\n  inner_join(get_sentiments(\"bing\"), by = \"word\")\nhead(bing_sentiments_S1)\n## # A tibble: 6 √ó 21\n##   product_id product_name                 category discounted_price actual_price\n##   &lt;chr&gt;      &lt;chr&gt;                        &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;       \n## 1 B07JW9H4J1 Wayona Nylon Braided USB to‚Ä¶ Compute‚Ä¶ ‚Çπ399             ‚Çπ1,099      \n## 2 B07JW9H4J1 Wayona Nylon Braided USB to‚Ä¶ Compute‚Ä¶ ‚Çπ399             ‚Çπ1,099      \n## 3 B07JW9H4J1 Wayona Nylon Braided USB to‚Ä¶ Compute‚Ä¶ ‚Çπ399             ‚Çπ1,099      \n## 4 B07JW9H4J1 Wayona Nylon Braided USB to‚Ä¶ Compute‚Ä¶ ‚Çπ399             ‚Çπ1,099      \n## 5 B07JW9H4J1 Wayona Nylon Braided USB to‚Ä¶ Compute‚Ä¶ ‚Çπ399             ‚Çπ1,099      \n## 6 B07JW9H4J1 Wayona Nylon Braided USB to‚Ä¶ Compute‚Ä¶ ‚Çπ399             ‚Çπ1,099      \n## # ‚Ñπ 16 more variables: discount_percentage &lt;chr&gt;, rating &lt;dbl&gt;,\n## #   rating_count &lt;dbl&gt;, about_product &lt;chr&gt;, user_id &lt;chr&gt;, user_name &lt;chr&gt;,\n## #   review_id &lt;chr&gt;, review_title &lt;chr&gt;, review_content &lt;chr&gt;, img_link &lt;chr&gt;,\n## #   product_link &lt;chr&gt;, textBU &lt;chr&gt;, review_index &lt;int&gt;, nwords &lt;int&gt;,\n## #   word &lt;chr&gt;, sentiment &lt;chr&gt;\nbing_sentiments_S2 &lt;- bing_sentiments_S1 |&gt; \n  count(review_index, sentiment)\nhead(bing_sentiments_S2)\n## # A tibble: 6 √ó 3\n##   review_index sentiment     n\n##          &lt;int&gt; &lt;chr&gt;     &lt;int&gt;\n## 1            1 negative      2\n## 2            1 positive      6\n## 3            2 negative      5\n## 4            2 positive      8\n## 5            3 positive      4\n## 6            4 positive      4\nbing_sentiments_S3 &lt;- bing_sentiments_S2 |&gt; \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)\n\nhead(bing_sentiments_S3)\n## # A tibble: 6 √ó 3\n##   review_index negative positive\n##          &lt;int&gt;    &lt;int&gt;    &lt;int&gt;\n## 1            1        2        6\n## 2            2        5        8\n## 3            3        0        4\n## 4            4        0        4\n## 5            5        9       11\n## 6            6        0        3\nbing_sentiments_S4 &lt;- bing_sentiments_S3 |&gt; \n  mutate(sentiment = positive - negative)\nhead(bing_sentiments_S4)\n## # A tibble: 6 √ó 4\n##   review_index negative positive sentiment\n##          &lt;int&gt;    &lt;int&gt;    &lt;int&gt;     &lt;int&gt;\n## 1            1        2        6         4\n## 2            2        5        8         3\n## 3            3        0        4         4\n## 4            4        0        4         4\n## 5            5        9       11         2\n## 6            6        0        3         3\nFull Pipe:\nbing_sentiments &lt;- amazon_tokens |&gt;\n  inner_join(get_sentiments(\"bing\"), by = \"word\") |&gt; \n  count(review_index, sentiment) |&gt; \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |&gt; \n  mutate(sentiment = positive - negative) |&gt; \n  mutate(method = \"Bing\")\nNow let‚Äôs repeat it with AFINN, from now on I am going to give you the full pipeline, if you want you can see step by step\nafinn_sentiments &lt;- amazon_tokens |&gt;\n  inner_join(get_sentiments(\"afinn\")) |&gt; \n  group_by(review_index) |&gt;  \n  summarise(sentiment = sum(value)) |&gt; \n  mutate(method = \"AFINN\")\nNow w/ NRC:\nnrc_sentiments &lt;-  amazon_tokens |&gt; \n    inner_join(get_sentiments(\"nrc\") |&gt; \n                 filter(sentiment %in% c(\"positive\", \n                                         \"negative\"))) |&gt; \n  count(review_index, sentiment) |&gt; \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |&gt; \n  mutate(sentiment = positive - negative) |&gt; \n  mutate(method = \"NRC\")\n\n\n\n2.4 Visualizing and Comparing Sentiment Analysis Results\nall_sentiments &lt;- bind_rows(afinn_sentiments,\n          bing_sentiments,\n          nrc_sentiments) |&gt; \n  dplyr::select(-positive, -negative)\n\n\nggplot(all_sentiments,\n       aes(review_index, sentiment, fill = method)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~method, ncol = 1, scales = \"free_y\")\n\nInterpreting the Sentiment Analysis Results\nThe visualization above shows sentiment scores across approximately 1,500 Amazon product reviews using three different lexicons:\nAFINN (Top Panel - Red):\n\nShows sentiment scores ranging from approximately -20 to +70\nMost reviews cluster around neutral to slightly positive (0-20 range)\nSeveral spikes indicate strongly positive reviews (scores above 50)\nThe high positive scores suggest customers who leave reviews tend to express strong satisfaction\nNegative sentiment appears less frequent and less extreme\n\nBing (Middle Panel - Green):\n\nDisplays scores from approximately -30 to +30\nMore balanced distribution between positive and negative sentiment\nThe zero line represents neutral sentiment (equal positive and negative words)\nGreen bars above zero indicate positive sentiment; bars below indicate negative\nShows more variability and captures both satisfied and dissatisfied customers\n\nNRC (Bottom Panel - Blue):\n\nGenerally lower scores, mostly ranging from 0 to 40\nFew negative values, indicating this lexicon captures more positive than negative emotions\nSeveral notable spikes (around review index 1000) suggest reviews with strong emotional content\nThe lower overall scores reflect that NRC filters for specific emotions (anger, joy, fear, trust) rather than general sentiment\n\nKey Observations:\n\nAll three methods show predominantly positive sentiment, which is typical for product reviews (satisfied customers are more likely to leave reviews)\nAFINN produces the highest magnitude scores, making it useful for detecting strong sentiment\nBing provides the most balanced view of positive vs.¬†negative sentiment\nDifferent lexicons can produce different results for the same text, highlighting the importance of comparing multiple methods\n\n\n\n\n2.5 Most common positive and negative words\nbing_word_counts &lt;- amazon_tokens |&gt; \n  inner_join(get_sentiments(\"bing\")) |&gt; \n  count(word, sentiment, sort = TRUE) |&gt; \n  ungroup()\n\nhead(bing_word_counts)\n## # A tibble: 6 √ó 3\n##   word  sentiment     n\n##   &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;\n## 1 nice  positive    947\n## 2 easy  positive    917\n## 3 fast  positive    580\n## 4 fine  positive    522\n## 5 worth positive    431\n## 6 issue negative    335\nVisualize it:\nbing_word_counts |&gt; \n  group_by(sentiment) |&gt; \n  slice_max(n, n = 10) |&gt; \n  ungroup() |&gt; \n  mutate(word = reorder(word, n)) |&gt; \n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(x = \"Sentiment Count\",\n       y = NULL)\n\nWe can also do wordclouds using wordcloud2\n# Get word frequencies for disgust and trust emotions\nsentiment_words &lt;- amazon_tokens |&gt; \n  inner_join(get_sentiments(\"nrc\")) |&gt; \n  filter(sentiment %in% c(\"disgust\", \"trust\")) |&gt; \n  count(word, sentiment, sort = TRUE)\n\n# # Check the data structure\n# head(sentiment_words, 20)\n\n# Create separate wordclouds for each sentiment\ndisgust_words &lt;- sentiment_words |&gt; \n  filter(sentiment == \"disgust\") |&gt; \n  select(word, n) |&gt;\n  rename(freq = n)  # wordcloud2 likes 'freq' as column name\n\ntrust_words &lt;- sentiment_words |&gt; \n  filter(sentiment == \"trust\") |&gt; \n  select(word, n) |&gt;\n  rename(freq = n)\n\n# # Check the structure before creating wordcloud\n# str(disgust_words)\n# head(disgust_words)\n\nwordcloud2(disgust_words, \n           size = 0.5,\n           color = \"random-dark\", \n           backgroundColor = \"white\",\n           minSize = 5)\n\n\n\n\nwordcloud2(trust_words, \n           size = 0.5,\n           color = \"random-light\", \n           backgroundColor = \"white\",\n           minSize = 5)\n\n\n\n\n\n\n\n2.6 Normalize sentiment scores\n\nWhat are some ways I can normalize sentiment scores?\n\nDivide by number of words in the review!\nThis accounts for review length - longer reviews naturally have more sentiment words\n\n\nafinn_sentiments2 &lt;- afinn_sentiments |&gt; \n  left_join(amazon_data, by = \"review_index\") |&gt; \n  group_by(review_index) |&gt;\n  mutate(normalized_score = sentiment / nwords)\n\nhead(afinn_sentiments2)\n## # A tibble: 6 √ó 23\n## # Groups:   review_index [6]\n##   review_index sentiment method product_id product_name                 category\n##          &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;                        &lt;chr&gt;   \n## 1            1         4 AFINN  B07JW9H4J1 Wayona Nylon Braided USB to‚Ä¶ Compute‚Ä¶\n## 2            2         7 AFINN  B098NS6PVG Ambrane Unbreakable 60W / 3‚Ä¶ Compute‚Ä¶\n## 3            3         6 AFINN  B096MSW6CT Sounce Fast Phone Charging ‚Ä¶ Compute‚Ä¶\n## 4            4         4 AFINN  B08HDJ86NZ boAt Deuce USB 300 2 in 1 T‚Ä¶ Compute‚Ä¶\n## 5            5        -1 AFINN  B08CF3B7N1 Portronics Konnect L 1.2M F‚Ä¶ Compute‚Ä¶\n## 6            6         4 AFINN  B08Y1TFSP6 pTron Solero TB301 3A Type-‚Ä¶ Compute‚Ä¶\n## # ‚Ñπ 17 more variables: discounted_price &lt;chr&gt;, actual_price &lt;chr&gt;,\n## #   discount_percentage &lt;chr&gt;, rating &lt;dbl&gt;, rating_count &lt;dbl&gt;,\n## #   about_product &lt;chr&gt;, user_id &lt;chr&gt;, user_name &lt;chr&gt;, review_id &lt;chr&gt;,\n## #   review_title &lt;chr&gt;, review_content &lt;chr&gt;, img_link &lt;chr&gt;,\n## #   product_link &lt;chr&gt;, textBU &lt;chr&gt;, text &lt;chr&gt;, nwords &lt;int&gt;,\n## #   normalized_score &lt;dbl&gt;\nggplot(afinn_sentiments2, aes(x = normalized_score)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"black\") +\n  labs(\n    title = \"Histogram of AFINN Normalized Sentiment Scores\",\n    x = \"AFINN Normalized Scores (Sentiment per Word)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\nTo compare the non-normalized scores:\nggplot(afinn_sentiments2, aes(x = sentiment)) +\n  geom_histogram() +\n  labs(\n    title = \"Histogram of AFINN  Sentiment Scores\",\n    x = \"AFINN  Scores\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\nWhy normalize?\n\nLonger reviews will naturally have higher absolute sentiment scores\nNormalization helps us compare sentiment intensity across reviews of different lengths\nA short review with score 10 might be more positive than a long review with score 10"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L7-github.html#word-embeddings",
    "href": "Spring2026/LectureSlides/bigdata_L7-github.html#word-embeddings",
    "title": "Text as Data: Dictionary Methods and Word Embeddings",
    "section": "3. Word Embeddings",
    "text": "3. Word Embeddings\nTraditional approaches treat words as discrete symbols with no inherent relationship - ‚Äúgood‚Äù and ‚Äúgreat‚Äù are just different words. Word embeddings change this by representing words as vectors of numbers where similar words have similar vector representations.\nKey Concept: Words that appear in similar contexts tend to have similar meanings.\nFor example, in product reviews:\n\n‚ÄúThis cable is durable‚Äù\n‚ÄúThis cable is sturdy‚Äù\n‚ÄúThis cable is reliable‚Äù\n\nThe words ‚Äúdurable,‚Äù ‚Äústurdy,‚Äù and ‚Äúreliable‚Äù appear in similar contexts, so their word embeddings will be close to each other in vector space.\nVector Arithmetic Magic: Once words are vectors, we can perform mathematical operations:\n\nvector(\"good\") - vector(\"bad\") captures the concept of quality\nvector(\"phone\") - vector(\"cable\") + vector(\"charger\") might land near words related to charging accessories\n\nWhy use word embeddings for product reviews?\n\nCapture subtle differences in how customers describe products\nIdentify similar product features across different reviews\nUnderstand relationships between descriptive words (e.g., ‚Äúfast charging‚Äù vs ‚Äúquick charging‚Äù)\nGo beyond simple positive/negative to understand what customers actually care about\n\n\n\n3.1 Introduction to Word Embeddings\nThere are several popular word embedding methods:\n\nWord2Vec: Uses a neural network model to generate word embeddings based on context. We‚Äôll use this method today.\nGloVe (Global Vectors): Constructs word vectors based on word co-occurrence statistics across the entire corpus.\nFastText: Extends Word2Vec by representing words as subword n-grams, improving performance for rare words and handling typos better.\n\nFor this lecture, we‚Äôll focus on Word2Vec with two training algorithms:\n\nCBOW (Continuous Bag of Words)\n\nPredicts a word based on its surrounding context words\n\nSkip-Gram\n\nPredicts surrounding context words based on a target word\n\n\n\n\n3.1.1 Continuous Bag of Words (CBOW)\nThe CBOW model predicts a target word based on the surrounding context words. It works as follows:\n\nInput Context: The model takes a window of words surrounding a target word.\nWord Representation: Each word is mapped to a vector embedding that captures its semantic and syntactic properties.\nAggregation: The individual word vectors in the context window are combined into a single vector.\nPrediction: The model uses this aggregated vector to predict the most probable target word.\nOptimization: The model is trained to minimize the difference between predicted and actual words, refining the vector representations over time.\n\nExample from a product review:\nGiven the review: ‚ÄúThis cable has excellent charging speed‚Äù\nWith a window size of 2, to predict the word ‚Äúexcellent‚Äù, the model uses:\n\nContext words: [‚Äúcable‚Äù, ‚Äúhas‚Äù, ‚Äúcharging‚Äù, ‚Äúspeed‚Äù]\nThe model learns that words appearing near ‚Äúexcellent‚Äù in reviews are often product features\nOver many reviews, ‚Äúexcellent‚Äù becomes closely associated with positive quality descriptors\n\nWhy CBOW is useful for reviews:\n\nFast to train, efficient for large datasets (like thousands of product reviews)\nGood at learning common patterns in customer language\nWorks well when you have many examples of similar contexts (e.g., ‚Äúgreat quality‚Äù, ‚Äúexcellent quality‚Äù, ‚Äúamazing quality‚Äù)\n\n image from: https://media.geeksforgeeks.org/wp-content/uploads/20231220164157/Screenshot-2023-12-20-164143.png\n\nInput layer: Context words [w(t-2), w(t-1), w(t+1), w(t+2)] - the words surrounding our target\nHidden layer (Sum): These context word vectors are averaged/summed together\nOutput layer: Predicts the target word w(t)\n\nCBOW is efficient for handling large datasets and is useful for tasks requiring general word representations.\n\n\n\n3.1.2 Skip-Gram Model\nUnlike CBOW, the Skip-Gram model works in reverse: it predicts context words given a target word. It works as follows:\n\nInput Target Word: The model takes a single word as input.\nWord Representation: The target word is mapped to a high-dimensional vector embedding.\nProbability Distribution: The model generates probabilities for words likely to appear in the surrounding context.\nContext Word Prediction: Words with the highest probability are selected as context words.\nTraining Optimization: The model fine-tunes word embeddings by maximizing the probability of correctly predicting surrounding words.\n\n\nimage from: https://media.geeksforgeeks.org/wp-content/uploads/20231220164505/Screenshot-2023-12-20-164451.png\n\nInput layer: Single target word w(t)\nProjection layer: The word is converted to its vector representation\nOutput layer: Predicts multiple context words [w(t-2), w(t-1), w(t+1), w(t+2)]\n\nExample from a product review:\nGiven the target word ‚Äúdurable‚Äù in the review: ‚ÄúThis cable is very durable and sturdy‚Äù\nWith a window size of 2, the model tries to predict context words:\n\nExpected context: [‚Äúcable‚Äù, ‚Äúvery‚Äù, ‚Äúand‚Äù, ‚Äústurdy‚Äù]\nThe model learns what words typically appear near ‚Äúdurable‚Äù in product reviews\nOver time, it understands that ‚Äúdurable‚Äù is associated with product quality descriptors\n\nSkip-Gram performs better on small datasets and captures relationships between rare words more effectively, making it ideal for identifying specific product features that might not appear frequently.\nCBOW vs Skip-Gram - Which to use?\n\n\n\n\n\n\n\n\nFeature\nCBOW\nSkip-Gram\n\n\n\n\nSpeed\nFaster to train\nSlower to train\n\n\nBest for\nFrequent words, large datasets\nRare words, smaller datasets\n\n\nAccuracy\nGood for common patterns\nBetter for capturing nuanced relationships\n\n\nOur Amazon data\nGood choice (1,465 reviews)\nAlso viable, better for specific product terms\n\n\n\n\n\n\n\n3.2 Applying Word Embeddings in R\nWe will train a Word2Vec model on the Amazon product reviews using both Continuous Bag of Words (CBOW) and Skip-Gram algorithms to analyze relationships between words customers use to describe products.\nFor more on word embeddings: https://s-ai-f.github.io/Natural-Language-Processing/Word-embeddings.html\n\n3.2.1 Training Word2Vec with CBOW\nStep 1: Select the text column\nreviews &lt;- amazon_data$text\nStep 2: Train a Word2Vec model using the CBOW algorithm\nWhat do the parameters mean?\n\ndim = 15: Each word will be represented as a vector with 15 dimensions\niter = 20: The model will iterate through the data 20 times to learn patterns\ntype = \"cbow\": Using Continuous Bag of Words algorithm\n\ncbow_model &lt;- word2vec(x = reviews, type = \"cbow\", dim = 15, iter = 20)\nStep 3: Create embeddings using the trained CBOW model and print\n# checking embeddings\ncbow_embedding &lt;- as.matrix(cbow_model)\ncbow_embedding &lt;- predict(cbow_model, c(\"quality\", \"durable\"), type = \"embedding\")\nprint(\"The CBOW embedding for 'quality' and 'durable' is as follows:\")\n## [1] \"The CBOW embedding for 'quality' and 'durable' is as follows:\"\nprint(cbow_embedding)\n##               [,1]      [,2]       [,3]     [,4]       [,5]        [,6]\n## quality  1.2025050 0.6638257 -0.1812494 1.059464 0.02755646 -2.46302342\n## durable -0.8650298 1.1314095 -2.3551283 1.536014 0.04510239 -0.08961504\n##               [,7]        [,8]      [,9]      [,10]     [,11]      [,12]\n## quality  0.5330371  0.03221637 -1.114101 -0.5676234 0.2730089 -0.4610037\n## durable -0.2005682 -0.39719358 -1.536827  0.1245560 0.5725173 -0.4151438\n##              [,13]     [,14]      [,15]\n## quality -0.3463746 -1.841939  0.4923424\n## durable  0.1623335 -1.281561 -0.5580249\nStep 4: Find similar words (look-alikes)\ncbow_lookslike &lt;- predict(cbow_model, c(\"quality\", \"durable\"), \n                          type = \"nearest\", top_n = 5)\nprint(\"The nearest words for 'quality' and 'durable' in CBOW model prediction:\")\n## [1] \"The nearest words for 'quality' and 'durable' in CBOW model prediction:\"\nprint(cbow_lookslike)\n## $quality\n##     term1             term2 similarity rank\n## 1 quality       performance  0.9098881    1\n## 2 quality      beautycamera  0.8908731    2\n## 3 quality decentperformance  0.8823302    3\n## 4 quality       lookinggood  0.8814546    4\n## 5 quality        impressive  0.8701320    5\n## \n## $durable\n##     term1    term2 similarity rank\n## 1 durable   sturdy  0.9551952    1\n## 2 durable reliable  0.9236155    2\n## 3 durable     hell  0.8916085    3\n## 4 durable  braided  0.8714685    4\n## 5 durable    thick  0.8687557    5\nInterpreting the results:\nThe output shows the most similar words based on the CBOW model:\nFor ‚Äúquality‚Äù:\n\nTop similar words: ‚Äúbuild‚Äù, ‚Äúok‚Äù, ‚Äúappealing‚Äù, ‚Äúbuilt‚Äù\nSimilarity scores range from ~0.87 to 0.89 (closer to 1 = more similar)\nThese words often appear in similar contexts when customers discuss product quality\n\nFor ‚Äúdurable‚Äù:\n\nTop similar words: ‚Äústurdy‚Äù, ‚Äúreliable‚Äù, ‚Äúthick‚Äù, ‚Äúwire‚Äù\nSimilarity scores are very high (~0.88 to 0.95)\nNotice how ‚Äústurdy‚Äù and ‚Äúreliable‚Äù are nearly synonymous with ‚Äúdurable‚Äù\n‚Äúthick‚Äù and ‚Äúwire‚Äù appear because customers often discuss cable thickness when describing durability\n\nWhy this matters:\n\nThe model learned these relationships just from how words appear together in reviews\nNo manual labeling or dictionary was needed\nYou could use these word groups to:\n\nIdentify product features customers care about\nFind alternative ways customers express the same sentiment\nGroup similar customer feedback together\n\n\n\n\n3.2.2 Visualize CBOW\nStep 1: Prepare word list using tidy approach\nWe‚Äôll extract the top 100 most frequent words from our reviews (excluding stopwords) to visualize.\n# Get top 100 words using tidytext\nword_freq &lt;- amazon_data |&gt;\n  unnest_tokens(word, text) |&gt;\n  anti_join(stop_words, by = \"word\") |&gt;\n  count(word, sort = TRUE) |&gt;\n  top_n(100, n) |&gt;\n  pull(word)\n\ncat(\"Number of words to visualize:\", length(word_freq), \"\\n\")\n## Number of words to visualize: 100\nhead(word_freq, 20)\n##  [1] \"product\"  \"quality\"  \"cable\"    \"price\"    \"phone\"    \"charging\"\n##  [7] \"nice\"     \"easy\"     \"battery\"  \"time\"     \"buy\"      \"sound\"   \n## [13] \"watch\"    \"tv\"       \"money\"    \"fast\"     \"fine\"     \"amazon\"  \n## [19] \"water\"    \"camera\"\nStep 2: Match the embeddings\n# checking embeddings\n# Get embeddings for our top 100 words\ncbow_embedding &lt;- as.matrix(cbow_model)\ncbow_embedding &lt;- predict(cbow_model, word_freq, type = \"embedding\")\ncbow_embedding &lt;- na.omit(cbow_embedding)  # Remove any words not found in the model\n\n# Check how many words we successfully embedded\ncat(\"Successfully embedded\", nrow(cbow_embedding), \"words\\n\")\n## Successfully embedded 100 words\nStep 3: Reduce dimensions with UMAP for visualization Since our embeddings have 15 dimensions, we need to reduce them to 2D for plotting. UMAP (Uniform Manifold Approximation and Projection) preserves the local structure of the high-dimensional data.\n# Reduce to 2D using UMAP\nvisualization &lt;- umap(cbow_embedding, n_neighbors = 15, n_threads = 2)\n\n# Create data frame for plotting\ndf &lt;- data.frame(\n  word = rownames(cbow_embedding), \n  x = visualization$layout[, 1], \n  y = visualization$layout[, 2], \n  stringsAsFactors = FALSE\n)\n\n# Preview the data\nhead(df)\n##              word           x           y\n## product   product -0.28426891  1.33326785\n## quality   quality  0.76065832  1.87926290\n## cable       cable -0.09329616 -0.08191678\n## price       price -0.80635330  1.46285291\n## phone       phone  0.73710298 -1.62912150\n## charging charging -1.00224282 -0.16201743\nStep 4: Create interactive visualization\n# Create interactive plot\nplot_ly(df, x = ~x, y = ~y, type = \"scatter\", mode = 'text', text = ~word) %&gt;%\n  layout(\n    title = \"CBOW Word Embeddings: Amazon Product Reviews\",\n    xaxis = list(title = \"UMAP Dimension 1\"),\n    yaxis = list(title = \"UMAP Dimension 2\"),\n    hovermode = \"closest\"\n  )\n\n\n\n\nWhat to look for in the visualization:\n\nClusters of similar words: Words close together have similar meanings or appear in similar contexts\nLook for these typical clusters:\n\nQuality descriptors: ‚Äúgood‚Äù, ‚Äúexcellent‚Äù, ‚Äúquality‚Äù, ‚Äúdurable‚Äù, ‚Äústurdy‚Äù\nPrice-related: ‚Äúprice‚Äù, ‚Äúworth‚Äù, ‚Äúvalue‚Äù, ‚Äúmoney‚Äù, ‚Äúcheap‚Äù\nProduct features: ‚Äúcable‚Äù, ‚Äúcharging‚Äù, ‚Äúfast‚Äù, ‚Äúlong‚Äù, ‚Äúwire‚Äù\nNegative feedback: ‚Äúwaste‚Äù, ‚Äúpoor‚Äù, ‚Äúbad‚Äù, ‚Äúworst‚Äù, ‚Äúdisappointed‚Äù\nPositive emotions: ‚Äúlove‚Äù, ‚Äúperfect‚Äù, ‚Äúhappy‚Äù, ‚Äúsatisfied‚Äù\n\n\nTips for interpretation:\n\nThe exact positions are not meaningful, but relative distances are\nWords that appear in similar review contexts will cluster together\nYou can hover over words to see their exact labels (in the interactive plot)\nIf two product-related words are close, customers likely use them interchangeably\n\n\n\n\n3.2.3 Training Word2Vec with Skip Gram\nNow let‚Äôs train a Skip-Gram model and compare it with CBOW.\nRemember: Skip-Gram predicts context words from a target word, making it better at capturing relationships for rare words.\nStep 1: Select the text column:\nreviews &lt;- amazon_data$text\nStep 2: Train the Skip-Gram model\n# Using skip-gram algorithm\nskip_gram_model &lt;- word2vec(x = reviews, type = \"skip-gram\", dim = 15, iter = 20)\nWhat‚Äôs different from CBOW?\n\ntype = \"skip-gram\": Uses Skip-Gram algorithm instead of CBOW\nSame dimensions (15) and iterations (20) for fair comparison\nGenerally slower to train but better for rare/specific product terms\n\nStep 3: Create embeddings and examine specific words\n# Checking embeddings\nskip_embedding &lt;- as.matrix(skip_gram_model)\nskip_embedding &lt;- predict(skip_gram_model, c(\"quality\", \"durable\"), type = \"embedding\")\nprint(\"The Skip-Gram embedding for 'quality' and 'durable' is as follows:\")\n## [1] \"The Skip-Gram embedding for 'quality' and 'durable' is as follows:\"\nprint(skip_embedding)\n##              [,1]     [,2]     [,3]       [,4]        [,5]       [,6]\n## quality 2.3674839 1.237583 1.120280 -0.4259838 0.007147176 -0.6298435\n## durable 0.8535268 0.726113 1.293574  0.9131581 0.537023365 -1.0645292\n##               [,7]        [,8]       [,9]     [,10]      [,11]     [,12]\n## quality -0.1963775  0.87424290 -0.5931958 0.3389825 -0.4168760 0.5876618\n## durable -0.4880621 -0.02917298  0.3693385 1.3844260 -0.2563458 1.7919627\n##               [,13]      [,14]    [,15]\n## quality -0.05699976 -1.2636389 1.625222\n## durable  0.16518299 -0.6538487 1.947228\nStep 4: Find similar words using Skip-Gram\n# Finding similar words\nskip_lookslike &lt;- predict(skip_gram_model, c(\"price\", \"quality\"), type = \"nearest\", \n                          top_n = 5)\nprint(\"The nearest words for 'price' and 'quality' in Skip-Gram model:\")\n## [1] \"The nearest words for 'price' and 'quality' in Skip-Gram model:\"\nprint(skip_lookslike)\n## $price\n##   term1      term2 similarity rank\n## 1 price      range  0.9737731    1\n## 2 price       best  0.9604432    2\n## 3 price  visionary  0.9554536    3\n## 4 price reasonable  0.9530410    4\n## 5 price affordable  0.9372116    5\n## \n## $quality\n##     term1       term2 similarity rank\n## 1 quality       build  0.9651373    1\n## 2 quality        good  0.9467291    2\n## 3 quality       great  0.9420540    3\n## 4 quality        nice  0.9389253    4\n## 5 quality undoubtedly  0.9370061    5\nCompare with CBOW results:\n\nDo you see different similar words than CBOW found?\nSkip-Gram might capture more nuanced relationships\nEspecially useful for less common product-specific terms\n\nStep 5: Create new embeddings for the words_list. And then draw the visualization.\n# Get top 100 words using tidytext\nword_freq &lt;- amazon_data |&gt;\n  unnest_tokens(word, text) |&gt;\n  anti_join(stop_words, by = \"word\") |&gt;\n  count(word, sort = TRUE) |&gt;\n  top_n(100, n) |&gt;\n  pull(word)\n\n# checking embeddings\nskip_embedding &lt;- as.matrix(skip_gram_model)\nskip_embedding &lt;- predict(skip_gram_model, word_freq, type = \"embedding\")\nskip_embedding &lt;- na.omit(skip_embedding)\n\n\nvizualization &lt;- umap(skip_embedding, n_neighbors = 15, n_threads = 2)\n\ndf  &lt;- data.frame(word = rownames(skip_embedding), \n                  xpos = gsub(\".+//\", \"\", rownames(skip_embedding)), \n                  x = vizualization$layout[, 1], y = vizualization$layout[, 2], \n                  stringsAsFactors = FALSE)\n\nplot_ly(df, x = ~x, y = ~y, type = \"scatter\", mode = 'text', text = ~word) |&gt; \n    layout(\n    title = \"Skip-Gram Word Embeddings: Amazon Product Reviews\",\n    xaxis = list(title = \"UMAP Dimension 1\"),\n    yaxis = list(title = \"UMAP Dimension 2\"),\n    hovermode = \"closest\"\n  )\n\n\n\n\nComparing CBOW vs Skip-Gram visualizations:\nAfter creating both visualizations, compare them side-by-side:\n\nWord clusters: Are the same words clustered together in both models?\nCluster tightness: Which model creates tighter, more distinct clusters?\nRare words: Does Skip-Gram better separate specific product terms?\nOverall structure: Which gives you more useful insights about customer language?\n\nKey differences you might observe:\n\nSkip-Gram often creates clearer separation between different product aspects\nCBOW might group more general descriptive words together\nSkip-Gram may better distinguish between specific features (e.g., ‚Äúfast charging‚Äù vs ‚Äúlong cable‚Äù)\nLook for how quality-related words (‚Äúdurable‚Äù, ‚Äústurdy‚Äù, ‚Äúreliable‚Äù) cluster together\n\nWhich model to use?\n\nCBOW: Faster, good for frequent words, general patterns\nSkip-Gram: Better for rare words, specific product terms, nuanced relationships\nFor Amazon reviews: Both work well! Try both and see which gives better insights for your specific analysis"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L7-github.html#class-exercises-sentiment-analysis-and-word-embeddings",
    "href": "Spring2026/LectureSlides/bigdata_L7-github.html#class-exercises-sentiment-analysis-and-word-embeddings",
    "title": "Text as Data: Dictionary Methods and Word Embeddings",
    "section": "4. Class Exercises: Sentiment Analysis and Word Embeddings",
    "text": "4. Class Exercises: Sentiment Analysis and Word Embeddings\n\nExercise 1: Sentiment Analysis on Airline Data\n\nLoad the Airline Sentiment dataset.\n\nurl &lt;- \"https://raw.githubusercontent.com/aysedeniz09/IntroCSS/refs/heads/main/data/tweets.csv\"\nairline_user_data &lt;- read_csv(url)\nstr(airline_user_data)\n## spc_tbl_ [14,640 √ó 15] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ tweet_id                    : num [1:14640] 5.7e+17 5.7e+17 5.7e+17 5.7e+17 5.7e+17 ...\n##  $ airline_sentiment           : chr [1:14640] \"neutral\" \"positive\" \"neutral\" \"negative\" ...\n##  $ airline_sentiment_confidence: num [1:14640] 1 0.349 0.684 1 1 ...\n##  $ negativereason              : chr [1:14640] NA NA NA \"Bad Flight\" ...\n##  $ negativereason_confidence   : num [1:14640] NA 0 NA 0.703 1 ...\n##  $ airline                     : chr [1:14640] \"Virgin America\" \"Virgin America\" \"Virgin America\" \"Virgin America\" ...\n##  $ airline_sentiment_gold      : chr [1:14640] NA NA NA NA ...\n##  $ name                        : chr [1:14640] \"cairdin\" \"jnardino\" \"yvonnalynn\" \"jnardino\" ...\n##  $ negativereason_gold         : chr [1:14640] NA NA NA NA ...\n##  $ retweet_count               : num [1:14640] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ text                        : chr [1:14640] \"@VirginAmerica What @dhepburn said.\" \"@VirginAmerica plus you've added commercials to the experience... tacky.\" \"@VirginAmerica I didn't today... Must mean I need to take another trip!\" \"@VirginAmerica it's really aggressive to blast obnoxious \\\"entertainment\\\" in your guests' faces &amp; they hav\"| __truncated__ ...\n##  $ tweet_coord                 : chr [1:14640] NA NA NA NA ...\n##  $ tweet_created               : chr [1:14640] \"2015-02-24 11:35:52 -0800\" \"2015-02-24 11:15:59 -0800\" \"2015-02-24 11:15:48 -0800\" \"2015-02-24 11:15:36 -0800\" ...\n##  $ tweet_location              : chr [1:14640] NA NA \"Lets Play\" NA ...\n##  $ user_timezone               : chr [1:14640] \"Eastern Time (US & Canada)\" \"Pacific Time (US & Canada)\" \"Central Time (US & Canada)\" \"Pacific Time (US & Canada)\" ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   tweet_id = col_double(),\n##   ..   airline_sentiment = col_character(),\n##   ..   airline_sentiment_confidence = col_double(),\n##   ..   negativereason = col_character(),\n##   ..   negativereason_confidence = col_double(),\n##   ..   airline = col_character(),\n##   ..   airline_sentiment_gold = col_character(),\n##   ..   name = col_character(),\n##   ..   negativereason_gold = col_character(),\n##   ..   retweet_count = col_double(),\n##   ..   text = col_character(),\n##   ..   tweet_coord = col_character(),\n##   ..   tweet_created = col_character(),\n##   ..   tweet_location = col_character(),\n##   ..   user_timezone = col_character()\n##   .. )\n##  - attr(*, \"problems\")=&lt;externalptr&gt;\n\nApply dictionary-based sentiment analysis using Bing, AFINN, and NRC.\nCompare the results and interpret the findings.\nCreate a visualization (bar chart or word cloud) of sentiment scores.\n\n\n\nExercise 2: Exploring Word Embeddings\n\nFind a pre-trained word embedding model (GloVe, Word2Vec, or FastText).\nIdentify the top 10 most similar words for ‚Äúpositive‚Äù and ‚Äúnegative‚Äù.\nVisualize word relationships.\n\n\n\nOptional Exercise 3: Combining Sentiment Analysis and Word Embeddings (This is an advanced exercise for those that want to try)\n\nSelect a subset of the Airline dataset.\nCompute sentiment scores using dictionary-based methods.\nExtract word embeddings for the most frequent words in positive and negative tweets.\nCompare sentiment-based results with word embedding similarities."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L7-github.html#lecture-7-cheat-sheet",
    "href": "Spring2026/LectureSlides/bigdata_L7-github.html#lecture-7-cheat-sheet",
    "title": "Text as Data: Dictionary Methods and Word Embeddings",
    "section": "Lecture 7 Cheat Sheet",
    "text": "Lecture 7 Cheat Sheet\n\n\n\n\n\n\n\n\nFunction/Concept\nDescription\nCode Example\n\n\n\n\nTokenization (unnest_tokens())\nBreaks text into individual words or phrases for processing.\ntwitter_tokens |&gt; unnest_tokens(word, text)\n\n\nRemoving Stopwords (anti_join(stop_words))\nRemoves common stopwords to focus on meaningful content.\ntwitter_tokens |&gt; anti_join(stop_words)\n\n\nSentiment Analysis (get_sentiments())\nApplies sentiment lexicons (Bing, AFINN, NRC) to categorize words.\ntwitter_tokens |&gt; inner_join(get_sentiments('bing'))\n\n\nBing Sentiment Analysis (inner_join(get_sentiments('bing')))\nClassifies words as positive or negative using the Bing lexicon.\nbing_sentiments |&gt; count(post_index, sentiment)\n\n\nAFINN Sentiment Analysis (inner_join(get_sentiments('afinn')))\nAssigns sentiment scores based on word intensity using AFINN.\nafinn_sentiments |&gt; group_by(text) |&gt; summarize(score = sum(value))\n\n\nNRC Sentiment Analysis (inner_join(get_sentiments('nrc')))\nCategorizes words by emotions such as anger, joy, and fear.\nnrc_sentiments |&gt; count(sentiment)\n\n\nWord Embeddings - CBOW (word2vec(type = 'cbow'))\nTrains a Continuous Bag of Words (CBOW) model for word embeddings.\ncbow_model &lt;- word2vec(x = tweets, type = 'cbow', dim = 15, iter = 20)\n\n\nWord Embeddings - Skip-Gram (word2vec(type = 'skip-gram'))\nTrains a Skip-Gram model to predict context words from target words.\nskip_gram_model &lt;- word2vec(x = tweets, type = 'skip-gram', dim = 15, iter = 20)\n\n\nFinding Similar Words (predict(model, type = 'nearest'))\nFinds words with similar meanings based on trained word embeddings.\npredict(cbow_model, c('election', 'vote'), type = 'nearest')\n\n\nExtracting Word Embeddings (predict(model, type = 'embedding'))\nExtracts vector representations of words for further analysis.\npredict(skip_gram_model, c('election', 'vote'), type = 'embedding')\n\n\nVisualizing Sentiments (ggplot() + geom_col())\nGenerates bar plots to visualize sentiment distribution in text.\nggplot(bing_summary, aes(x = sentiment, y = n, fill = sentiment)) + geom_col()\n\n\nUMAP for Dimensionality Reduction (umap())\nReduces high-dimensional word embeddings for visualization.\numap_result &lt;- umap(word_embeddings, n_neighbors = 15, n_threads = 2)\n\n\nNormalize Sentiment Scores (mutate(normalized_score = score / word_count))\nNormalizes sentiment scores by dividing by word count.\nafinn_scores |&gt; mutate(normalized_score = score / word_count)\n\n\nCreating a Sentiment Pipeline (pivot_wider() + mutate())\nCombines multiple sentiment analysis steps into a single pipeline.\nbing_sentiments |&gt; pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |&gt; mutate(sentiment = positive - negative)\n\n\nWord Frequency with Tidy (count())\nCounts word frequencies using tidy approach\ntwitter_tokens |&gt; count(word, sort = TRUE)\n\n\nTop N Words (top_n())\nSelects top n rows based on a variable\nword_freq |&gt; top_n(100, n)\n\n\nWord Cloud (wordcloud2())\nCreates interactive word clouds\nwordcloud2(word_freq, color = \"gray20\")"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L5-github.html",
    "href": "Spring2026/LectureSlides/bigdata_L5-github.html",
    "title": "Descriptive Data",
    "section": "",
    "text": "Dr.¬†Ayse D. Lokmanoglu Lecture 5, (B) Feb 18, (A) Feb 23\n\n\n\n\n\nSection\nTopic\n\n\n\n\n1\nIntroduction to Descriptive Statistics\n\n\n1.1\nWhy Descriptive Statistics?\n\n\n1.2\nTypes of Variables\n\n\n2\nMeasures of Central Tendency\n\n\n2.1\nMean\n\n\n2.2\nMedian\n\n\n2.3\nMode\n\n\n2.4\nWhen to Use Which?\n\n\n3\nMeasures of Spread\n\n\n3.1\nRange\n\n\n3.2\nVariance and Standard Deviation\n\n\n3.3\nInterquartile Range (IQR)\n\n\n4\nVisualizing Distributions\n\n\n4.1\nHistograms\n\n\n4.2\nDensity Plots\n\n\n4.3\nBoxplots\n\n\n4.4\nCombining Visualizations\n\n\n5\nDetecting Outliers\n\n\n5.1\nWhat are Outliers?\n\n\n5.2\nIQR Method\n\n\n5.3\nZ-Score Method\n\n\n5.4\nVisualizing Outliers\n\n\n6\nGrouped Summary Statistics\n\n\n6.1\nUsing group_by() and summarize()\n\n\n6.2\nMultiple Summary Statistics\n\n\n6.3\nVisualizing Grouped Data\n\n\n7\nCorrelation\n\n\n7.1\nUnderstanding Correlation\n\n\n7.2\nCalculating Correlation\n\n\n7.3\nCorrelation Matrices\n\n\n7.4\nVisualizing Correlations\n\n\n8\nCross-Tabulations\n\n\n8.1\nCreating Frequency Tables\n\n\n8.2\nProportions and Percentages\n\n\n8.3\nVisualizing Cross-Tabulations\n\n\n\n\nALWAYS load our libraries first\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n\n\n\n\nDescriptive statistics help us understand and summarize our data before diving into more complex analyses. They answer questions like:\n\nWhat is the typical value in my data?\nHow spread out are my values?\nAre there any unusual observations?\nHow are my variables related?\n\nDescriptive vs.¬†Inferential Statistics:\n\n\n\n\n\n\n\nDescriptive\nInferential\n\n\n\n\nSummarizes data\nMakes predictions\n\n\nDescribes what IS\nEstimates what MIGHT BE\n\n\nNo uncertainty\nIncludes uncertainty\n\n\ne.g., ‚ÄúThe mean age is 25‚Äù\ne.g., ‚ÄúThe population mean is likely between 23-27‚Äù\n\n\n\n\n\n\n\nUnderstanding variable types helps us choose the right statistics:\nNumeric (Quantitative):\n\nContinuous: Can take any value (e.g., height, weight, temperature)\nDiscrete: Only whole numbers (e.g., count of children, number of books)\n\nCategorical (Qualitative):\n\nNominal: No natural order (e.g., color, gender, major)\nOrdinal: Has natural order (e.g., education level, satisfaction rating)\n\n# Create our example dataset\nstudent_data &lt;- read_csv(\"https://raw.githubusercontent.com/aysedeniz09/IntroCSS/refs/heads/main/data/student_lifestyle_dataset.csv\")\nIdentify variable types:\n# Check each variable\nstr(student_data)\n## spc_tbl_ [2,000 √ó 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ Student_ID                     : num [1:2000] 1 2 3 4 5 6 7 8 9 10 ...\n##  $ Study_Hours_Per_Day            : num [1:2000] 6.9 5.3 5.1 6.5 8.1 6 8 8.4 5.2 7.7 ...\n##  $ Extracurricular_Hours_Per_Day  : num [1:2000] 3.8 3.5 3.9 2.1 0.6 2.1 0.7 1.8 3.6 0.7 ...\n##  $ Sleep_Hours_Per_Day            : num [1:2000] 8.7 8 9.2 7.2 6.5 8 5.3 5.6 6.3 9.8 ...\n##  $ Social_Hours_Per_Day           : num [1:2000] 2.8 4.2 1.2 1.7 2.2 0.3 5.7 3 4 4.5 ...\n##  $ Physical_Activity_Hours_Per_Day: num [1:2000] 1.8 3 4.6 6.5 6.6 7.6 4.3 5.2 4.9 1.3 ...\n##  $ GPA                            : num [1:2000] 2.99 2.75 2.67 2.88 3.51 2.85 3.08 3.2 2.82 2.76 ...\n##  $ Stress_Level                   : chr [1:2000] \"Moderate\" \"Low\" \"Low\" \"Moderate\" ...\n##  $ college                        : chr [1:2000] \"Education\" \"Humanities\" \"Arts & Sciences\" \"Communication\" ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   Student_ID = col_double(),\n##   ..   Study_Hours_Per_Day = col_double(),\n##   ..   Extracurricular_Hours_Per_Day = col_double(),\n##   ..   Sleep_Hours_Per_Day = col_double(),\n##   ..   Social_Hours_Per_Day = col_double(),\n##   ..   Physical_Activity_Hours_Per_Day = col_double(),\n##   ..   GPA = col_double(),\n##   ..   Stress_Level = col_character(),\n##   ..   college = col_character()\n##   .. )\n##  - attr(*, \"problems\")=&lt;externalptr&gt;\nLet‚Äôs create GPA levels for some categorical data\nstudent_data &lt;- student_data |&gt; \n  mutate(\n        GPA_Level = case_when(\n      GPA &lt; 2.90 ~ \"LowGPA\",\n      GPA &gt;= 2.90 & GPA &lt;= 3.33 ~ \"MediumGPA\",\n      GPA &gt; 3.33 ~ \"HighGPA\"\n    ))\n\nstr(student_data)\n## tibble [2,000 √ó 10] (S3: tbl_df/tbl/data.frame)\n##  $ Student_ID                     : num [1:2000] 1 2 3 4 5 6 7 8 9 10 ...\n##  $ Study_Hours_Per_Day            : num [1:2000] 6.9 5.3 5.1 6.5 8.1 6 8 8.4 5.2 7.7 ...\n##  $ Extracurricular_Hours_Per_Day  : num [1:2000] 3.8 3.5 3.9 2.1 0.6 2.1 0.7 1.8 3.6 0.7 ...\n##  $ Sleep_Hours_Per_Day            : num [1:2000] 8.7 8 9.2 7.2 6.5 8 5.3 5.6 6.3 9.8 ...\n##  $ Social_Hours_Per_Day           : num [1:2000] 2.8 4.2 1.2 1.7 2.2 0.3 5.7 3 4 4.5 ...\n##  $ Physical_Activity_Hours_Per_Day: num [1:2000] 1.8 3 4.6 6.5 6.6 7.6 4.3 5.2 4.9 1.3 ...\n##  $ GPA                            : num [1:2000] 2.99 2.75 2.67 2.88 3.51 2.85 3.08 3.2 2.82 2.76 ...\n##  $ Stress_Level                   : chr [1:2000] \"Moderate\" \"Low\" \"Low\" \"Moderate\" ...\n##  $ college                        : chr [1:2000] \"Education\" \"Humanities\" \"Arts & Sciences\" \"Communication\" ...\n##  $ GPA_Level                      : chr [1:2000] \"MediumGPA\" \"LowGPA\" \"LowGPA\" \"LowGPA\" ...\n\n\n\n\n\nCentral tendency tells us about the typical or center value in our data.\n\n\nThe arithmetic mean (average) is the sum of all values divided by the number of values.\n\\[\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\]\n# Calculate mean\nmean(student_data$Study_Hours_Per_Day)\n## [1] 7.4758\n# Mean with missing values\nx &lt;- c(10, 20, NA, 30, 40)\nmean(x)  # Returns NA\n## [1] NA\n# Remove NA values\nmean(x, na.rm = TRUE)\n## [1] 25\n\n\n\n\nThe median is the middle value when data is sorted. It‚Äôs more robust to outliers than the mean.\n# Calculate median\nmedian(student_data$Study_Hours_Per_Day)\n## [1] 7.4\n\n\n\n\nThe mode is the most frequently occurring value. R doesn‚Äôt have a built-in mode function, so we create one:\n# Create a mode function\nget_mode &lt;- function(x) {\n  unique_x &lt;- unique(x)\n  unique_x[which.max(tabulate(match(x, unique_x)))]\n}\n\n# Find mode\nget_mode(student_data$Study_Hours_Per_Day)\n## [1] 6.3\n# Mode is most useful for categorical data\nget_mode(student_data$Stress_Level)\n## [1] \"High\"\n\n\n\n\n\n\n\nMeasure\nBest For\nSensitive to Outliers?\n\n\n\n\nMean\nSymmetric distributions, interval/ratio data\nYes\n\n\nMedian\nSkewed distributions, ordinal data\nNo\n\n\nMode\nCategorical data, finding most common value\nNo\n\n\n\nTRY: Compare all three measures\n# Summary function gives us multiple measures\nsummary(student_data$GPA)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   2.240   2.900   3.110   3.116   3.330   4.000\n\n\n\n\nCalculate the mean, median, and mode of GPA.\nWhich measure would you report for wellness ratings? Why?\n\n### Your workspace\n\n\n\n\n\n\nSpread (or dispersion) tells us how varied or spread out our data is.\n\n\nThe simplest measure of spread - the difference between maximum and minimum values.\n# Range\nrange(student_data$Study_Hours_Per_Day)\n## [1]  5 10\n# Difference\nmax(student_data$Study_Hours_Per_Day) - min(student_data$Study_Hours_Per_Day)\n## [1] 5\n# Or use diff()\ndiff(range(student_data$Study_Hours_Per_Day))\n## [1] 5\nLimitation: Range is very sensitive to outliers since it only uses two values.\n\n\n\n\nVariance measures the average squared deviation from the mean:\n\\[s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\\]\nStandard Deviation is the square root of variance (same units as the data):\n\\[s = \\sqrt{s^2}\\]\n# Variance\nvar(student_data$Study_Hours_Per_Day)\n## [1] 2.027458\n# Standard deviation\nsd(student_data$Study_Hours_Per_Day)\n## [1] 1.423888\n# Verify: SD is square root of variance\nsqrt(var(student_data$Study_Hours_Per_Day))\n## [1] 1.423888\nInterpretation: On average, study hours deviate from the mean by about 1.4 hours.\n\n\n\n\nThe IQR is the range of the middle 50% of data (Q3 - Q1). It‚Äôs robust to outliers.\n# Quartiles\nprint(\"Quantile: \")\n## [1] \"Quantile: \"\nquantile(student_data$Study_Hours_Per_Day)\n##   0%  25%  50%  75% 100% \n##  5.0  6.3  7.4  8.7 10.0\n# IQR\nprint(\"IQR: \")\n## [1] \"IQR: \"\nIQR(student_data$Study_Hours_Per_Day)\n## [1] 2.4\n# Verify\nprint(\"Quantile 75% - 25%: \")\n## [1] \"Quantile 75% - 25%: \"\nquantile(student_data$Study_Hours_Per_Day, 0.75) - quantile(student_data$Study_Hours_Per_Day, 0.25)\n## 75% \n## 2.4\nUnderstanding Quartiles:\n\nQ1 (25th percentile): 25% of data falls below this value\nQ2 (50th percentile): The median\nQ3 (75th percentile): 75% of data falls below this value\n\n\nSummary of Spread Measures:\n# All spread measures together\ntibble(\n  Measure = c(\"Range\", \"Variance\", \"Std Dev\", \"IQR\"),\n  Value = c(\n    diff(range(student_data$Study_Hours_Per_Day)),\n    var(student_data$Study_Hours_Per_Day),\n    sd(student_data$Study_Hours_Per_Day),\n    IQR(student_data$Study_Hours_Per_Day)\n  )\n)\n## # A tibble: 4 √ó 2\n##   Measure  Value\n##   &lt;chr&gt;    &lt;dbl&gt;\n## 1 Range     5   \n## 2 Variance  2.03\n## 3 Std Dev   1.42\n## 4 IQR       2.4\n\n\n\n\n\n\n\nHistograms show the frequency distribution of continuous data by dividing it into bins.\n# Basic histogram\nggplot(student_data, aes(x = Study_Hours_Per_Day)) +\n  geom_histogram(binwidth = 3, fill = \"steelblue\", color = \"white\") +\n  labs(\n    title = \"Distribution of Study Hours\",\n    x = \"Study Hours per day\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\nAdjusting bin width:\n# Different bin widths\np1 &lt;- ggplot(student_data, aes(x = Study_Hours_Per_Day)) +\n  geom_histogram(binwidth = 20, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Binwidth = 20\") +\n  theme_minimal()\n\np2 &lt;- ggplot(student_data, aes(x = Study_Hours_Per_Day)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Binwidth = 1\") +\n  theme_minimal()\n\n# Display side by side (requires patchwork or gridExtra)\n# install.packages(\"patchwork\")\nlibrary(patchwork)\np1 + p2\n\n\n\n\n\nDensity plots show a smoothed version of the distribution.\n# Density plot\nggplot(student_data, aes(x = Study_Hours_Per_Day)) +\n  geom_density(fill = \"steelblue\", alpha = 0.5) +\n  labs(\n    title = \"Density of Study Hours\",\n    x = \"Study Hours per Day\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\nComparing groups with density:\n# Density by major\nggplot(student_data, aes(x = Study_Hours_Per_Day, fill = Stress_Level)) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Study Hours by Stress Level\",\n    x = \"Study Hours per Day\",\n    y = \"Density\",\n    fill = \"Major\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nBoxplots display the five-number summary (min, Q1, median, Q3, max) and outliers.\n# Basic boxplot\nggplot(student_data, aes(y = Physical_Activity_Hours_Per_Day)) +\n  geom_boxplot(fill = \"steelblue\", width = 0.3) +\n  labs(\n    title = \"Boxplot of Study Hours\",\n    y = \"Physical Activity Hours per day\"\n  ) +\n  theme_minimal()\n\nReading a boxplot:\n\nBox: Contains the middle 50% of data (Q1 to Q3)\nLine in box: The median\nWhiskers: Extend to min/max within 1.5 √ó IQR\nPoints beyond whiskers: Potential outliers\n\nBoxplots by group:\n# Boxplot by major\nggplot(student_data, aes(x = Stress_Level, y = Study_Hours_Per_Day, fill = Stress_Level)) +\n  geom_boxplot() +\n  labs(\n    title = \"Study Hours by Stress Level\",\n    x = \"Stress Level\",\n    y = \"Study Hours per Day\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nHistogram + Density:\nggplot(student_data, aes(x = Study_Hours_Per_Day)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 binwidth = 1, fill = \"steelblue\", color = \"white\") +\n  geom_density(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Study Hours: Histogram with Density Overlay\",\n    x = \"Study Hours per Day\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\nBoxplot + Points (Jitter):\nggplot(student_data, aes(x = Stress_Level, y = Study_Hours_Per_Day, fill = Stress_Level)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"Study Hours by Stress Level with Individual Points\",\n    x = \"Stress Level\",\n    y = \"Study Hours per Day\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nCreate a histogram of GPA with an appropriate bin width.\nAdd an overlapping density plot of GPA.\nCreate a boxplot comparing GPA across Stress_Level.\nWhich stress level has the highest median GPA? Which has the most variation?\n\n### Your workspace\n\n\n\n\n\n\n\n\nOutliers are observations that are unusually far from other values. They can be:\n\nValid extreme values: Legitimate data points that happen to be unusual\nErrors: Data entry mistakes, measurement errors\nDifferent populations: Data from a different group mixed in\n\nImportant: Don‚Äôt automatically remove outliers! First investigate why they exist.\n\n\n\n\nThe IQR method defines outliers as values beyond:\n\nLower bound: Q1 - 1.5 √ó IQR\nUpper bound: Q3 + 1.5 √ó IQR\n\n# Calculate bounds\nQ1 &lt;- quantile(student_data$Physical_Activity_Hours_Per_Day, 0.25)\nQ3 &lt;- quantile(student_data$Physical_Activity_Hours_Per_Day, 0.75)\nIQR_val &lt;- IQR(student_data$Physical_Activity_Hours_Per_Day)\n\nlower_bound &lt;- Q1 - 1.5 * IQR_val\nupper_bound &lt;- Q3 + 1.5 * IQR_val\n\ncat(\"Q1:\", Q1, \"\\n\")\n## Q1: 2.4\ncat(\"Q3:\", Q3, \"\\n\")\n## Q3: 6.1\ncat(\"IQR:\", IQR_val, \"\\n\")\n## IQR: 3.7\ncat(\"Lower bound:\", lower_bound, \"\\n\")\n## Lower bound: -3.15\ncat(\"Upper bound:\", upper_bound, \"\\n\")\n## Upper bound: 11.65\n# Find outliers\noutliers &lt;- student_data |&gt; \n  filter(Physical_Activity_Hours_Per_Day &lt; lower_bound | Physical_Activity_Hours_Per_Day &gt; upper_bound)\n\ncat(\"Number of outliers:\", nrow(outliers), \"\\n\")\n## Number of outliers: 5\nprint(outliers)\n## # A tibble: 5 √ó 10\n##   Student_ID Study_Hours_Per_Day Extracurricular_Hours_Per‚Ä¶¬π Sleep_Hours_Per_Day\n##        &lt;dbl&gt;               &lt;dbl&gt;                       &lt;dbl&gt;               &lt;dbl&gt;\n## 1         63                 5.1                         0.5                 5.2\n## 2        172                 5.1                         0.3                 6  \n## 3        785                 5.1                         0.3                 6.3\n## 4       1175                 5.3                         0.3                 5.9\n## 5       1716                 5.4                         0.2                 6.3\n## # ‚Ñπ abbreviated name: ¬π‚ÄãExtracurricular_Hours_Per_Day\n## # ‚Ñπ 6 more variables: Social_Hours_Per_Day &lt;dbl&gt;,\n## #   Physical_Activity_Hours_Per_Day &lt;dbl&gt;, GPA &lt;dbl&gt;, Stress_Level &lt;chr&gt;,\n## #   college &lt;chr&gt;, GPA_Level &lt;chr&gt;\n\n\n\n\nThe Z-score measures how many standard deviations a value is from the mean:\n\\[z = \\frac{x - \\bar{x}}{s}\\]\nValues with |z| &gt; 2 or |z| &gt; 3 are often considered outliers.\n# Calculate z-scores\nstudent_data &lt;- student_data |&gt; \n  mutate(\n    Physical_Activity_Hours_Per_Day_z = (Physical_Activity_Hours_Per_Day - mean(Physical_Activity_Hours_Per_Day)) / sd(Physical_Activity_Hours_Per_Day)\n  )\n\n# View z-scores\nstudent_data |&gt; \n  select(Student_ID, Stress_Level, Physical_Activity_Hours_Per_Day, Physical_Activity_Hours_Per_Day_z) |&gt; \n  arrange(desc(abs(Physical_Activity_Hours_Per_Day_z))) |&gt; \n  head(10)\n## # A tibble: 10 √ó 4\n##    Student_ID Stress_Level Physical_Activity_Hours_Per_‚Ä¶¬π Physical_Activity_Ho‚Ä¶¬≤\n##         &lt;dbl&gt; &lt;chr&gt;                                 &lt;dbl&gt;                  &lt;dbl&gt;\n##  1         63 High                                   13                     3.45\n##  2        172 Low                                    12.4                   3.21\n##  3       1175 High                                   12.3                   3.17\n##  4        785 Low                                    12.1                   3.09\n##  5       1716 Low                                    11.7                   2.93\n##  6        616 High                                   11.6                   2.89\n##  7        546 High                                   11.4                   2.81\n##  8        251 Low                                    11.2                   2.73\n##  9        383 High                                   11.1                   2.69\n## 10       1077 High                                   11.1                   2.69\n## # ‚Ñπ abbreviated names: ¬π‚ÄãPhysical_Activity_Hours_Per_Day,\n## #   ¬≤‚ÄãPhysical_Activity_Hours_Per_Day_z\n# Find outliers using z-score &gt; 2\nz_outliers &lt;- student_data |&gt; \n  filter(abs(Physical_Activity_Hours_Per_Day_z) &gt; 2)\n\ncat(\"Outliers (|z| &gt; 2):\", nrow(z_outliers), \"\\n\")\n## Outliers (|z| &gt; 2): 60\nprint(z_outliers |&gt; select(Student_ID, Stress_Level, Physical_Activity_Hours_Per_Day, Physical_Activity_Hours_Per_Day_z))\n## # A tibble: 60 √ó 4\n##    Student_ID Stress_Level Physical_Activity_Hours_Per_‚Ä¶¬π Physical_Activity_Ho‚Ä¶¬≤\n##         &lt;dbl&gt; &lt;chr&gt;                                 &lt;dbl&gt;                  &lt;dbl&gt;\n##  1         63 High                                   13                     3.45\n##  2        126 Low                                    10.7                   2.53\n##  3        172 Low                                    12.4                   3.21\n##  4        187 Low                                     9.5                   2.06\n##  5        251 Low                                    11.2                   2.73\n##  6        254 High                                    9.7                   2.14\n##  7        269 Moderate                               11                     2.65\n##  8        284 Moderate                                9.8                   2.18\n##  9        323 High                                   11                     2.65\n## 10        381 Low                                     9.8                   2.18\n## # ‚Ñπ 50 more rows\n## # ‚Ñπ abbreviated names: ¬π‚ÄãPhysical_Activity_Hours_Per_Day,\n## #   ¬≤‚ÄãPhysical_Activity_Hours_Per_Day_z\n\n\n\n\nBoxplot (automatically shows outliers):\n# Boxplot shows outliers as points\nggplot(student_data, aes(x = Stress_Level, y = Physical_Activity_Hours_Per_Day, fill = Stress_Level)) +\n  geom_boxplot() +\n  labs(\n    title = \"Boxplot Showing Outliers\",\n    x = \"Stress Level\",\n    y = \"Physical Activity Hours\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nScatter plot with outliers highlighted:\n# Identify outliers\nstudent_data &lt;- student_data |&gt; \n  mutate(\n    is_outlier = Physical_Activity_Hours_Per_Day &lt; lower_bound | Physical_Activity_Hours_Per_Day &gt; upper_bound\n  )\n\nggplot(student_data, aes(x = Physical_Activity_Hours_Per_Day, y = GPA, color = is_outlier)) +\n  geom_point(size = 3) +\n  scale_color_manual(values = c(\"FALSE\" = \"steelblue\", \"TRUE\" = \"red\"),\n                     labels = c(\"Normal\", \"Outlier\")) +\n  labs(\n    title = \"Scatter Plot with Outliers Highlighted\",\n    x = \"Physical Activity Hours\",\n    y = \"GPA\",\n    color = \"Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nCalculate the IQR bounds for GPA.\nIdentify any outliers using the IQR method.\nCalculate z-scores for GPA and find values with |z| &gt; 2.\nCreate a visualization that highlights outliers.\n\n### Your workspace\n\n\n\n\n\n\n\n\nThe combination of group_by() and summarize() is powerful for calculating statistics by group.\n# Mean study hours by stress level\nstudent_data |&gt; \n  group_by(Stress_Level) |&gt; \n  summarize(mean_hours = mean(Study_Hours_Per_Day))\n## # A tibble: 3 √ó 2\n##   Stress_Level mean_hours\n##   &lt;chr&gt;             &lt;dbl&gt;\n## 1 High               8.39\n## 2 Low                5.47\n## 3 Moderate           6.97\n\n\n\n\nCalculate multiple statistics at once:\n# Comprehensive summary by stress level\nsummary_by_stresslevel &lt;- student_data |&gt; \n  group_by(Stress_Level) |&gt; \n  summarize(\n    n = n(),\n    mean_hours = mean(Study_Hours_Per_Day),\n    median_hours = median(Study_Hours_Per_Day),\n    sd_hours = sd(Study_Hours_Per_Day),\n    min_hours = min(Study_Hours_Per_Day),\n    max_hours = max(Study_Hours_Per_Day),\n    mean_score = mean(GPA)\n  )\n\nprint(summary_by_major)\n## Error: object 'summary_by_major' not found\nGroup by multiple variables:\n# Summary by Stress Level AND college\nstudent_data |&gt; \n  group_by(Stress_Level, college) |&gt; \n  summarize(\n    n = n(),\n    mean_hours = mean(Study_Hours_Per_Day),\n    median_hours = median(Study_Hours_Per_Day),\n    .groups = \"drop\"\n  )\n## # A tibble: 15 √ó 5\n##    Stress_Level college             n mean_hours median_hours\n##    &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n##  1 High         Arts & Sciences   170       8.36          8.6\n##  2 High         Communication     148       8.46          8.6\n##  3 High         Education         217       8.38          8.7\n##  4 High         Humanities        269       8.34          8.7\n##  5 High         Medicine          225       8.41          8.7\n##  6 Low          Arts & Sciences    51       5.47          5.4\n##  7 Low          Communication      35       5.42          5.4\n##  8 Low          Education          70       5.50          5.5\n##  9 Low          Humanities         84       5.47          5.4\n## 10 Low          Medicine           57       5.48          5.5\n## 11 Moderate     Arts & Sciences   130       7.06          7.1\n## 12 Moderate     Communication      97       6.92          6.9\n## 13 Moderate     Education         147       6.99          7.1\n## 14 Moderate     Humanities        141       6.93          6.9\n## 15 Moderate     Medicine          159       6.94          6.9\n\n\n\n\nBar chart of means:\nggplot(summary_by_major, aes(x = Stress_Level, y = mean_hours, fill = Stress_Level)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean_hours - sd_hours, \n                    ymax = mean_hours + sd_hours),\n                width = 0.2) +\n  labs(\n    title = \"Mean Study Hours by Stress Level (¬±1 SD)\",\n    x = \"Stress Level\",\n    y = \"Mean Study Hours\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n## Error: object 'summary_by_major' not found\nGrouped bar chart:\n# Summary by stress level and college\nsummary_stress_college &lt;- student_data |&gt; \n  group_by(Stress_Level, college) |&gt; \n  summarize(mean_hours = mean(Study_Hours_Per_Day), .groups = \"drop\")\n\nggplot(summary_stress_college, aes(x = Stress_Level, y = mean_hours, fill = college)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Mean Study Hours by Stress Level and College\",\n    x = \"Stress Level\",\n    y = \"Mean Study Hours\",\n    fill = \"College\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCorrelation measures the strength and direction of the linear relationship between two numeric variables.\n\nr = 1: Perfect positive correlation\nr = 0: No linear correlation\nr = -1: Perfect negative correlation\n\nInterpretation guidelines:\n\n\n\n\nr\n\n\n\n\n0.00 - 0.19\nVery weak\n\n\n0.20 - 0.39\nWeak\n\n\n0.40 - 0.59\nModerate\n\n\n0.60 - 0.79\nStrong\n\n\n0.80 - 1.00\nVery strong\n\n\n\n\n\n\n\n# Pearson correlation (default)\ncor(student_data$Study_Hours_Per_Day, student_data$GPA)\n## [1] 0.734468\nCorrelation test with p-value:\n# Correlation test\ncor_test &lt;- cor.test(student_data$Study_Hours_Per_Day, student_data$GPA)\ncor_test\n## \n##  Pearson's product-moment correlation\n## \n## data:  student_data$Study_Hours_Per_Day and student_data$GPA\n## t = 48.376, df = 1998, p-value &lt; 2.2e-16\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.7136099 0.7540250\n## sample estimates:\n##      cor \n## 0.734468\nInterpretation: There is a very strong positive correlation (r = 0.734) between study hours and exam scores, p &lt; 0.001.\n\n\n\n\nWhen you have multiple numeric variables, create a correlation matrix:\n# Select numeric columns\nnumeric_vars &lt;- student_data |&gt; \n  select(Study_Hours_Per_Day, GPA)\n\n# Correlation matrix\ncor(numeric_vars)\n##                     Study_Hours_Per_Day      GPA\n## Study_Hours_Per_Day            1.000000 0.734468\n## GPA                            0.734468 1.000000\nWith more variables:\n# Correlation matrix\nnumeric_vars &lt;- student_data |&gt; \n  select(Study_Hours_Per_Day, GPA, Sleep_Hours_Per_Day, Physical_Activity_Hours_Per_Day)\n\nround(cor(numeric_vars), 3)\n##                                 Study_Hours_Per_Day    GPA Sleep_Hours_Per_Day\n## Study_Hours_Per_Day                           1.000  0.734               0.027\n## GPA                                           0.734  1.000              -0.004\n## Sleep_Hours_Per_Day                           0.027 -0.004               1.000\n## Physical_Activity_Hours_Per_Day              -0.488 -0.341              -0.470\n##                                 Physical_Activity_Hours_Per_Day\n## Study_Hours_Per_Day                                      -0.488\n## GPA                                                      -0.341\n## Sleep_Hours_Per_Day                                      -0.470\n## Physical_Activity_Hours_Per_Day                           1.000\n\n\n\n\nScatter plot with correlation:\nggplot(student_data, aes(x = Study_Hours_Per_Day, y = GPA)) +\n  geom_point(color = \"steelblue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = paste(\"Study Hours vs GPA (r =\", \n                  round(cor(student_data$Study_Hours_Per_Day, student_data$GPA), 2), \")\"),\n    x = \"Study Hours\",\n    y = \"GPA\"\n  ) +\n  theme_minimal()\n\nCorrelation heatmap:\n# Create correlation matrix\ncor_matrix &lt;- cor(numeric_vars)\n\n# Convert to long format for ggplot\ncor_long &lt;- cor_matrix |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"var1\") |&gt; \n  pivot_longer(-var1, names_to = \"var2\", values_to = \"correlation\")\n\n# Heatmap\nggplot(cor_long, aes(x = var1, y = var2, fill = correlation)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = round(correlation, 2)), color = \"black\", size = 4) +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", \n                       midpoint = 0, limits = c(-1, 1)) +\n  labs(\n    title = \"Correlation Heatmap\",\n    x = \"\", y = \"\",\n    fill = \"Correlation\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nCalculate the correlation between GPA and Sleep_Hours_Per_day.\nIs the correlation statistically significant? (Use cor.test())\nCreate a scatter plot showing this relationship.\nWhat does this correlation tell us?\n\n### Your workspace\n\n\n\n\n\n\nCross-tabulations (contingency tables) summarize the relationship between categorical variables.\n\n\nSingle variable:\n# Using tidyverse\nstudent_data |&gt; \n  count(college)\n## # A tibble: 5 √ó 2\n##   college             n\n##   &lt;chr&gt;           &lt;int&gt;\n## 1 Arts & Sciences   351\n## 2 Communication     280\n## 3 Education         434\n## 4 Humanities        494\n## 5 Medicine          441\nTwo variables (cross-tabulation):\n# Using tidyverse\nstudent_data |&gt; \n  count(college, Stress_Level) |&gt; \n  pivot_wider(names_from = Stress_Level, values_from = n)\n## # A tibble: 5 √ó 4\n##   college          High   Low Moderate\n##   &lt;chr&gt;           &lt;int&gt; &lt;int&gt;    &lt;int&gt;\n## 1 Arts & Sciences   170    51      130\n## 2 Communication     148    35       97\n## 3 Education         217    70      147\n## 4 Humanities        269    84      141\n## 5 Medicine          225    57      159\n\n\n\n\n# Percentages by group\nstudent_data |&gt; \n  count(college, Stress_Level) |&gt; \n  group_by(college) |&gt; \n  mutate(\n    percentage = n / sum(n) * 100\n  )\n## # A tibble: 15 √ó 4\n## # Groups:   college [5]\n##    college         Stress_Level     n percentage\n##    &lt;chr&gt;           &lt;chr&gt;        &lt;int&gt;      &lt;dbl&gt;\n##  1 Arts & Sciences High           170       48.4\n##  2 Arts & Sciences Low             51       14.5\n##  3 Arts & Sciences Moderate       130       37.0\n##  4 Communication   High           148       52.9\n##  5 Communication   Low             35       12.5\n##  6 Communication   Moderate        97       34.6\n##  7 Education       High           217       50  \n##  8 Education       Low             70       16.1\n##  9 Education       Moderate       147       33.9\n## 10 Humanities      High           269       54.5\n## 11 Humanities      Low             84       17.0\n## 12 Humanities      Moderate       141       28.5\n## 13 Medicine        High           225       51.0\n## 14 Medicine        Low             57       12.9\n## 15 Medicine        Moderate       159       36.1\n\n\n\n\nStacked bar chart:\nggplot(student_data, aes(x = college, fill = Stress_Level)) +\n  geom_bar() +\n  labs(\n    title = \"Distribution of Stress Level by College\",\n    x = \"College\",\n    y = \"Count\",\n    fill = \"Stress Level\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_minimal()\n\nGrouped bar chart:\nggplot(student_data, aes(x = college, fill = Stress_Level)) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    title = \"Distribution of Stress Level by College\",\n    x = \"College\",\n    y = \"Count\",\n    fill = \"Stress Level\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_minimal()\n\nProportional bar chart:\nggplot(student_data, aes(x = college, fill = Stress_Level)) +\n  geom_bar(position = \"fill\") +\n  labs(\n    title = \"Proportion of Stress Level by College\",\n    x = \"College\",\n    y = \"Proportion\",\n    fill = \"Stress Level\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal()\n\n\nThree-way cross-tabulation:\n# College by Stress Level by GPA\nstudent_data |&gt; \n  count(college, Stress_Level, GPA_Level) |&gt; \n  pivot_wider(names_from = GPA_Level, values_from = n, values_fill = 0)\n## # A tibble: 15 √ó 5\n##    college         Stress_Level HighGPA LowGPA MediumGPA\n##    &lt;chr&gt;           &lt;chr&gt;          &lt;int&gt;  &lt;int&gt;     &lt;int&gt;\n##  1 Arts & Sciences High              82     12        76\n##  2 Arts & Sciences Low                0     33        18\n##  3 Arts & Sciences Moderate          16     31        83\n##  4 Communication   High              61     15        72\n##  5 Communication   Low                0     24        11\n##  6 Communication   Moderate          12     29        56\n##  7 Education       High              89     17       111\n##  8 Education       Low                0     41        29\n##  9 Education       Moderate          12     47        88\n## 10 Humanities      High             104     31       134\n## 11 Humanities      Low                1     53        30\n## 12 Humanities      Moderate           9     45        87\n## 13 Medicine        High              86     25       114\n## 14 Medicine        Low                2     39        16\n## 15 Medicine        Moderate          14     45       100\n\n\n\n\nCreate a cross-tabulation of College and GPA_Level.\nWhat percentage of Medicine students have ‚ÄúHigh‚Äù GPA?\nCreate a proportional bar chart showing GPA levels by college\nWhich major has the highest proportion of GPA_level to Stress_Level?\n\n### Your workspace\n\n\n\n\n\n\nUsing the mtcars dataset:\ndata(mtcars)\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nCentral Tendency: Calculate mean, median for mpg (miles per gallon).\nSpread: Calculate standard deviation and IQR for mpg.\nDistribution: Create a histogram and density plot of mpg.\nOutliers: Use the IQR method to identify any outliers in hp (horsepower).\nGrouped Statistics: Calculate mean mpg grouped by number of cylinders (cyl).\nCorrelation: Calculate the correlation between mpg and wt (weight). Create a scatter plot.\nCross-tabulation: Create a cross-tabulation of cyl and gear. Visualize with a bar chart.\n\n### Your workspace\n\n\n\n\n\n\n\n\n\n\n\n\nFunction/Concept\nDescription\nCode Example\n\n\n\n\nmean()\nCalculate arithmetic mean\nmean(x, na.rm = TRUE)\n\n\nmedian()\nCalculate median (middle value)\nmedian(x)\n\n\nsd()\nCalculate standard deviation\nsd(x)\n\n\nvar()\nCalculate variance\nvar(x)\n\n\nrange()\nGet minimum and maximum\nrange(x)\n\n\nIQR()\nCalculate interquartile range\nIQR(x)\n\n\nquantile()\nCalculate quantiles/percentiles\nquantile(x, 0.25)\n\n\nsummary()\nGet 5-number summary + mean\nsummary(x)\n\n\ntable()\nCreate frequency table\ntable(df$category)\n\n\nprop.table()\nConvert counts to proportions\nprop.table(table(x))\n\n\ncor()\nCalculate correlation\ncor(x, y)\n\n\ncor.test()\nCorrelation test with p-value\ncor.test(x, y)\n\n\ngeom_histogram()\nCreate histogram\ngeom_histogram(binwidth = 5)\n\n\ngeom_density()\nCreate density plot\ngeom_density(fill = \"blue\")\n\n\ngeom_boxplot()\nCreate boxplot\ngeom_boxplot()\n\n\ngeom_tile()\nCreate heatmap tiles\ngeom_tile(aes(fill = value))\n\n\ngroup_by() + summarize()\nGrouped statistics\ngroup_by(var) |&gt; summarize(mean = mean(x))\n\n\ncount()\nCount observations\ncount(var1, var2)\n\n\npivot_wider()\nReshape long to wide\npivot_wider(names_from, values_from)\n\n\nZ-score\nStandardize values\n(x - mean(x)) / sd(x)\n\n\nIQR outliers\nValues beyond 1.5√óIQR\nQ1 - 1.5*IQR or Q3 + 1.5*IQR"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L5-github.html#lecture-5-table-of-contents",
    "href": "Spring2026/LectureSlides/bigdata_L5-github.html#lecture-5-table-of-contents",
    "title": "Descriptive Data",
    "section": "",
    "text": "Section\nTopic\n\n\n\n\n1\nIntroduction to Descriptive Statistics\n\n\n1.1\nWhy Descriptive Statistics?\n\n\n1.2\nTypes of Variables\n\n\n2\nMeasures of Central Tendency\n\n\n2.1\nMean\n\n\n2.2\nMedian\n\n\n2.3\nMode\n\n\n2.4\nWhen to Use Which?\n\n\n3\nMeasures of Spread\n\n\n3.1\nRange\n\n\n3.2\nVariance and Standard Deviation\n\n\n3.3\nInterquartile Range (IQR)\n\n\n4\nVisualizing Distributions\n\n\n4.1\nHistograms\n\n\n4.2\nDensity Plots\n\n\n4.3\nBoxplots\n\n\n4.4\nCombining Visualizations\n\n\n5\nDetecting Outliers\n\n\n5.1\nWhat are Outliers?\n\n\n5.2\nIQR Method\n\n\n5.3\nZ-Score Method\n\n\n5.4\nVisualizing Outliers\n\n\n6\nGrouped Summary Statistics\n\n\n6.1\nUsing group_by() and summarize()\n\n\n6.2\nMultiple Summary Statistics\n\n\n6.3\nVisualizing Grouped Data\n\n\n7\nCorrelation\n\n\n7.1\nUnderstanding Correlation\n\n\n7.2\nCalculating Correlation\n\n\n7.3\nCorrelation Matrices\n\n\n7.4\nVisualizing Correlations\n\n\n8\nCross-Tabulations\n\n\n8.1\nCreating Frequency Tables\n\n\n8.2\nProportions and Percentages\n\n\n8.3\nVisualizing Cross-Tabulations\n\n\n\n\nALWAYS load our libraries first\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L5-github.html#introduction-to-descriptive-statistics",
    "href": "Spring2026/LectureSlides/bigdata_L5-github.html#introduction-to-descriptive-statistics",
    "title": "Descriptive Data",
    "section": "",
    "text": "Descriptive statistics help us understand and summarize our data before diving into more complex analyses. They answer questions like:\n\nWhat is the typical value in my data?\nHow spread out are my values?\nAre there any unusual observations?\nHow are my variables related?\n\nDescriptive vs.¬†Inferential Statistics:\n\n\n\n\n\n\n\nDescriptive\nInferential\n\n\n\n\nSummarizes data\nMakes predictions\n\n\nDescribes what IS\nEstimates what MIGHT BE\n\n\nNo uncertainty\nIncludes uncertainty\n\n\ne.g., ‚ÄúThe mean age is 25‚Äù\ne.g., ‚ÄúThe population mean is likely between 23-27‚Äù\n\n\n\n\n\n\n\nUnderstanding variable types helps us choose the right statistics:\nNumeric (Quantitative):\n\nContinuous: Can take any value (e.g., height, weight, temperature)\nDiscrete: Only whole numbers (e.g., count of children, number of books)\n\nCategorical (Qualitative):\n\nNominal: No natural order (e.g., color, gender, major)\nOrdinal: Has natural order (e.g., education level, satisfaction rating)\n\n# Create our example dataset\nstudent_data &lt;- read_csv(\"https://raw.githubusercontent.com/aysedeniz09/IntroCSS/refs/heads/main/data/student_lifestyle_dataset.csv\")\nIdentify variable types:\n# Check each variable\nstr(student_data)\n## spc_tbl_ [2,000 √ó 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ Student_ID                     : num [1:2000] 1 2 3 4 5 6 7 8 9 10 ...\n##  $ Study_Hours_Per_Day            : num [1:2000] 6.9 5.3 5.1 6.5 8.1 6 8 8.4 5.2 7.7 ...\n##  $ Extracurricular_Hours_Per_Day  : num [1:2000] 3.8 3.5 3.9 2.1 0.6 2.1 0.7 1.8 3.6 0.7 ...\n##  $ Sleep_Hours_Per_Day            : num [1:2000] 8.7 8 9.2 7.2 6.5 8 5.3 5.6 6.3 9.8 ...\n##  $ Social_Hours_Per_Day           : num [1:2000] 2.8 4.2 1.2 1.7 2.2 0.3 5.7 3 4 4.5 ...\n##  $ Physical_Activity_Hours_Per_Day: num [1:2000] 1.8 3 4.6 6.5 6.6 7.6 4.3 5.2 4.9 1.3 ...\n##  $ GPA                            : num [1:2000] 2.99 2.75 2.67 2.88 3.51 2.85 3.08 3.2 2.82 2.76 ...\n##  $ Stress_Level                   : chr [1:2000] \"Moderate\" \"Low\" \"Low\" \"Moderate\" ...\n##  $ college                        : chr [1:2000] \"Education\" \"Humanities\" \"Arts & Sciences\" \"Communication\" ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   Student_ID = col_double(),\n##   ..   Study_Hours_Per_Day = col_double(),\n##   ..   Extracurricular_Hours_Per_Day = col_double(),\n##   ..   Sleep_Hours_Per_Day = col_double(),\n##   ..   Social_Hours_Per_Day = col_double(),\n##   ..   Physical_Activity_Hours_Per_Day = col_double(),\n##   ..   GPA = col_double(),\n##   ..   Stress_Level = col_character(),\n##   ..   college = col_character()\n##   .. )\n##  - attr(*, \"problems\")=&lt;externalptr&gt;\nLet‚Äôs create GPA levels for some categorical data\nstudent_data &lt;- student_data |&gt; \n  mutate(\n        GPA_Level = case_when(\n      GPA &lt; 2.90 ~ \"LowGPA\",\n      GPA &gt;= 2.90 & GPA &lt;= 3.33 ~ \"MediumGPA\",\n      GPA &gt; 3.33 ~ \"HighGPA\"\n    ))\n\nstr(student_data)\n## tibble [2,000 √ó 10] (S3: tbl_df/tbl/data.frame)\n##  $ Student_ID                     : num [1:2000] 1 2 3 4 5 6 7 8 9 10 ...\n##  $ Study_Hours_Per_Day            : num [1:2000] 6.9 5.3 5.1 6.5 8.1 6 8 8.4 5.2 7.7 ...\n##  $ Extracurricular_Hours_Per_Day  : num [1:2000] 3.8 3.5 3.9 2.1 0.6 2.1 0.7 1.8 3.6 0.7 ...\n##  $ Sleep_Hours_Per_Day            : num [1:2000] 8.7 8 9.2 7.2 6.5 8 5.3 5.6 6.3 9.8 ...\n##  $ Social_Hours_Per_Day           : num [1:2000] 2.8 4.2 1.2 1.7 2.2 0.3 5.7 3 4 4.5 ...\n##  $ Physical_Activity_Hours_Per_Day: num [1:2000] 1.8 3 4.6 6.5 6.6 7.6 4.3 5.2 4.9 1.3 ...\n##  $ GPA                            : num [1:2000] 2.99 2.75 2.67 2.88 3.51 2.85 3.08 3.2 2.82 2.76 ...\n##  $ Stress_Level                   : chr [1:2000] \"Moderate\" \"Low\" \"Low\" \"Moderate\" ...\n##  $ college                        : chr [1:2000] \"Education\" \"Humanities\" \"Arts & Sciences\" \"Communication\" ...\n##  $ GPA_Level                      : chr [1:2000] \"MediumGPA\" \"LowGPA\" \"LowGPA\" \"LowGPA\" ..."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L5-github.html#measures-of-central-tendency",
    "href": "Spring2026/LectureSlides/bigdata_L5-github.html#measures-of-central-tendency",
    "title": "Descriptive Data",
    "section": "",
    "text": "Central tendency tells us about the typical or center value in our data.\n\n\nThe arithmetic mean (average) is the sum of all values divided by the number of values.\n\\[\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\]\n# Calculate mean\nmean(student_data$Study_Hours_Per_Day)\n## [1] 7.4758\n# Mean with missing values\nx &lt;- c(10, 20, NA, 30, 40)\nmean(x)  # Returns NA\n## [1] NA\n# Remove NA values\nmean(x, na.rm = TRUE)\n## [1] 25\n\n\n\n\nThe median is the middle value when data is sorted. It‚Äôs more robust to outliers than the mean.\n# Calculate median\nmedian(student_data$Study_Hours_Per_Day)\n## [1] 7.4\n\n\n\n\nThe mode is the most frequently occurring value. R doesn‚Äôt have a built-in mode function, so we create one:\n# Create a mode function\nget_mode &lt;- function(x) {\n  unique_x &lt;- unique(x)\n  unique_x[which.max(tabulate(match(x, unique_x)))]\n}\n\n# Find mode\nget_mode(student_data$Study_Hours_Per_Day)\n## [1] 6.3\n# Mode is most useful for categorical data\nget_mode(student_data$Stress_Level)\n## [1] \"High\"\n\n\n\n\n\n\n\nMeasure\nBest For\nSensitive to Outliers?\n\n\n\n\nMean\nSymmetric distributions, interval/ratio data\nYes\n\n\nMedian\nSkewed distributions, ordinal data\nNo\n\n\nMode\nCategorical data, finding most common value\nNo\n\n\n\nTRY: Compare all three measures\n# Summary function gives us multiple measures\nsummary(student_data$GPA)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   2.240   2.900   3.110   3.116   3.330   4.000\n\n\n\n\nCalculate the mean, median, and mode of GPA.\nWhich measure would you report for wellness ratings? Why?\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L5-github.html#measures-of-spread",
    "href": "Spring2026/LectureSlides/bigdata_L5-github.html#measures-of-spread",
    "title": "Descriptive Data",
    "section": "",
    "text": "Spread (or dispersion) tells us how varied or spread out our data is.\n\n\nThe simplest measure of spread - the difference between maximum and minimum values.\n# Range\nrange(student_data$Study_Hours_Per_Day)\n## [1]  5 10\n# Difference\nmax(student_data$Study_Hours_Per_Day) - min(student_data$Study_Hours_Per_Day)\n## [1] 5\n# Or use diff()\ndiff(range(student_data$Study_Hours_Per_Day))\n## [1] 5\nLimitation: Range is very sensitive to outliers since it only uses two values.\n\n\n\n\nVariance measures the average squared deviation from the mean:\n\\[s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\\]\nStandard Deviation is the square root of variance (same units as the data):\n\\[s = \\sqrt{s^2}\\]\n# Variance\nvar(student_data$Study_Hours_Per_Day)\n## [1] 2.027458\n# Standard deviation\nsd(student_data$Study_Hours_Per_Day)\n## [1] 1.423888\n# Verify: SD is square root of variance\nsqrt(var(student_data$Study_Hours_Per_Day))\n## [1] 1.423888\nInterpretation: On average, study hours deviate from the mean by about 1.4 hours.\n\n\n\n\nThe IQR is the range of the middle 50% of data (Q3 - Q1). It‚Äôs robust to outliers.\n# Quartiles\nprint(\"Quantile: \")\n## [1] \"Quantile: \"\nquantile(student_data$Study_Hours_Per_Day)\n##   0%  25%  50%  75% 100% \n##  5.0  6.3  7.4  8.7 10.0\n# IQR\nprint(\"IQR: \")\n## [1] \"IQR: \"\nIQR(student_data$Study_Hours_Per_Day)\n## [1] 2.4\n# Verify\nprint(\"Quantile 75% - 25%: \")\n## [1] \"Quantile 75% - 25%: \"\nquantile(student_data$Study_Hours_Per_Day, 0.75) - quantile(student_data$Study_Hours_Per_Day, 0.25)\n## 75% \n## 2.4\nUnderstanding Quartiles:\n\nQ1 (25th percentile): 25% of data falls below this value\nQ2 (50th percentile): The median\nQ3 (75th percentile): 75% of data falls below this value\n\n\nSummary of Spread Measures:\n# All spread measures together\ntibble(\n  Measure = c(\"Range\", \"Variance\", \"Std Dev\", \"IQR\"),\n  Value = c(\n    diff(range(student_data$Study_Hours_Per_Day)),\n    var(student_data$Study_Hours_Per_Day),\n    sd(student_data$Study_Hours_Per_Day),\n    IQR(student_data$Study_Hours_Per_Day)\n  )\n)\n## # A tibble: 4 √ó 2\n##   Measure  Value\n##   &lt;chr&gt;    &lt;dbl&gt;\n## 1 Range     5   \n## 2 Variance  2.03\n## 3 Std Dev   1.42\n## 4 IQR       2.4"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L5-github.html#visualizing-distributions",
    "href": "Spring2026/LectureSlides/bigdata_L5-github.html#visualizing-distributions",
    "title": "Descriptive Data",
    "section": "",
    "text": "Histograms show the frequency distribution of continuous data by dividing it into bins.\n# Basic histogram\nggplot(student_data, aes(x = Study_Hours_Per_Day)) +\n  geom_histogram(binwidth = 3, fill = \"steelblue\", color = \"white\") +\n  labs(\n    title = \"Distribution of Study Hours\",\n    x = \"Study Hours per day\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\nAdjusting bin width:\n# Different bin widths\np1 &lt;- ggplot(student_data, aes(x = Study_Hours_Per_Day)) +\n  geom_histogram(binwidth = 20, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Binwidth = 20\") +\n  theme_minimal()\n\np2 &lt;- ggplot(student_data, aes(x = Study_Hours_Per_Day)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Binwidth = 1\") +\n  theme_minimal()\n\n# Display side by side (requires patchwork or gridExtra)\n# install.packages(\"patchwork\")\nlibrary(patchwork)\np1 + p2\n\n\n\n\n\nDensity plots show a smoothed version of the distribution.\n# Density plot\nggplot(student_data, aes(x = Study_Hours_Per_Day)) +\n  geom_density(fill = \"steelblue\", alpha = 0.5) +\n  labs(\n    title = \"Density of Study Hours\",\n    x = \"Study Hours per Day\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\nComparing groups with density:\n# Density by major\nggplot(student_data, aes(x = Study_Hours_Per_Day, fill = Stress_Level)) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Study Hours by Stress Level\",\n    x = \"Study Hours per Day\",\n    y = \"Density\",\n    fill = \"Major\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nBoxplots display the five-number summary (min, Q1, median, Q3, max) and outliers.\n# Basic boxplot\nggplot(student_data, aes(y = Physical_Activity_Hours_Per_Day)) +\n  geom_boxplot(fill = \"steelblue\", width = 0.3) +\n  labs(\n    title = \"Boxplot of Study Hours\",\n    y = \"Physical Activity Hours per day\"\n  ) +\n  theme_minimal()\n\nReading a boxplot:\n\nBox: Contains the middle 50% of data (Q1 to Q3)\nLine in box: The median\nWhiskers: Extend to min/max within 1.5 √ó IQR\nPoints beyond whiskers: Potential outliers\n\nBoxplots by group:\n# Boxplot by major\nggplot(student_data, aes(x = Stress_Level, y = Study_Hours_Per_Day, fill = Stress_Level)) +\n  geom_boxplot() +\n  labs(\n    title = \"Study Hours by Stress Level\",\n    x = \"Stress Level\",\n    y = \"Study Hours per Day\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nHistogram + Density:\nggplot(student_data, aes(x = Study_Hours_Per_Day)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 binwidth = 1, fill = \"steelblue\", color = \"white\") +\n  geom_density(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Study Hours: Histogram with Density Overlay\",\n    x = \"Study Hours per Day\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\nBoxplot + Points (Jitter):\nggplot(student_data, aes(x = Stress_Level, y = Study_Hours_Per_Day, fill = Stress_Level)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"Study Hours by Stress Level with Individual Points\",\n    x = \"Stress Level\",\n    y = \"Study Hours per Day\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nCreate a histogram of GPA with an appropriate bin width.\nAdd an overlapping density plot of GPA.\nCreate a boxplot comparing GPA across Stress_Level.\nWhich stress level has the highest median GPA? Which has the most variation?\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L5-github.html#detecting-outliers",
    "href": "Spring2026/LectureSlides/bigdata_L5-github.html#detecting-outliers",
    "title": "Descriptive Data",
    "section": "",
    "text": "Outliers are observations that are unusually far from other values. They can be:\n\nValid extreme values: Legitimate data points that happen to be unusual\nErrors: Data entry mistakes, measurement errors\nDifferent populations: Data from a different group mixed in\n\nImportant: Don‚Äôt automatically remove outliers! First investigate why they exist.\n\n\n\n\nThe IQR method defines outliers as values beyond:\n\nLower bound: Q1 - 1.5 √ó IQR\nUpper bound: Q3 + 1.5 √ó IQR\n\n# Calculate bounds\nQ1 &lt;- quantile(student_data$Physical_Activity_Hours_Per_Day, 0.25)\nQ3 &lt;- quantile(student_data$Physical_Activity_Hours_Per_Day, 0.75)\nIQR_val &lt;- IQR(student_data$Physical_Activity_Hours_Per_Day)\n\nlower_bound &lt;- Q1 - 1.5 * IQR_val\nupper_bound &lt;- Q3 + 1.5 * IQR_val\n\ncat(\"Q1:\", Q1, \"\\n\")\n## Q1: 2.4\ncat(\"Q3:\", Q3, \"\\n\")\n## Q3: 6.1\ncat(\"IQR:\", IQR_val, \"\\n\")\n## IQR: 3.7\ncat(\"Lower bound:\", lower_bound, \"\\n\")\n## Lower bound: -3.15\ncat(\"Upper bound:\", upper_bound, \"\\n\")\n## Upper bound: 11.65\n# Find outliers\noutliers &lt;- student_data |&gt; \n  filter(Physical_Activity_Hours_Per_Day &lt; lower_bound | Physical_Activity_Hours_Per_Day &gt; upper_bound)\n\ncat(\"Number of outliers:\", nrow(outliers), \"\\n\")\n## Number of outliers: 5\nprint(outliers)\n## # A tibble: 5 √ó 10\n##   Student_ID Study_Hours_Per_Day Extracurricular_Hours_Per‚Ä¶¬π Sleep_Hours_Per_Day\n##        &lt;dbl&gt;               &lt;dbl&gt;                       &lt;dbl&gt;               &lt;dbl&gt;\n## 1         63                 5.1                         0.5                 5.2\n## 2        172                 5.1                         0.3                 6  \n## 3        785                 5.1                         0.3                 6.3\n## 4       1175                 5.3                         0.3                 5.9\n## 5       1716                 5.4                         0.2                 6.3\n## # ‚Ñπ abbreviated name: ¬π‚ÄãExtracurricular_Hours_Per_Day\n## # ‚Ñπ 6 more variables: Social_Hours_Per_Day &lt;dbl&gt;,\n## #   Physical_Activity_Hours_Per_Day &lt;dbl&gt;, GPA &lt;dbl&gt;, Stress_Level &lt;chr&gt;,\n## #   college &lt;chr&gt;, GPA_Level &lt;chr&gt;\n\n\n\n\nThe Z-score measures how many standard deviations a value is from the mean:\n\\[z = \\frac{x - \\bar{x}}{s}\\]\nValues with |z| &gt; 2 or |z| &gt; 3 are often considered outliers.\n# Calculate z-scores\nstudent_data &lt;- student_data |&gt; \n  mutate(\n    Physical_Activity_Hours_Per_Day_z = (Physical_Activity_Hours_Per_Day - mean(Physical_Activity_Hours_Per_Day)) / sd(Physical_Activity_Hours_Per_Day)\n  )\n\n# View z-scores\nstudent_data |&gt; \n  select(Student_ID, Stress_Level, Physical_Activity_Hours_Per_Day, Physical_Activity_Hours_Per_Day_z) |&gt; \n  arrange(desc(abs(Physical_Activity_Hours_Per_Day_z))) |&gt; \n  head(10)\n## # A tibble: 10 √ó 4\n##    Student_ID Stress_Level Physical_Activity_Hours_Per_‚Ä¶¬π Physical_Activity_Ho‚Ä¶¬≤\n##         &lt;dbl&gt; &lt;chr&gt;                                 &lt;dbl&gt;                  &lt;dbl&gt;\n##  1         63 High                                   13                     3.45\n##  2        172 Low                                    12.4                   3.21\n##  3       1175 High                                   12.3                   3.17\n##  4        785 Low                                    12.1                   3.09\n##  5       1716 Low                                    11.7                   2.93\n##  6        616 High                                   11.6                   2.89\n##  7        546 High                                   11.4                   2.81\n##  8        251 Low                                    11.2                   2.73\n##  9        383 High                                   11.1                   2.69\n## 10       1077 High                                   11.1                   2.69\n## # ‚Ñπ abbreviated names: ¬π‚ÄãPhysical_Activity_Hours_Per_Day,\n## #   ¬≤‚ÄãPhysical_Activity_Hours_Per_Day_z\n# Find outliers using z-score &gt; 2\nz_outliers &lt;- student_data |&gt; \n  filter(abs(Physical_Activity_Hours_Per_Day_z) &gt; 2)\n\ncat(\"Outliers (|z| &gt; 2):\", nrow(z_outliers), \"\\n\")\n## Outliers (|z| &gt; 2): 60\nprint(z_outliers |&gt; select(Student_ID, Stress_Level, Physical_Activity_Hours_Per_Day, Physical_Activity_Hours_Per_Day_z))\n## # A tibble: 60 √ó 4\n##    Student_ID Stress_Level Physical_Activity_Hours_Per_‚Ä¶¬π Physical_Activity_Ho‚Ä¶¬≤\n##         &lt;dbl&gt; &lt;chr&gt;                                 &lt;dbl&gt;                  &lt;dbl&gt;\n##  1         63 High                                   13                     3.45\n##  2        126 Low                                    10.7                   2.53\n##  3        172 Low                                    12.4                   3.21\n##  4        187 Low                                     9.5                   2.06\n##  5        251 Low                                    11.2                   2.73\n##  6        254 High                                    9.7                   2.14\n##  7        269 Moderate                               11                     2.65\n##  8        284 Moderate                                9.8                   2.18\n##  9        323 High                                   11                     2.65\n## 10        381 Low                                     9.8                   2.18\n## # ‚Ñπ 50 more rows\n## # ‚Ñπ abbreviated names: ¬π‚ÄãPhysical_Activity_Hours_Per_Day,\n## #   ¬≤‚ÄãPhysical_Activity_Hours_Per_Day_z\n\n\n\n\nBoxplot (automatically shows outliers):\n# Boxplot shows outliers as points\nggplot(student_data, aes(x = Stress_Level, y = Physical_Activity_Hours_Per_Day, fill = Stress_Level)) +\n  geom_boxplot() +\n  labs(\n    title = \"Boxplot Showing Outliers\",\n    x = \"Stress Level\",\n    y = \"Physical Activity Hours\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nScatter plot with outliers highlighted:\n# Identify outliers\nstudent_data &lt;- student_data |&gt; \n  mutate(\n    is_outlier = Physical_Activity_Hours_Per_Day &lt; lower_bound | Physical_Activity_Hours_Per_Day &gt; upper_bound\n  )\n\nggplot(student_data, aes(x = Physical_Activity_Hours_Per_Day, y = GPA, color = is_outlier)) +\n  geom_point(size = 3) +\n  scale_color_manual(values = c(\"FALSE\" = \"steelblue\", \"TRUE\" = \"red\"),\n                     labels = c(\"Normal\", \"Outlier\")) +\n  labs(\n    title = \"Scatter Plot with Outliers Highlighted\",\n    x = \"Physical Activity Hours\",\n    y = \"GPA\",\n    color = \"Status\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nCalculate the IQR bounds for GPA.\nIdentify any outliers using the IQR method.\nCalculate z-scores for GPA and find values with |z| &gt; 2.\nCreate a visualization that highlights outliers.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L5-github.html#grouped-summary-statistics",
    "href": "Spring2026/LectureSlides/bigdata_L5-github.html#grouped-summary-statistics",
    "title": "Descriptive Data",
    "section": "",
    "text": "The combination of group_by() and summarize() is powerful for calculating statistics by group.\n# Mean study hours by stress level\nstudent_data |&gt; \n  group_by(Stress_Level) |&gt; \n  summarize(mean_hours = mean(Study_Hours_Per_Day))\n## # A tibble: 3 √ó 2\n##   Stress_Level mean_hours\n##   &lt;chr&gt;             &lt;dbl&gt;\n## 1 High               8.39\n## 2 Low                5.47\n## 3 Moderate           6.97\n\n\n\n\nCalculate multiple statistics at once:\n# Comprehensive summary by stress level\nsummary_by_stresslevel &lt;- student_data |&gt; \n  group_by(Stress_Level) |&gt; \n  summarize(\n    n = n(),\n    mean_hours = mean(Study_Hours_Per_Day),\n    median_hours = median(Study_Hours_Per_Day),\n    sd_hours = sd(Study_Hours_Per_Day),\n    min_hours = min(Study_Hours_Per_Day),\n    max_hours = max(Study_Hours_Per_Day),\n    mean_score = mean(GPA)\n  )\n\nprint(summary_by_major)\n## Error: object 'summary_by_major' not found\nGroup by multiple variables:\n# Summary by Stress Level AND college\nstudent_data |&gt; \n  group_by(Stress_Level, college) |&gt; \n  summarize(\n    n = n(),\n    mean_hours = mean(Study_Hours_Per_Day),\n    median_hours = median(Study_Hours_Per_Day),\n    .groups = \"drop\"\n  )\n## # A tibble: 15 √ó 5\n##    Stress_Level college             n mean_hours median_hours\n##    &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n##  1 High         Arts & Sciences   170       8.36          8.6\n##  2 High         Communication     148       8.46          8.6\n##  3 High         Education         217       8.38          8.7\n##  4 High         Humanities        269       8.34          8.7\n##  5 High         Medicine          225       8.41          8.7\n##  6 Low          Arts & Sciences    51       5.47          5.4\n##  7 Low          Communication      35       5.42          5.4\n##  8 Low          Education          70       5.50          5.5\n##  9 Low          Humanities         84       5.47          5.4\n## 10 Low          Medicine           57       5.48          5.5\n## 11 Moderate     Arts & Sciences   130       7.06          7.1\n## 12 Moderate     Communication      97       6.92          6.9\n## 13 Moderate     Education         147       6.99          7.1\n## 14 Moderate     Humanities        141       6.93          6.9\n## 15 Moderate     Medicine          159       6.94          6.9\n\n\n\n\nBar chart of means:\nggplot(summary_by_major, aes(x = Stress_Level, y = mean_hours, fill = Stress_Level)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean_hours - sd_hours, \n                    ymax = mean_hours + sd_hours),\n                width = 0.2) +\n  labs(\n    title = \"Mean Study Hours by Stress Level (¬±1 SD)\",\n    x = \"Stress Level\",\n    y = \"Mean Study Hours\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n## Error: object 'summary_by_major' not found\nGrouped bar chart:\n# Summary by stress level and college\nsummary_stress_college &lt;- student_data |&gt; \n  group_by(Stress_Level, college) |&gt; \n  summarize(mean_hours = mean(Study_Hours_Per_Day), .groups = \"drop\")\n\nggplot(summary_stress_college, aes(x = Stress_Level, y = mean_hours, fill = college)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Mean Study Hours by Stress Level and College\",\n    x = \"Stress Level\",\n    y = \"Mean Study Hours\",\n    fill = \"College\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_minimal()"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L5-github.html#correlation",
    "href": "Spring2026/LectureSlides/bigdata_L5-github.html#correlation",
    "title": "Descriptive Data",
    "section": "",
    "text": "Correlation measures the strength and direction of the linear relationship between two numeric variables.\n\nr = 1: Perfect positive correlation\nr = 0: No linear correlation\nr = -1: Perfect negative correlation\n\nInterpretation guidelines:\n\n\n\n\nr\n\n\n\n\n0.00 - 0.19\nVery weak\n\n\n0.20 - 0.39\nWeak\n\n\n0.40 - 0.59\nModerate\n\n\n0.60 - 0.79\nStrong\n\n\n0.80 - 1.00\nVery strong\n\n\n\n\n\n\n\n# Pearson correlation (default)\ncor(student_data$Study_Hours_Per_Day, student_data$GPA)\n## [1] 0.734468\nCorrelation test with p-value:\n# Correlation test\ncor_test &lt;- cor.test(student_data$Study_Hours_Per_Day, student_data$GPA)\ncor_test\n## \n##  Pearson's product-moment correlation\n## \n## data:  student_data$Study_Hours_Per_Day and student_data$GPA\n## t = 48.376, df = 1998, p-value &lt; 2.2e-16\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.7136099 0.7540250\n## sample estimates:\n##      cor \n## 0.734468\nInterpretation: There is a very strong positive correlation (r = 0.734) between study hours and exam scores, p &lt; 0.001.\n\n\n\n\nWhen you have multiple numeric variables, create a correlation matrix:\n# Select numeric columns\nnumeric_vars &lt;- student_data |&gt; \n  select(Study_Hours_Per_Day, GPA)\n\n# Correlation matrix\ncor(numeric_vars)\n##                     Study_Hours_Per_Day      GPA\n## Study_Hours_Per_Day            1.000000 0.734468\n## GPA                            0.734468 1.000000\nWith more variables:\n# Correlation matrix\nnumeric_vars &lt;- student_data |&gt; \n  select(Study_Hours_Per_Day, GPA, Sleep_Hours_Per_Day, Physical_Activity_Hours_Per_Day)\n\nround(cor(numeric_vars), 3)\n##                                 Study_Hours_Per_Day    GPA Sleep_Hours_Per_Day\n## Study_Hours_Per_Day                           1.000  0.734               0.027\n## GPA                                           0.734  1.000              -0.004\n## Sleep_Hours_Per_Day                           0.027 -0.004               1.000\n## Physical_Activity_Hours_Per_Day              -0.488 -0.341              -0.470\n##                                 Physical_Activity_Hours_Per_Day\n## Study_Hours_Per_Day                                      -0.488\n## GPA                                                      -0.341\n## Sleep_Hours_Per_Day                                      -0.470\n## Physical_Activity_Hours_Per_Day                           1.000\n\n\n\n\nScatter plot with correlation:\nggplot(student_data, aes(x = Study_Hours_Per_Day, y = GPA)) +\n  geom_point(color = \"steelblue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = paste(\"Study Hours vs GPA (r =\", \n                  round(cor(student_data$Study_Hours_Per_Day, student_data$GPA), 2), \")\"),\n    x = \"Study Hours\",\n    y = \"GPA\"\n  ) +\n  theme_minimal()\n\nCorrelation heatmap:\n# Create correlation matrix\ncor_matrix &lt;- cor(numeric_vars)\n\n# Convert to long format for ggplot\ncor_long &lt;- cor_matrix |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"var1\") |&gt; \n  pivot_longer(-var1, names_to = \"var2\", values_to = \"correlation\")\n\n# Heatmap\nggplot(cor_long, aes(x = var1, y = var2, fill = correlation)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = round(correlation, 2)), color = \"black\", size = 4) +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", \n                       midpoint = 0, limits = c(-1, 1)) +\n  labs(\n    title = \"Correlation Heatmap\",\n    x = \"\", y = \"\",\n    fill = \"Correlation\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nCalculate the correlation between GPA and Sleep_Hours_Per_day.\nIs the correlation statistically significant? (Use cor.test())\nCreate a scatter plot showing this relationship.\nWhat does this correlation tell us?\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L5-github.html#cross-tabulations",
    "href": "Spring2026/LectureSlides/bigdata_L5-github.html#cross-tabulations",
    "title": "Descriptive Data",
    "section": "",
    "text": "Cross-tabulations (contingency tables) summarize the relationship between categorical variables.\n\n\nSingle variable:\n# Using tidyverse\nstudent_data |&gt; \n  count(college)\n## # A tibble: 5 √ó 2\n##   college             n\n##   &lt;chr&gt;           &lt;int&gt;\n## 1 Arts & Sciences   351\n## 2 Communication     280\n## 3 Education         434\n## 4 Humanities        494\n## 5 Medicine          441\nTwo variables (cross-tabulation):\n# Using tidyverse\nstudent_data |&gt; \n  count(college, Stress_Level) |&gt; \n  pivot_wider(names_from = Stress_Level, values_from = n)\n## # A tibble: 5 √ó 4\n##   college          High   Low Moderate\n##   &lt;chr&gt;           &lt;int&gt; &lt;int&gt;    &lt;int&gt;\n## 1 Arts & Sciences   170    51      130\n## 2 Communication     148    35       97\n## 3 Education         217    70      147\n## 4 Humanities        269    84      141\n## 5 Medicine          225    57      159\n\n\n\n\n# Percentages by group\nstudent_data |&gt; \n  count(college, Stress_Level) |&gt; \n  group_by(college) |&gt; \n  mutate(\n    percentage = n / sum(n) * 100\n  )\n## # A tibble: 15 √ó 4\n## # Groups:   college [5]\n##    college         Stress_Level     n percentage\n##    &lt;chr&gt;           &lt;chr&gt;        &lt;int&gt;      &lt;dbl&gt;\n##  1 Arts & Sciences High           170       48.4\n##  2 Arts & Sciences Low             51       14.5\n##  3 Arts & Sciences Moderate       130       37.0\n##  4 Communication   High           148       52.9\n##  5 Communication   Low             35       12.5\n##  6 Communication   Moderate        97       34.6\n##  7 Education       High           217       50  \n##  8 Education       Low             70       16.1\n##  9 Education       Moderate       147       33.9\n## 10 Humanities      High           269       54.5\n## 11 Humanities      Low             84       17.0\n## 12 Humanities      Moderate       141       28.5\n## 13 Medicine        High           225       51.0\n## 14 Medicine        Low             57       12.9\n## 15 Medicine        Moderate       159       36.1\n\n\n\n\nStacked bar chart:\nggplot(student_data, aes(x = college, fill = Stress_Level)) +\n  geom_bar() +\n  labs(\n    title = \"Distribution of Stress Level by College\",\n    x = \"College\",\n    y = \"Count\",\n    fill = \"Stress Level\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_minimal()\n\nGrouped bar chart:\nggplot(student_data, aes(x = college, fill = Stress_Level)) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    title = \"Distribution of Stress Level by College\",\n    x = \"College\",\n    y = \"Count\",\n    fill = \"Stress Level\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_minimal()\n\nProportional bar chart:\nggplot(student_data, aes(x = college, fill = Stress_Level)) +\n  geom_bar(position = \"fill\") +\n  labs(\n    title = \"Proportion of Stress Level by College\",\n    x = \"College\",\n    y = \"Proportion\",\n    fill = \"Stress Level\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal()\n\n\nThree-way cross-tabulation:\n# College by Stress Level by GPA\nstudent_data |&gt; \n  count(college, Stress_Level, GPA_Level) |&gt; \n  pivot_wider(names_from = GPA_Level, values_from = n, values_fill = 0)\n## # A tibble: 15 √ó 5\n##    college         Stress_Level HighGPA LowGPA MediumGPA\n##    &lt;chr&gt;           &lt;chr&gt;          &lt;int&gt;  &lt;int&gt;     &lt;int&gt;\n##  1 Arts & Sciences High              82     12        76\n##  2 Arts & Sciences Low                0     33        18\n##  3 Arts & Sciences Moderate          16     31        83\n##  4 Communication   High              61     15        72\n##  5 Communication   Low                0     24        11\n##  6 Communication   Moderate          12     29        56\n##  7 Education       High              89     17       111\n##  8 Education       Low                0     41        29\n##  9 Education       Moderate          12     47        88\n## 10 Humanities      High             104     31       134\n## 11 Humanities      Low                1     53        30\n## 12 Humanities      Moderate           9     45        87\n## 13 Medicine        High              86     25       114\n## 14 Medicine        Low                2     39        16\n## 15 Medicine        Moderate          14     45       100\n\n\n\n\nCreate a cross-tabulation of College and GPA_Level.\nWhat percentage of Medicine students have ‚ÄúHigh‚Äù GPA?\nCreate a proportional bar chart showing GPA levels by college\nWhich major has the highest proportion of GPA_level to Stress_Level?\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L5-github.html#class-exercise-2",
    "href": "Spring2026/LectureSlides/bigdata_L5-github.html#class-exercise-2",
    "title": "Descriptive Data",
    "section": "",
    "text": "Using the mtcars dataset:\ndata(mtcars)\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nCentral Tendency: Calculate mean, median for mpg (miles per gallon).\nSpread: Calculate standard deviation and IQR for mpg.\nDistribution: Create a histogram and density plot of mpg.\nOutliers: Use the IQR method to identify any outliers in hp (horsepower).\nGrouped Statistics: Calculate mean mpg grouped by number of cylinders (cyl).\nCorrelation: Calculate the correlation between mpg and wt (weight). Create a scatter plot.\nCross-tabulation: Create a cross-tabulation of cyl and gear. Visualize with a bar chart.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L5-github.html#lecture-5-cheat-sheet",
    "href": "Spring2026/LectureSlides/bigdata_L5-github.html#lecture-5-cheat-sheet",
    "title": "Descriptive Data",
    "section": "",
    "text": "Function/Concept\nDescription\nCode Example\n\n\n\n\nmean()\nCalculate arithmetic mean\nmean(x, na.rm = TRUE)\n\n\nmedian()\nCalculate median (middle value)\nmedian(x)\n\n\nsd()\nCalculate standard deviation\nsd(x)\n\n\nvar()\nCalculate variance\nvar(x)\n\n\nrange()\nGet minimum and maximum\nrange(x)\n\n\nIQR()\nCalculate interquartile range\nIQR(x)\n\n\nquantile()\nCalculate quantiles/percentiles\nquantile(x, 0.25)\n\n\nsummary()\nGet 5-number summary + mean\nsummary(x)\n\n\ntable()\nCreate frequency table\ntable(df$category)\n\n\nprop.table()\nConvert counts to proportions\nprop.table(table(x))\n\n\ncor()\nCalculate correlation\ncor(x, y)\n\n\ncor.test()\nCorrelation test with p-value\ncor.test(x, y)\n\n\ngeom_histogram()\nCreate histogram\ngeom_histogram(binwidth = 5)\n\n\ngeom_density()\nCreate density plot\ngeom_density(fill = \"blue\")\n\n\ngeom_boxplot()\nCreate boxplot\ngeom_boxplot()\n\n\ngeom_tile()\nCreate heatmap tiles\ngeom_tile(aes(fill = value))\n\n\ngroup_by() + summarize()\nGrouped statistics\ngroup_by(var) |&gt; summarize(mean = mean(x))\n\n\ncount()\nCount observations\ncount(var1, var2)\n\n\npivot_wider()\nReshape long to wide\npivot_wider(names_from, values_from)\n\n\nZ-score\nStandardize values\n(x - mean(x)) / sd(x)\n\n\nIQR outliers\nValues beyond 1.5√óIQR\nQ1 - 1.5*IQR or Q3 + 1.5*IQR"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "Dr.¬†Ayse D. Lokmanoglu Lecture 3, (B) Feb 4, (A) Feb 9\n\n\n\n\n\nSection\nTopic\n\n\n\n\n1\nIntroduction to ggplot2\n\n\n1.1\nThe Grammar of Graphics\n\n\n1.2\nBasic ggplot Structure\n\n\n2\nGeoms: Types of Graphs\n\n\n2.1\nCommon Geoms Overview\n\n\n2.2\nScatter Plots (geom_point())\n\n\n2.3\nLine Plots (geom_line())\n\n\n2.4\nBar Charts (geom_bar(), geom_col())\n\n\n2.5\nHistograms (geom_histogram())\n\n\n2.6\nBox Plots (geom_boxplot())\n\n\n3\nAesthetics\n\n\n3.1\nMapping vs.¬†Setting Aesthetics\n\n\n3.2\nColor, Fill, and Alpha\n\n\n3.3\nSize, Shape, and Linetype\n\n\n4\nLabels and Annotations\n\n\n4.1\nAdding Labels with labs()\n\n\n4.2\nAdding Text and Annotations\n\n\n5\nScales and Axes\n\n\n5.1\nCustomizing Continuous Scales\n\n\n5.2\nCustomizing Discrete Scales\n\n\n5.3\nDate Scales\n\n\n5.4\nColor Scales\n\n\n6\nThemes\n\n\n6.1\nBuilt-in Themes\n\n\n6.2\nCustomizing Theme Elements\n\n\n6.3\nExternal Themes (ggthemes)\n\n\n7\nFacets\n\n\n7.1\nfacet_wrap()\n\n\n7.2\nfacet_grid()\n\n\n7.3\nCustomizing Facet Labels\n\n\n8\nColor Palettes\n\n\n8.1\nManual Colors\n\n\n8.2\nColorBrewer\n\n\n8.3\nViridis\n\n\n8.4\nFun Palettes (wesanderson, NatParksPalettes)\n\n\n\n\n\n\n\nALWAYS load our libraries first\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n\n\n\n\n\nggplot2 is a powerful visualization package that follows the Grammar of Graphics - a systematic approach to building plots layer by layer.\nCore Components:\n\n\n\n\n\n\n\n\nComponent\nDescription\nExample\n\n\n\n\nData\nThe dataset you‚Äôre plotting\ndata = mtcars\n\n\nAesthetics\nMappings between data and visual properties\naes(x = wt, y = mpg)\n\n\nGeometries\nThe type of plot\ngeom_point(), geom_bar()\n\n\nScales\nControl how data maps to visual properties\nscale_x_continuous()\n\n\nFacets\nSplit data into panels\nfacet_wrap(~ cyl)\n\n\nThemes\nControl non-data elements\ntheme_minimal()\n\n\n\nGGPlot Cheat Sheet\n\n\n\n\nEvery ggplot follows this basic structure:\nggplot(data = &lt;DATA&gt;, aes(&lt;MAPPINGS&gt;)) +\n  &lt;GEOM_FUNCTION&gt;() +\n  &lt;OPTIONAL_LAYERS&gt;\nCreate a sample dataset:\ndf &lt;- data.frame(\n  Movie_Title = c(\n    \"YOLO\", \"Successor\", \"Pegasus 2\", \"Deadpool & Wolverine\", \"Moana 2\", \n    \"The Hidden Blade\", \"Avatar: The Spirit Returns\"\n  ),\n  Release_Date = c(\n    \"January 2024\", \"February 2024\", \"March 2024\", \"July 2024\", \n    \"November 2024\", \"April 2024\", \"December 2024\"\n  ),\n  China_Box_Office_Gross = c(\n    479597304, 469612890, 466930272, 450000000, 350000000, 320000000, 550000000\n  ),\n  US_Box_Office_Gross = c(\n    310000000, 280000000, 290000000, 438000000, 221000000, 75000000, 600000000\n  ),\n  Total_Worldwide_Gross = c(\n    479597304, 469612890, 466930272, 850000000, 421000000, 400000000, 1300000000\n  )\n)\nBasic scatter plot:\nggplot(data = df, aes(x = China_Box_Office_Gross, y = US_Box_Office_Gross)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeom Function\nDescription\nKey Aesthetics\n\n\n\n\ngeom_point()\nScatter plot\nx, y, color, size, shape\n\n\ngeom_line()\nLine plot\nx, y, color, linetype\n\n\ngeom_bar()\nBar chart (counts)\nx, fill\n\n\ngeom_col()\nBar chart (values)\nx, y, fill\n\n\ngeom_histogram()\nHistogram\nx, fill, bins\n\n\ngeom_boxplot()\nBox plot\nx, y, fill\n\n\ngeom_smooth()\nTrend line\nx, y, method\n\n\ngeom_text()\nAdd text\nx, y, label\n\n\n\nAdditional Geoms:\n\n\n\nGeom Function\nDescription\n\n\n\n\ngeom_area()\nArea under a line\n\n\ngeom_violin()\nViolin plot\n\n\ngeom_density()\nDensity curve\n\n\ngeom_tile()\nHeatmap tiles\n\n\ngeom_segment()\nLine segments\n\n\ngeom_abline()\nReference line (slope/intercept)\n\n\ngeom_hline()\nHorizontal reference line\n\n\ngeom_vline()\nVertical reference line\n\n\n\n\n\n\n\n# Basic scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(\n    title = \"Fuel Efficiency vs. Weight\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\"\n  )\n\nWith color mapping:\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Fuel Efficiency by Weight and Cylinders\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\"\n  )\n\n\n\n\n\n# Create time series data\ntime_data &lt;- data.frame(\n  month = 1:12,\n  sales = c(100, 120, 115, 130, 145, 160, 155, 170, 180, 175, 190, 210)\n)\n\nggplot(time_data, aes(x = month, y = sales)) +\n  geom_line(color = \"steelblue\", size = 1) +\n  geom_point(color = \"steelblue\", size = 2) +\n  labs(\n    title = \"Monthly Sales Trend\",\n    x = \"Month\",\n    y = \"Sales\"\n  )\n\n\n\n\n\ngeom_bar() - counts observations (stat = ‚Äúcount‚Äù)\nggplot(mtcars, aes(x = factor(cyl))) +\n  geom_bar(fill = \"steelblue\") +\n  labs(\n    title = \"Count of Cars by Cylinder\",\n    x = \"Cylinders\",\n    y = \"Count\"\n  )\n\ngeom_col() - uses values directly (stat = ‚Äúidentity‚Äù)\nggplot(df, aes(x = Movie_Title, y = Total_Worldwide_Gross)) +\n  geom_col(fill = \"steelblue\") +\n  labs(\n    title = \"Worldwide Gross by Movie\",\n    x = \"Movie\",\n    y = \"Gross ($)\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nFlip coordinates for horizontal bars:\nggplot(df, aes(x = Movie_Title, y = Total_Worldwide_Gross)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Worldwide Gross by Movie\",\n    x = \"Movie\",\n    y = \"Gross ($)\"\n  )\n\n\n\n\n\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 2, fill = \"steelblue\", color = \"white\") +\n  labs(\n    title = \"Distribution of MPG\",\n    x = \"Miles Per Gallon\",\n    y = \"Count\"\n  )\n\n\n\n\n\nBox plots show distribution and outliers: - Box: Interquartile range (IQR) - middle 50% of data - Line in box: Median - Whiskers: Extend to 1.5 √ó IQR - Points: Outliers beyond whiskers\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_boxplot() +\n  labs(\n    title = \"MPG Distribution by Cylinders\",\n    x = \"Cylinders\",\n    y = \"Miles Per Gallon\"\n  ) +\n  theme(legend.position = \"none\")\n\nAdd jittered points to show actual data:\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"MPG Distribution by Cylinders\",\n    x = \"Cylinders\",\n    y = \"Miles Per Gallon\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nUsing the mtcars dataset:\n\nCreate a scatter plot of hp (horsepower) vs mpg (miles per gallon).\nCreate a bar chart showing the count of cars by number of gears (gear).\nCreate a histogram of hp with 10 bins.\nCreate a boxplot of mpg grouped by gear.\n\n### Your workspace\n\n\n\n\n\n\n\n\nA critical distinction! There‚Äôs an important difference between mapping a variable to an aesthetic and setting an aesthetic to a fixed value.\nMapping (inside aes()) - connects a variable to a visual property:\n# Color varies BY the data (each cylinder group gets a different color)\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 4)\n\nSetting (outside aes()) - applies a fixed value to all points:\n# ALL points are purple\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point(size = 4, color = \"purple\")\n\nCommon Error: What happens if you put a fixed value inside aes()?\n# WRONG! This creates a weird legend\nggplot(data = mtcars, aes(x = wt, y = mpg, color = \"purple\")) +\n  geom_point(size = 4)\n\nR thinks ‚Äúpurple‚Äù is a variable name, recycles it for every row, and picks its default color (salmon) ‚Äî not what you wanted!\n\n\n\n\n\n\n\nAesthetic\nDescription\nUsed With\n\n\n\n\ncolor\nOutline/line color\nPoints, lines, text\n\n\nfill\nInterior color\nBars, boxes, areas\n\n\nalpha\nTransparency (0-1)\nAll geoms\n\n\n\n# color vs fill\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot(color = \"darkblue\", fill = \"lightblue\", alpha = 0.7)\n\n\n\n\n\nPoint Shapes:\n\n\n\nShape Integer\nShape Name\nVisualization\n\n\n\n\n0\nsquare open\n‚ñ°\n\n\n1\ncircle open\n‚óã\n\n\n2\ntriangle open\n‚ñ≥\n\n\n15\nsquare filled\n‚ñ†\n\n\n16\ncircle filled\n‚óè\n\n\n17\ntriangle filled\n‚ñ≤\n\n\n18\ndiamond filled\n‚óÜ\n\n\n\n# Multiple aesthetics\nggplot(mtcars, aes(x = wt, y = mpg, \n                   color = factor(cyl), \n                   size = hp,\n                   shape = factor(gear))) +\n  geom_point(alpha = 0.7) +\n  labs(\n    color = \"Cylinders\",\n    size = \"Horsepower\",\n    shape = \"Gears\"\n  )\n\nLine Types:\n# Different line types\ndf_lines &lt;- data.frame(\n  x = rep(1:5, 3),\n  y = c(1:5, 2:6, 3:7),\n  group = rep(c(\"A\", \"B\", \"C\"), each = 5)\n)\n\nggplot(df_lines, aes(x = x, y = y, linetype = group, color = group)) +\n  geom_line(size = 1) +\n  labs(title = \"Different Line Types\")\n\n\n\n\n\n\n\n\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Fuel Efficiency by Vehicle Weight\",\n    subtitle = \"Data from mtcars dataset\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\",\n    caption = \"Source: mtcars dataset in R\"\n  )\n\n\n\n\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_text(aes(label = rownames(mtcars)), size = 2, vjust = -0.5) +\n  labs(title = \"MPG vs Weight with Car Names\")\n\nUse geom_label() for boxed labels or annotate() for single annotations:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  annotate(\"text\", x = 5, y = 30, label = \"High efficiency, heavy cars\", color = \"red\") +\n  annotate(\"rect\", xmin = 4.5, xmax = 5.5, ymin = 28, ymax = 35, alpha = 0.2, fill = \"red\")\n\n\n\n\n\n\n\n\nUse scale_x_continuous() and scale_y_continuous() to customize numeric axes.\nggplot(df, aes(x = China_Box_Office_Gross, y = US_Box_Office_Gross)) +\n  geom_point(size = 4, color = \"steelblue\") +\n  scale_x_continuous(\n    labels = scales::comma,\n    breaks = seq(300000000, 600000000, by = 50000000)\n  ) +\n  scale_y_continuous(\n    labels = scales::dollar,\n    limits = c(0, 700000000)\n  ) +\n  labs(\n    title = \"Box Office Performance\",\n    x = \"China Gross\",\n    y = \"US Gross\"\n  )\n\n\n\n\n\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_boxplot() +\n  scale_x_discrete(labels = c(\"4\" = \"Four\", \"6\" = \"Six\", \"8\" = \"Eight\")) +\n  scale_fill_discrete(name = \"Cylinders\") +\n  labs(x = \"Number of Cylinders\")\n\n\n\n\n\nWhen working with dates, use scale_x_date() to format axis labels.\n# Create date data\ndf_dates &lt;- df |&gt; \n  mutate(Release_Date_Parsed = dmy(paste(\"01\", Release_Date)))\n\nggplot(df_dates, aes(x = Release_Date_Parsed, y = Total_Worldwide_Gross)) +\n  geom_line(color = \"steelblue\", size = 1) +\n  geom_point(color = \"steelblue\", size = 3) +\n  scale_x_date(\n    date_breaks = \"1 month\",\n    date_labels = \"%b %Y\"\n  ) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(\n    title = \"Worldwide Gross Over Time\",\n    x = \"Release Date\",\n    y = \"Total Worldwide Gross\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nFunction\nPurpose\n\n\n\n\nscale_color_manual()\nSet colors manually\n\n\nscale_fill_manual()\nSet fill colors manually\n\n\nscale_color_brewer()\nUse ColorBrewer palettes\n\n\nscale_color_viridis_d()\nViridis discrete palette\n\n\nscale_color_viridis_c()\nViridis continuous palette\n\n\nscale_color_gradient()\nContinuous gradient\n\n\nscale_color_gradient2()\nDiverging gradient\n\n\n\n# Manual colors\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  scale_color_manual(values = c(\"red\", \"green\", \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\nTheme\nDescription\n\n\n\n\ntheme_minimal()\nClean, minimal design\n\n\ntheme_classic()\nClassic with axes, no gridlines\n\n\ntheme_light()\nLight background, subtle gridlines\n\n\ntheme_dark()\nDark background\n\n\ntheme_bw()\nBlack and white, good for printing\n\n\ntheme_void()\nEmpty, no axes or background\n\n\n\np &lt;- ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Theme Comparison\")\n\n# Compare themes\np + theme_minimal() + labs(subtitle = \"theme_minimal()\")\n\np + theme_classic() + labs(subtitle = \"theme_classic()\")\n\np + theme_bw() + labs(subtitle = \"theme_bw()\")\n\n\n\n\n\nUse theme() to customize individual elements:\n\n\n\nElement\nDescription\n\n\n\n\nplot.title\nMain title appearance\n\n\nplot.subtitle\nSubtitle appearance\n\n\naxis.title\nAxis label appearance\n\n\naxis.text\nAxis tick label appearance\n\n\nlegend.position\nLegend location\n\n\npanel.grid\nGridline appearance\n\n\npanel.background\nPlot area background\n\n\n\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 4) +\n  labs(\n    title = \"Fuel Efficiency by Vehicle Weight\",\n    subtitle = \"Custom themed plot\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\"\n  ) +\n  theme_light() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5, color = \"darkblue\"),\n    plot.subtitle = element_text(face = \"italic\", size = 12, hjust = 0.5),\n    axis.title = element_text(face = \"bold\", size = 12),\n    axis.text = element_text(size = 10),\n    legend.position = \"top\",\n    panel.grid.major = element_line(color = \"gray90\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\nThe ggthemes package provides additional themes:\ninstall.packages(\"ggthemes\")\nlibrary(ggthemes)\n\n# Economist theme\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Economist Theme\") +\n  theme_economist() +\n  scale_color_economist()\n\n# Wall Street Journal theme\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Wall Street Journal Theme\") +\n  theme_wsj()\n\n# Tufte theme (minimalist)\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Tufte Theme\") +\n  theme_tufte()\n\n\n\n\n\n\nFacets split data into multiple panels by a variable.\n\n\nUse facet_wrap() for a single grouping variable:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\", size = 2) +\n  facet_wrap(~ cyl) +\n  labs(title = \"MPG vs Weight by Cylinders\")\n\nControl layout with ncol and nrow:\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  facet_wrap(~ cyl, ncol = 3) +\n  labs(title = \"Faceted by Cylinders (3 columns)\") +\n  theme(legend.position = \"none\")\n\nFree scales:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\") +\n  facet_wrap(~ gear, scales = \"free\") +\n  labs(title = \"Faceted with Free Scales\")\n\n\n\n\n\nUse facet_grid() for two grouping variables:\n# Rows = cyl, Columns = gear\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  facet_grid(cyl ~ gear) +\n  labs(title = \"Facet Grid: Cylinders √ó Gears\") +\n  theme(legend.position = \"none\")\n\nFacet by rows only:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\") +\n  facet_grid(cyl ~ .) +\n  labs(title = \"Facet Grid: Rows by Cylinders\")\n\nFacet by columns only:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\") +\n  facet_grid(. ~ gear) +\n  labs(title = \"Facet Grid: Columns by Gears\")\n\n\n\n\n\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  facet_wrap(\n    ~ cyl,\n    labeller = labeller(cyl = c(\"4\" = \"4 Cylinders\", \"6\" = \"6 Cylinders\", \"8\" = \"8 Cylinders\"))\n  ) +\n  labs(title = \"Custom Facet Labels\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"tomato\", \"steelblue\", \"forestgreen\")) +\n  labs(title = \"Manual Color Selection\")\n\n\n\n\n\nColorBrewer provides color palettes designed for data visualization:\nlibrary(RColorBrewer)\ndisplay.brewer.all()\n\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"ColorBrewer: Set2\")\n\n\n\n\n\nViridis palettes are colorblind-friendly and print well in grayscale:\nggplot(mtcars, aes(x = wt, y = mpg, color = hp)) +\n  geom_point(size = 4) +\n  scale_color_viridis_c() +\n  labs(title = \"Viridis Continuous Scale\")\n\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_viridis_d() +\n  labs(title = \"Viridis Discrete Scale\")\n\n\n\n\n\nwesanderson:\ninstall.packages(\"wesanderson\")\nlibrary(wesanderson)\n\n# See available palettes\nnames(wes_palettes)\n##  [1] \"BottleRocket1\"     \"BottleRocket2\"     \"Rushmore1\"        \n##  [4] \"Rushmore\"          \"Royal1\"            \"Royal2\"           \n##  [7] \"Zissou1\"           \"Zissou1Continuous\" \"Darjeeling1\"      \n## [10] \"Darjeeling2\"       \"Chevalier1\"        \"FantasticFox1\"    \n## [13] \"Moonrise1\"         \"Moonrise2\"         \"Moonrise3\"        \n## [16] \"Cavalcanti1\"       \"GrandBudapest1\"    \"GrandBudapest2\"   \n## [19] \"IsleofDogs1\"       \"IsleofDogs2\"       \"FrenchDispatch\"   \n## [22] \"AsteroidCity1\"     \"AsteroidCity2\"     \"AsteroidCity3\"\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_manual(values = wes_palette(\"GrandBudapest1\", n = 3)) +\n  labs(title = \"Wes Anderson: Grand Budapest\")\n\nNatParksPalettes:\ninstall.packages(\"NatParksPalettes\")\nlibrary(NatParksPalettes)\n\n# See available palettes\nnames(NatParksPalettes)\n##  [1] \"Acadia\"      \"Arches\"      \"Arches2\"     \"Banff\"       \"BryceCanyon\"\n##  [6] \"CapitolReef\" \"Charmonix\"   \"CraterLake\"  \"Cuyahoga\"    \"DeathValley\"\n## [11] \"Denali\"      \"Everglades\"  \"Glacier\"     \"GrandCanyon\" \"Halekala\"   \n## [16] \"IguazuFalls\" \"KingsCanyon\" \"LakeNakuru\"  \"Olympic\"     \"Redwood\"    \n## [21] \"RockyMtn\"    \"Saguaro\"     \"SmokyMtns\"   \"SouthDowns\"  \"Torres\"     \n## [26] \"Triglav\"     \"WindCave\"    \"Volcanoes\"   \"Yellowstone\" \"Yosemite\"\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_manual(values = natparks.pals(\"Yellowstone\", n = 3)) +\n  labs(title = \"National Parks: Yellowstone\")\n\n\n\n\nUsing the iris dataset, create a publication-ready visualization:\n\nCreate a scatter plot of Sepal.Length vs Sepal.Width, colored by Species.\nAdd appropriate labels (title, subtitle, axis labels, caption).\nUse a custom color palette (try wesanderson or ColorBrewer).\nApply a theme (try theme_minimal() or theme_classic()).\nCustomize the theme with:\n\nBold, centered title\nLegend at the bottom\nCustom axis text size\n\nCreate a faceted version by Species.\n\n# Starter code\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(size = 3) +\n  # Add your customizations here\n  labs(\n    title = \"...\",\n    subtitle = \"...\",\n    x = \"...\",\n    y = \"...\",\n    caption = \"...\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic\nKey Points\n\n\n\n\nggplot Structure\nggplot(data, aes()) + geom_*() + ... - build plots layer by layer\n\n\nCommon Geoms\ngeom_point(): scatter; geom_line(): lines; geom_bar()/geom_col(): bars; geom_histogram(): distribution; geom_boxplot(): box plots\n\n\nMapping vs Setting\nInside aes(): map variable to aesthetic; Outside aes(): set fixed value\n\n\nAesthetics\ncolor: outlines/lines; fill: interior; alpha: transparency; size: point/line size; shape: point shape; linetype: line style\n\n\nLabels\nlabs(title, subtitle, x, y, color, fill, caption)\n\n\nScales\nscale_x_continuous(), scale_y_continuous(), scale_x_date(), scale_color_manual(), scale_fill_brewer()\n\n\nThemes\ntheme_minimal(), theme_classic(), theme_bw(), theme_void(); Customize with theme()\n\n\nFacets\nfacet_wrap(~ var): single variable; facet_grid(row ~ col): two variables\n\n\nColor Palettes\nManual: scale_*_manual(values = c(...)); Brewer: scale_*_brewer(palette = \"...\"); Viridis: scale_*_viridis_d()\n\n\nggthemes\ntheme_economist(), theme_wsj(), theme_tufte() for publication-ready styles"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#lecture-3-table-of-contents",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#lecture-3-table-of-contents",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "Section\nTopic\n\n\n\n\n1\nIntroduction to ggplot2\n\n\n1.1\nThe Grammar of Graphics\n\n\n1.2\nBasic ggplot Structure\n\n\n2\nGeoms: Types of Graphs\n\n\n2.1\nCommon Geoms Overview\n\n\n2.2\nScatter Plots (geom_point())\n\n\n2.3\nLine Plots (geom_line())\n\n\n2.4\nBar Charts (geom_bar(), geom_col())\n\n\n2.5\nHistograms (geom_histogram())\n\n\n2.6\nBox Plots (geom_boxplot())\n\n\n3\nAesthetics\n\n\n3.1\nMapping vs.¬†Setting Aesthetics\n\n\n3.2\nColor, Fill, and Alpha\n\n\n3.3\nSize, Shape, and Linetype\n\n\n4\nLabels and Annotations\n\n\n4.1\nAdding Labels with labs()\n\n\n4.2\nAdding Text and Annotations\n\n\n5\nScales and Axes\n\n\n5.1\nCustomizing Continuous Scales\n\n\n5.2\nCustomizing Discrete Scales\n\n\n5.3\nDate Scales\n\n\n5.4\nColor Scales\n\n\n6\nThemes\n\n\n6.1\nBuilt-in Themes\n\n\n6.2\nCustomizing Theme Elements\n\n\n6.3\nExternal Themes (ggthemes)\n\n\n7\nFacets\n\n\n7.1\nfacet_wrap()\n\n\n7.2\nfacet_grid()\n\n\n7.3\nCustomizing Facet Labels\n\n\n8\nColor Palettes\n\n\n8.1\nManual Colors\n\n\n8.2\nColorBrewer\n\n\n8.3\nViridis\n\n\n8.4\nFun Palettes (wesanderson, NatParksPalettes)"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#r-exercises",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#r-exercises",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "ALWAYS load our libraries first\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(lubridate)"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#introduction-to-ggplot2",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#introduction-to-ggplot2",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "ggplot2 is a powerful visualization package that follows the Grammar of Graphics - a systematic approach to building plots layer by layer.\nCore Components:\n\n\n\n\n\n\n\n\nComponent\nDescription\nExample\n\n\n\n\nData\nThe dataset you‚Äôre plotting\ndata = mtcars\n\n\nAesthetics\nMappings between data and visual properties\naes(x = wt, y = mpg)\n\n\nGeometries\nThe type of plot\ngeom_point(), geom_bar()\n\n\nScales\nControl how data maps to visual properties\nscale_x_continuous()\n\n\nFacets\nSplit data into panels\nfacet_wrap(~ cyl)\n\n\nThemes\nControl non-data elements\ntheme_minimal()\n\n\n\nGGPlot Cheat Sheet\n\n\n\n\nEvery ggplot follows this basic structure:\nggplot(data = &lt;DATA&gt;, aes(&lt;MAPPINGS&gt;)) +\n  &lt;GEOM_FUNCTION&gt;() +\n  &lt;OPTIONAL_LAYERS&gt;\nCreate a sample dataset:\ndf &lt;- data.frame(\n  Movie_Title = c(\n    \"YOLO\", \"Successor\", \"Pegasus 2\", \"Deadpool & Wolverine\", \"Moana 2\", \n    \"The Hidden Blade\", \"Avatar: The Spirit Returns\"\n  ),\n  Release_Date = c(\n    \"January 2024\", \"February 2024\", \"March 2024\", \"July 2024\", \n    \"November 2024\", \"April 2024\", \"December 2024\"\n  ),\n  China_Box_Office_Gross = c(\n    479597304, 469612890, 466930272, 450000000, 350000000, 320000000, 550000000\n  ),\n  US_Box_Office_Gross = c(\n    310000000, 280000000, 290000000, 438000000, 221000000, 75000000, 600000000\n  ),\n  Total_Worldwide_Gross = c(\n    479597304, 469612890, 466930272, 850000000, 421000000, 400000000, 1300000000\n  )\n)\nBasic scatter plot:\nggplot(data = df, aes(x = China_Box_Office_Gross, y = US_Box_Office_Gross)) +\n  geom_point()"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#geoms-types-of-graphs",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#geoms-types-of-graphs",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "Geom Function\nDescription\nKey Aesthetics\n\n\n\n\ngeom_point()\nScatter plot\nx, y, color, size, shape\n\n\ngeom_line()\nLine plot\nx, y, color, linetype\n\n\ngeom_bar()\nBar chart (counts)\nx, fill\n\n\ngeom_col()\nBar chart (values)\nx, y, fill\n\n\ngeom_histogram()\nHistogram\nx, fill, bins\n\n\ngeom_boxplot()\nBox plot\nx, y, fill\n\n\ngeom_smooth()\nTrend line\nx, y, method\n\n\ngeom_text()\nAdd text\nx, y, label\n\n\n\nAdditional Geoms:\n\n\n\nGeom Function\nDescription\n\n\n\n\ngeom_area()\nArea under a line\n\n\ngeom_violin()\nViolin plot\n\n\ngeom_density()\nDensity curve\n\n\ngeom_tile()\nHeatmap tiles\n\n\ngeom_segment()\nLine segments\n\n\ngeom_abline()\nReference line (slope/intercept)\n\n\ngeom_hline()\nHorizontal reference line\n\n\ngeom_vline()\nVertical reference line\n\n\n\n\n\n\n\n# Basic scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(\n    title = \"Fuel Efficiency vs. Weight\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\"\n  )\n\nWith color mapping:\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Fuel Efficiency by Weight and Cylinders\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\"\n  )\n\n\n\n\n\n# Create time series data\ntime_data &lt;- data.frame(\n  month = 1:12,\n  sales = c(100, 120, 115, 130, 145, 160, 155, 170, 180, 175, 190, 210)\n)\n\nggplot(time_data, aes(x = month, y = sales)) +\n  geom_line(color = \"steelblue\", size = 1) +\n  geom_point(color = \"steelblue\", size = 2) +\n  labs(\n    title = \"Monthly Sales Trend\",\n    x = \"Month\",\n    y = \"Sales\"\n  )\n\n\n\n\n\ngeom_bar() - counts observations (stat = ‚Äúcount‚Äù)\nggplot(mtcars, aes(x = factor(cyl))) +\n  geom_bar(fill = \"steelblue\") +\n  labs(\n    title = \"Count of Cars by Cylinder\",\n    x = \"Cylinders\",\n    y = \"Count\"\n  )\n\ngeom_col() - uses values directly (stat = ‚Äúidentity‚Äù)\nggplot(df, aes(x = Movie_Title, y = Total_Worldwide_Gross)) +\n  geom_col(fill = \"steelblue\") +\n  labs(\n    title = \"Worldwide Gross by Movie\",\n    x = \"Movie\",\n    y = \"Gross ($)\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nFlip coordinates for horizontal bars:\nggplot(df, aes(x = Movie_Title, y = Total_Worldwide_Gross)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Worldwide Gross by Movie\",\n    x = \"Movie\",\n    y = \"Gross ($)\"\n  )\n\n\n\n\n\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 2, fill = \"steelblue\", color = \"white\") +\n  labs(\n    title = \"Distribution of MPG\",\n    x = \"Miles Per Gallon\",\n    y = \"Count\"\n  )\n\n\n\n\n\nBox plots show distribution and outliers: - Box: Interquartile range (IQR) - middle 50% of data - Line in box: Median - Whiskers: Extend to 1.5 √ó IQR - Points: Outliers beyond whiskers\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_boxplot() +\n  labs(\n    title = \"MPG Distribution by Cylinders\",\n    x = \"Cylinders\",\n    y = \"Miles Per Gallon\"\n  ) +\n  theme(legend.position = \"none\")\n\nAdd jittered points to show actual data:\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"MPG Distribution by Cylinders\",\n    x = \"Cylinders\",\n    y = \"Miles Per Gallon\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nUsing the mtcars dataset:\n\nCreate a scatter plot of hp (horsepower) vs mpg (miles per gallon).\nCreate a bar chart showing the count of cars by number of gears (gear).\nCreate a histogram of hp with 10 bins.\nCreate a boxplot of mpg grouped by gear.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#aesthetics",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#aesthetics",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "A critical distinction! There‚Äôs an important difference between mapping a variable to an aesthetic and setting an aesthetic to a fixed value.\nMapping (inside aes()) - connects a variable to a visual property:\n# Color varies BY the data (each cylinder group gets a different color)\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 4)\n\nSetting (outside aes()) - applies a fixed value to all points:\n# ALL points are purple\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point(size = 4, color = \"purple\")\n\nCommon Error: What happens if you put a fixed value inside aes()?\n# WRONG! This creates a weird legend\nggplot(data = mtcars, aes(x = wt, y = mpg, color = \"purple\")) +\n  geom_point(size = 4)\n\nR thinks ‚Äúpurple‚Äù is a variable name, recycles it for every row, and picks its default color (salmon) ‚Äî not what you wanted!\n\n\n\n\n\n\n\nAesthetic\nDescription\nUsed With\n\n\n\n\ncolor\nOutline/line color\nPoints, lines, text\n\n\nfill\nInterior color\nBars, boxes, areas\n\n\nalpha\nTransparency (0-1)\nAll geoms\n\n\n\n# color vs fill\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot(color = \"darkblue\", fill = \"lightblue\", alpha = 0.7)\n\n\n\n\n\nPoint Shapes:\n\n\n\nShape Integer\nShape Name\nVisualization\n\n\n\n\n0\nsquare open\n‚ñ°\n\n\n1\ncircle open\n‚óã\n\n\n2\ntriangle open\n‚ñ≥\n\n\n15\nsquare filled\n‚ñ†\n\n\n16\ncircle filled\n‚óè\n\n\n17\ntriangle filled\n‚ñ≤\n\n\n18\ndiamond filled\n‚óÜ\n\n\n\n# Multiple aesthetics\nggplot(mtcars, aes(x = wt, y = mpg, \n                   color = factor(cyl), \n                   size = hp,\n                   shape = factor(gear))) +\n  geom_point(alpha = 0.7) +\n  labs(\n    color = \"Cylinders\",\n    size = \"Horsepower\",\n    shape = \"Gears\"\n  )\n\nLine Types:\n# Different line types\ndf_lines &lt;- data.frame(\n  x = rep(1:5, 3),\n  y = c(1:5, 2:6, 3:7),\n  group = rep(c(\"A\", \"B\", \"C\"), each = 5)\n)\n\nggplot(df_lines, aes(x = x, y = y, linetype = group, color = group)) +\n  geom_line(size = 1) +\n  labs(title = \"Different Line Types\")"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#labels-and-annotations",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#labels-and-annotations",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Fuel Efficiency by Vehicle Weight\",\n    subtitle = \"Data from mtcars dataset\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\",\n    caption = \"Source: mtcars dataset in R\"\n  )\n\n\n\n\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_text(aes(label = rownames(mtcars)), size = 2, vjust = -0.5) +\n  labs(title = \"MPG vs Weight with Car Names\")\n\nUse geom_label() for boxed labels or annotate() for single annotations:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  annotate(\"text\", x = 5, y = 30, label = \"High efficiency, heavy cars\", color = \"red\") +\n  annotate(\"rect\", xmin = 4.5, xmax = 5.5, ymin = 28, ymax = 35, alpha = 0.2, fill = \"red\")"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#scales-and-axes",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#scales-and-axes",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "Use scale_x_continuous() and scale_y_continuous() to customize numeric axes.\nggplot(df, aes(x = China_Box_Office_Gross, y = US_Box_Office_Gross)) +\n  geom_point(size = 4, color = \"steelblue\") +\n  scale_x_continuous(\n    labels = scales::comma,\n    breaks = seq(300000000, 600000000, by = 50000000)\n  ) +\n  scale_y_continuous(\n    labels = scales::dollar,\n    limits = c(0, 700000000)\n  ) +\n  labs(\n    title = \"Box Office Performance\",\n    x = \"China Gross\",\n    y = \"US Gross\"\n  )\n\n\n\n\n\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_boxplot() +\n  scale_x_discrete(labels = c(\"4\" = \"Four\", \"6\" = \"Six\", \"8\" = \"Eight\")) +\n  scale_fill_discrete(name = \"Cylinders\") +\n  labs(x = \"Number of Cylinders\")\n\n\n\n\n\nWhen working with dates, use scale_x_date() to format axis labels.\n# Create date data\ndf_dates &lt;- df |&gt; \n  mutate(Release_Date_Parsed = dmy(paste(\"01\", Release_Date)))\n\nggplot(df_dates, aes(x = Release_Date_Parsed, y = Total_Worldwide_Gross)) +\n  geom_line(color = \"steelblue\", size = 1) +\n  geom_point(color = \"steelblue\", size = 3) +\n  scale_x_date(\n    date_breaks = \"1 month\",\n    date_labels = \"%b %Y\"\n  ) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(\n    title = \"Worldwide Gross Over Time\",\n    x = \"Release Date\",\n    y = \"Total Worldwide Gross\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nFunction\nPurpose\n\n\n\n\nscale_color_manual()\nSet colors manually\n\n\nscale_fill_manual()\nSet fill colors manually\n\n\nscale_color_brewer()\nUse ColorBrewer palettes\n\n\nscale_color_viridis_d()\nViridis discrete palette\n\n\nscale_color_viridis_c()\nViridis continuous palette\n\n\nscale_color_gradient()\nContinuous gradient\n\n\nscale_color_gradient2()\nDiverging gradient\n\n\n\n# Manual colors\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  scale_color_manual(values = c(\"red\", \"green\", \"blue\"))"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#themes",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#themes",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "Theme\nDescription\n\n\n\n\ntheme_minimal()\nClean, minimal design\n\n\ntheme_classic()\nClassic with axes, no gridlines\n\n\ntheme_light()\nLight background, subtle gridlines\n\n\ntheme_dark()\nDark background\n\n\ntheme_bw()\nBlack and white, good for printing\n\n\ntheme_void()\nEmpty, no axes or background\n\n\n\np &lt;- ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Theme Comparison\")\n\n# Compare themes\np + theme_minimal() + labs(subtitle = \"theme_minimal()\")\n\np + theme_classic() + labs(subtitle = \"theme_classic()\")\n\np + theme_bw() + labs(subtitle = \"theme_bw()\")\n\n\n\n\n\nUse theme() to customize individual elements:\n\n\n\nElement\nDescription\n\n\n\n\nplot.title\nMain title appearance\n\n\nplot.subtitle\nSubtitle appearance\n\n\naxis.title\nAxis label appearance\n\n\naxis.text\nAxis tick label appearance\n\n\nlegend.position\nLegend location\n\n\npanel.grid\nGridline appearance\n\n\npanel.background\nPlot area background\n\n\n\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 4) +\n  labs(\n    title = \"Fuel Efficiency by Vehicle Weight\",\n    subtitle = \"Custom themed plot\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles Per Gallon\",\n    color = \"Cylinders\"\n  ) +\n  theme_light() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5, color = \"darkblue\"),\n    plot.subtitle = element_text(face = \"italic\", size = 12, hjust = 0.5),\n    axis.title = element_text(face = \"bold\", size = 12),\n    axis.text = element_text(size = 10),\n    legend.position = \"top\",\n    panel.grid.major = element_line(color = \"gray90\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\nThe ggthemes package provides additional themes:\ninstall.packages(\"ggthemes\")\nlibrary(ggthemes)\n\n# Economist theme\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Economist Theme\") +\n  theme_economist() +\n  scale_color_economist()\n\n# Wall Street Journal theme\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Wall Street Journal Theme\") +\n  theme_wsj()\n\n# Tufte theme (minimalist)\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(title = \"Tufte Theme\") +\n  theme_tufte()"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#facets",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#facets",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "Facets split data into multiple panels by a variable.\n\n\nUse facet_wrap() for a single grouping variable:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\", size = 2) +\n  facet_wrap(~ cyl) +\n  labs(title = \"MPG vs Weight by Cylinders\")\n\nControl layout with ncol and nrow:\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  facet_wrap(~ cyl, ncol = 3) +\n  labs(title = \"Faceted by Cylinders (3 columns)\") +\n  theme(legend.position = \"none\")\n\nFree scales:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\") +\n  facet_wrap(~ gear, scales = \"free\") +\n  labs(title = \"Faceted with Free Scales\")\n\n\n\n\n\nUse facet_grid() for two grouping variables:\n# Rows = cyl, Columns = gear\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  facet_grid(cyl ~ gear) +\n  labs(title = \"Facet Grid: Cylinders √ó Gears\") +\n  theme(legend.position = \"none\")\n\nFacet by rows only:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\") +\n  facet_grid(cyl ~ .) +\n  labs(title = \"Facet Grid: Rows by Cylinders\")\n\nFacet by columns only:\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\") +\n  facet_grid(. ~ gear) +\n  labs(title = \"Facet Grid: Columns by Gears\")\n\n\n\n\n\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  facet_wrap(\n    ~ cyl,\n    labeller = labeller(cyl = c(\"4\" = \"4 Cylinders\", \"6\" = \"6 Cylinders\", \"8\" = \"8 Cylinders\"))\n  ) +\n  labs(title = \"Custom Facet Labels\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#color-palettes",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#color-palettes",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "ggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"tomato\", \"steelblue\", \"forestgreen\")) +\n  labs(title = \"Manual Color Selection\")\n\n\n\n\n\nColorBrewer provides color palettes designed for data visualization:\nlibrary(RColorBrewer)\ndisplay.brewer.all()\n\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"ColorBrewer: Set2\")\n\n\n\n\n\nViridis palettes are colorblind-friendly and print well in grayscale:\nggplot(mtcars, aes(x = wt, y = mpg, color = hp)) +\n  geom_point(size = 4) +\n  scale_color_viridis_c() +\n  labs(title = \"Viridis Continuous Scale\")\n\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_viridis_d() +\n  labs(title = \"Viridis Discrete Scale\")\n\n\n\n\n\nwesanderson:\ninstall.packages(\"wesanderson\")\nlibrary(wesanderson)\n\n# See available palettes\nnames(wes_palettes)\n##  [1] \"BottleRocket1\"     \"BottleRocket2\"     \"Rushmore1\"        \n##  [4] \"Rushmore\"          \"Royal1\"            \"Royal2\"           \n##  [7] \"Zissou1\"           \"Zissou1Continuous\" \"Darjeeling1\"      \n## [10] \"Darjeeling2\"       \"Chevalier1\"        \"FantasticFox1\"    \n## [13] \"Moonrise1\"         \"Moonrise2\"         \"Moonrise3\"        \n## [16] \"Cavalcanti1\"       \"GrandBudapest1\"    \"GrandBudapest2\"   \n## [19] \"IsleofDogs1\"       \"IsleofDogs2\"       \"FrenchDispatch\"   \n## [22] \"AsteroidCity1\"     \"AsteroidCity2\"     \"AsteroidCity3\"\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_manual(values = wes_palette(\"GrandBudapest1\", n = 3)) +\n  labs(title = \"Wes Anderson: Grand Budapest\")\n\nNatParksPalettes:\ninstall.packages(\"NatParksPalettes\")\nlibrary(NatParksPalettes)\n\n# See available palettes\nnames(NatParksPalettes)\n##  [1] \"Acadia\"      \"Arches\"      \"Arches2\"     \"Banff\"       \"BryceCanyon\"\n##  [6] \"CapitolReef\" \"Charmonix\"   \"CraterLake\"  \"Cuyahoga\"    \"DeathValley\"\n## [11] \"Denali\"      \"Everglades\"  \"Glacier\"     \"GrandCanyon\" \"Halekala\"   \n## [16] \"IguazuFalls\" \"KingsCanyon\" \"LakeNakuru\"  \"Olympic\"     \"Redwood\"    \n## [21] \"RockyMtn\"    \"Saguaro\"     \"SmokyMtns\"   \"SouthDowns\"  \"Torres\"     \n## [26] \"Triglav\"     \"WindCave\"    \"Volcanoes\"   \"Yellowstone\" \"Yosemite\"\nggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +\n  geom_bar() +\n  scale_fill_manual(values = natparks.pals(\"Yellowstone\", n = 3)) +\n  labs(title = \"National Parks: Yellowstone\")\n\n\n\n\nUsing the iris dataset, create a publication-ready visualization:\n\nCreate a scatter plot of Sepal.Length vs Sepal.Width, colored by Species.\nAdd appropriate labels (title, subtitle, axis labels, caption).\nUse a custom color palette (try wesanderson or ColorBrewer).\nApply a theme (try theme_minimal() or theme_classic()).\nCustomize the theme with:\n\nBold, centered title\nLegend at the bottom\nCustom axis text size\n\nCreate a faceted version by Species.\n\n# Starter code\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(size = 3) +\n  # Add your customizations here\n  labs(\n    title = \"...\",\n    subtitle = \"...\",\n    x = \"...\",\n    y = \"...\",\n    caption = \"...\"\n  )"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L3-github.html#lecture-3-cheat-sheet",
    "href": "Spring2026/LectureSlides/bigdata_L3-github.html#lecture-3-cheat-sheet",
    "title": "GGPlot and Visualizations",
    "section": "",
    "text": "Topic\nKey Points\n\n\n\n\nggplot Structure\nggplot(data, aes()) + geom_*() + ... - build plots layer by layer\n\n\nCommon Geoms\ngeom_point(): scatter; geom_line(): lines; geom_bar()/geom_col(): bars; geom_histogram(): distribution; geom_boxplot(): box plots\n\n\nMapping vs Setting\nInside aes(): map variable to aesthetic; Outside aes(): set fixed value\n\n\nAesthetics\ncolor: outlines/lines; fill: interior; alpha: transparency; size: point/line size; shape: point shape; linetype: line style\n\n\nLabels\nlabs(title, subtitle, x, y, color, fill, caption)\n\n\nScales\nscale_x_continuous(), scale_y_continuous(), scale_x_date(), scale_color_manual(), scale_fill_brewer()\n\n\nThemes\ntheme_minimal(), theme_classic(), theme_bw(), theme_void(); Customize with theme()\n\n\nFacets\nfacet_wrap(~ var): single variable; facet_grid(row ~ col): two variables\n\n\nColor Palettes\nManual: scale_*_manual(values = c(...)); Brewer: scale_*_brewer(palette = \"...\"); Viridis: scale_*_viridis_d()\n\n\nggthemes\ntheme_economist(), theme_wsj(), theme_tufte() for publication-ready styles"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html",
    "title": "Why computational social science?",
    "section": "",
    "text": "Dr.¬†Ayse D. Lokmanoglu Lecture 1, (B) Jan 21, 2026, (A) Jan 26, 2026"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#lecture-1-table-of-contents",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#lecture-1-table-of-contents",
    "title": "Why computational social science?",
    "section": "Lecture 1 Table of Contents",
    "text": "Lecture 1 Table of Contents\n\n\n\nSection\nTopic\n\n\n\n\n1\nIntro to R Studio\n\n\n1.1\nThe Console\n\n\n1.2\nThe Terminal\n\n\n1.3\nProjects\n\n\n1.4\nOrganize your project file\n\n\n1.5\nScripts\n\n\n2\nWorking directory and paths\n\n\n2.1\nCheck your working directory\n\n\n3\nR Markdown\n\n\n3.1\nHow to create a document in R Markdown\n\n\n3.2\nEmbed Code\n\n\n3.3\nGlobal Chunk Options\n\n\n3.4\nRunning vs knitting\n\n\n4\nR Packages\n\n\n4.1\nInstalling & Loading\n\n\n5\nR Basic Grammar\n\n\n5.1\nVariable Assignment\n\n\n5.2\nCase Sensitivity\n\n\n5.3\nComments\n\n\n5.4\nReserved Keywords\n\n\n6\nData Types\n\n\n6.1\nNumeric, Character, Logical, Factor\n\n\n6.2\nDates\n\n\n6.3\nLubridate Package\n\n\n7\nBasic Arithmetic\n\n\n8\nVectors\n\n\n8.1\nCreating Vectors\n\n\n8.2\nVector Indexing\n\n\n8.3\nVector Functions\n\n\n8.4\nSequences\n\n\n8.5\nCombining Vectors & Type Coercion\n\n\n8.6\nVector Arithmetic & Recycling\n\n\n9\nData Frames\n\n\n9.1\nCreating Data Frames\n\n\n9.2\nExploring/Inspecting Data Frames\n\n\n9.3\nAccessing Data\n\n\n9.4\nAdding and Removing Data\n\n\n9.5\nFiltering and Subsetting\n\n\n9.6\nOrdering Data\n\n\n9.7\nCommon Data Frame Functions\n\n\n9.8\nBasic Aggregations"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#intro-to-r-studio",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#intro-to-r-studio",
    "title": "Why computational social science?",
    "section": "1. Intro to R Studio",
    "text": "1. Intro to R Studio\nRStudio is an integrated development environment (IDE) for R, a programming language widely used for data analysis, statistical modeling, and visualization\n\n\n\n1.1 The Console\nThe Console is where you directly interact with R. You can type R commands, hit Enter, and see the immediate results.\nKey Features: - Executes code line-by-line. - Useful for quick calculations or testing small snippets of code. - Does not save commands‚Äîonce you close RStudio, the history of commands in the Console is gone unless explicitly saved.\nTRY: write 2 + 2 in the console and press Enter.\n\n\n\n\n1.2 The Terminal\nThe Terminal is separate from the Console and provides access to your computer‚Äôs command-line interface (CLI).\nKey Features: - Allows you to run system-level commands (e.g., navigating file systems, managing files). - Useful for integrating with tools like Git or installing software packages.\nKey Difference from Console: - The Console is R-specific, while the Terminal is for general command-line tasks.\nWe will not use Terminal most of the time\nTRY: In the Terminal, type ls (Mac/Linux) or dir (Windows) to list files in your current directory.\n\n\n\n\n1.3 Project\nAn RStudio Project is a way to organize your work by grouping files, data, and scripts for a specific task or analysis.\nWhy Use Projects? - Keeps your workspace clean and focused. - Automatically sets the working directory to the project folder. - Ensures reproducibility by keeping everything needed for a project in one place.\nHow to Create a Project: 1. Click on File ‚Üí New Project‚Ä¶.. 2. Choose whether to create a new directory, use an existing directory, or clone a Git repository. 3. Give it a clear name (example: EMS747_Project) 4. Choose a location you can easily find again\nTRY: Create a project called ‚ÄúEMS747_Class_Exercises‚Äù and notice how RStudio creates a .Rproj file for managing it.\nWhen you create a project, RStudio makes a file ending in .Rproj. Always open your work by clicking that .Rproj file.\n\n\n\n\n1.4 Organize your project file\nInside your project folder, create these folders:\n\ndata/ for datasets (CSV, Excel, etc.)\n\nscripts/ for R scripts you write during the semester\n\nIn RStudio: use the Files pane ‚Üí New Folder.\n\nYour project should look like this:\nEMS747_Project/\n  EMS747_Project.Rproj\n  data/\n  scripts/\n  bigdata_L1_github.Rmd\n\n\n\n1.5 Scripts\nA script is a text file where you write and save R code for future use. - Scripts let you document your work, making it reproducible and shareable. - You can save time by re-running pre-written code instead of typing commands repeatedly. How to Create and Use a Script: - Click on File ‚Üí New File ‚Üí R Script. - Write R code in the script editor. - E.g.\nx &lt;- 5\ny &lt;- 10\nz &lt;- x + y\nprint(z)\n## [1] 15\n\nHighlight the code you want to run and press Ctrl+Enter (Windows/Linux) or Cmd+Enter (Mac) to execute it in the Console."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#working-directory-and-paths",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#working-directory-and-paths",
    "title": "Why computational social science?",
    "section": "2. Working directory and paths",
    "text": "2. Working directory and paths\n\n2.1 Check your working directory\nWhen your project is open, run:\ngetwd()\n## [1] \"/Users/alokman/Library/CloudStorage/GoogleDrive-alokman@bu.edu/My Drive/Teaching/S26_EMS747/ClassSlides\"\nYou should see a path ending in your project folder name.\n\nWhy this matters\nIn the next lecture, we will load datasets from your data/ folder using relative paths like:\nread.csv(\"data/my_file.csv\")"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#r-markdown",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#r-markdown",
    "title": "Why computational social science?",
    "section": "3. R Markdown",
    "text": "3. R Markdown\n\nR Markdown is a framework that allows you to integrate code, output, and text in one document.\nYou can produce reports in multiple formats: HTML, PDF, Word, etc.\n\nInstead of copying results into a separate document, you generate the document directly from the code.\nWhy do we use R Markdown? - Combine code and text for reproducible research. - Create interactive and visually appealing documents. - Easy to share analyses with others.\nR Markdown Cheathseet\n\n\n3.1 How to create a document in R Markdown\n\nClick on File &gt; New File &gt; R Markdown.\nChoose a title, author, and output format (HTML, PDF, or Word).\nEdit the template provided in the new .Rmd file.\nFor our class you can use the Markdown from BB and take notes on that!\n\n\n\n\n3.2 Embed Code\nR code in R Markdown is written inside code chunks.\nYou can create a chunk by typing it manually or by using the Insert Code Chunk button in RStudio.\nCode chunks start with {r} and end with.\nYou can also add: - OPTIONAL: a chunk label (for identification) - chunk options (to control how code and output appear)\nExample:\n\nChunk labels are OPTIONAL and if you do put one it should be short and descriptive.\nChunk options are written after the label, separated by commas.\n\n\n\n\n\n3.3 Global Chunk Options\nYou can set options that apply to all code chunks in the document. This is usually done in the first chunk of the file.\nExample, this displays code in the output document, prevents messages (such as package startup messages) and warnings from appearing in the output document. :\nknitr::opts_chunk$set(\n  echo = TRUE,\n  message = FALSE, \n  warning = FALSE\n)\n\n\n\n\n\n\n\n\nOption\nDefault\nEffects\n\n\n\n\necho\nTRUE\nDisplay code in the output document\n\n\nerror\nFALSE\nTRUE = display error messages in the document; FALSE = stop rendering when an error occurs\n\n\neval\nTRUE\nRun code in the chunk\n\n\ninclude\nTRUE\nInclude the chunk output in the document after running\n\n\nmessage\nTRUE\nDisplay messages in the document\n\n\nwarning\nTRUE\nDisplay warnings in the document\n\n\nresults\n\"markup\"\n\"asis\" = pass results through as-is; \"hide\" = hide results; \"hold\" = show all results after all code\n\n\nfig.align\n\"default\"\nAlign figure: \"left\", \"right\", or \"center\"\n\n\nfig.alt\nNULL\nAlt text for a figure\n\n\nfig.cap\nNULL\nFigure caption (character string)\n\n\nfig.path\n\"figure/\"\nPrefix folder for generated figure file paths\n\n\nfig.width / fig.height\n7\nPlot dimensions in inches\n\n\nout.width\n(none)\nRescale output width (e.g., \"75%\", \"300px\")\n\n\ncollapse\nFALSE\nCollapse source and output into a single block\n\n\ncomment\n\"##\"\nPrefix for each line of results\n\n\nchild\nNULL\nFile(s) to knit and include in the document\n\n\npurl\nTRUE\nInclude/exclude chunk when extracting code with knitr::purl()\n\n\n\nSee more options and defaults by running str(knitr::opts_chunk$get()).\n\n\n\n3.4 Running vs knitting\n\nRun a chunk to execute code line by line\n\nKnit to rebuild the entire document from top to bottom.\nIn RMD empty space is for text.\nWhen you add {r, eval = FALSE} that code chunk will NOT run."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#r-packages",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#r-packages",
    "title": "Why computational social science?",
    "section": "4. R Packages",
    "text": "4. R Packages\n\nPackages are collections of R functions, data, and compiled code.\nExtend the functionality of base R.\nSimplifies complex tasks.\nWidely used for specialized analyses.\n\nE.g., tidy, dplyr, ggplot2\n\n\n4.1 Installing & Loading Packages\n\nInstallation\n\nUse the install.packages() function to install a package from CRAN.\n\n\ninstall.packages(\"tidyr\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"stringr\")\n\nLoading\n\nUse the library() function to load an installed package.\n\n\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(stringr)\n\nCheck the package documentation with ?\n\n?tidyr\n?dplyr\nN.B. Packages only need to be installed once but must be loaded in each session library(). Keep packages updated with update.packages()"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#r-basic-grammar",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#r-basic-grammar",
    "title": "Why computational social science?",
    "section": "5. R: Basic Grammar",
    "text": "5. R: Basic Grammar\n\n5.1 Variable Assignment\nVariables are used to store data or values.\n= (Simple Assignment) Similar to python\n&lt;- (Leftward Assignment) Most Common used by R coders and we will use this\n-&gt; (Rightward Assignment) Rarely used\nx = \"Simple_Assignment\" \nprint(x)\n## [1] \"Simple_Assignment\"\ny &lt;- \"Leftward Assignment\"\nprint(y)\n## [1] \"Leftward Assignment\"\n\"Rightward_Assignment\" -&gt; z\nprint(z)\n## [1] \"Rightward_Assignment\"\n\n\n\n5.2 R is case-sensitive\nx &lt;- 2\n\nprint(X)\n## Error: object 'X' not found\nIt gave an error, why?\nCause R is  case sensitive \nprint(x)\n## [1] 2\n\n\n\n5.3 Comments\nTo comment - Comments are notes in the code that R ignores. Use # to write comments. - R only has single line comments so if you want multiple lines you need to repeat the # for each line.\nvariable_2 &lt;- \"Leftward Assignment\" ## this is the most common used by R coders\n# Other's work as well \n\n\n\n5.4 R Reserved Keywords\nYou cannot use these keywords as variable names. These are reserved keywords for R.\n\n\n\n\n\n\n\nWords\nDescription\n\n\n\n\nif\nUsed for conditional execution of code blocks.\n\n\nelse\nSpecifies an alternative block of code to execute if the if condition is false.\n\n\nwhile\nExecutes a block of code repeatedly as long as a condition is true.\n\n\nrepeat\nCreates an infinite loop that must be terminated with a break statement.\n\n\nfor\nLoops through a sequence of elements.\n\n\nfunction\nDefines a function, a reusable block of code.\n\n\nin\nUsed in for loops to specify the sequence being iterated over.\n\n\nnext\nSkips the current iteration in a loop and moves to the next one.\n\n\nbreak\nExits a loop immediately.\n\n\nTRUE\nLogical constant representing a boolean value of true.\n\n\nFALSE\nLogical constant representing a boolean value of false.\n\n\nNULL\nRepresents the absence of a value or an undefined value.\n\n\nInf\nRepresents infinity (e.g., division by zero).\n\n\nNaN\nRepresents ‚ÄúNot a Number,‚Äù often resulting from undefined mathematical operations.\n\n\nNA\nRepresents missing data or ‚ÄúNot Available.‚Äù\n\n\nNA_integer_\nRepresents a missing integer value.\n\n\nNA_complex_\nRepresents a missing complex number.\n\n\nNA_real_\nRepresents a missing real (numeric) value."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#data-types",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#data-types",
    "title": "Why computational social science?",
    "section": "6. Data Types",
    "text": "6. Data Types\n\n6.1 Numeric, Character, Logical, Factor\n\nNumeric: Numbers\n\ne.g., 3.14, 42\n\n-Character: Text or strings\n\ne.g., \"Hello\", \"R\"\n\nLogical: Boolean values\n\nTRUE, FALSE\n\nFactor: Categorical data\n\ne.g., \"Male\", \"Female\"\n\nYou can use typeof() to see what type of data it is\n\nTRY: Writing different data types\nage &lt;- 25  # Numeric\ntypeof(age)\n## [1] \"double\"\nname &lt;- \"Alice\"  # Character\ntypeof(name)\n## [1] \"character\"\nis_student &lt;- TRUE  # Logical\ntypeof(is_student)\n## [1] \"logical\"\nQuestion? What is double?\n\nTRY: Create variables and see what types they are\n\n\n\n6.2 Dates\n\nDate: Represents calendar dates.\n\ne.g., ‚Äú2023-01-01‚Äù\n\nPOSIXct/POSIXlt: Represents date and time.\n\ne.g., ‚Äú2023-01-01 12:34:56‚Äù\n\nR uses the Date and POSIXct/POSIXlt classes for working with dates and times.\nUse as.Date() to convert strings to dates.\nUse Sys.Date() for the current date.\nUse Sys.time() for the current date and time.\n\na &lt;- \"2023-01-01\"\ntypeof(a)\n## [1] \"character\"\nb &lt;- as.Date(a)\ntypeof(b)\n## [1] \"double\"\nTRY: Converting strings to dates\n# Convert a string to a date\nmy_date &lt;- as.Date(\"2023-01-01\")\ntypeof(my_date)\n\n# Get the current date\ntoday &lt;- Sys.Date()\n\n# Add 7 days to a date\nfuture_date &lt;- today + 7\n\n# Display the date and class\nprint(future_date)\nclass(future_date)\n\n\n6.3 Lubridate Package\nLubridate is a package that makes working with dates easier.\n\nLubridate Cheat Sheet - It provides easy functions to parse, manipulate, and extract date-time components.\n\ninstall.packages(\"lubridate\") # only once\nlibrary(lubridate) # everytime you start R\nKey Functions:\n\nParsing Dates and Times:\n\nymd(), dmy(), mdy(): Convert strings to dates.\nymd_hms(), mdy_hms(): Handle date-time strings with hours, minutes, seconds.\n\nExtracting Components:\n\nyear(), month(), day(): Extract parts of a date.\nhour(), minute(), second(): Extract time components.\n\nManipulating Dates:\n\ntoday(), now(): Current date or date-time.\nArithmetic: Add or subtract days, months, etc.\n\nTime Zones:\n\nSet or change time zones with with_tz() or force_tz().\n\n\n\nTRY: Play with dates\nlibrary(lubridate)\n\n# Parse a date\nmy_date &lt;- ymd(\"2023-01-01\")\n\n# Parse a date-time\nmy_datetime &lt;- ymd_hms(\"2023-01-01 12:34:56\")\n\n# Extract components\nyear(my_date)    # 2023\n## [1] 2023\nmonth(my_date)   # 1\n## [1] 1\nday(my_date)     # 1\n## [1] 1\nhour(my_datetime) # 12\n## [1] 12\n# Add 7 days\nfuture_date &lt;- my_date + days(7)\nmy_date + months(6)\n## [1] \"2023-07-01\"\n# Set a time zone\nnew_timezone &lt;- with_tz(my_datetime, tzone = \"America/New_York\")\nprint(new_timezone)\n## [1] \"2023-01-01 07:34:56 EST\""
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#basic-arithmetic",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#basic-arithmetic",
    "title": "Why computational social science?",
    "section": "7. Basic Arithmetic",
    "text": "7. Basic Arithmetic\nOperators in R:\n\nAddition: +\nSubtraction: -\nMultiplication: *\nDivision: /\nExponentiation: ^\n\nTRY\na &lt;- 10\nb &lt;- 3\n\nsum &lt;- a + b  # Addition\nprint(sum)  \n## [1] 13\nproduct &lt;- a * b  # Multiplication\nprint(product)\n## [1] 30\npower &lt;- a ^ b  # Exponentiation\nprint(power)\n## [1] 1000\nTRY - Create variables and use basic arithmetic, I started for you play with them more!\nx&lt;-20\nx&lt;-5\nx\n## [1] 5\ny&lt;-10\nx*y\n## [1] 50"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#vectors",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#vectors",
    "title": "Why computational social science?",
    "section": "8. Vectors",
    "text": "8. Vectors\nA vector is a sequence of data elements of the same type.\n\n\n8.1 Creating Vectors\n\nCreating Vectors: Use the c() function.\n\nTRY\nnumbers &lt;- c(1, 2, 3, 4, 5)  # Numeric vector\nnames &lt;- c(\"Alice\", \"Bob\", \"Charlie\")  # Character vector\nis_student &lt;- c(TRUE, FALSE, TRUE)  # Logical vector\nTRY what types they are\ntypeof(numbers)\n## [1] \"double\"\nYou can access an element in a vector with square brackets []\nprint(numbers[2])  # Prints the second element of the vector\n## [1] 2\n\n\nVectors beyond numbers\n\nVectors can hold various types of data, including:\nCharacter: Strings of text.\nNumeric: Numbers.\nLogical: TRUE, FALSE.\n\nphrase &lt;- \"Vectors are fun!\"\nphrase\n## [1] \"Vectors are fun!\"\nfruits &lt;- c(\"Banana\", \"Mango\", \"Strawberry\", \"Grapes\")  # ADD YOUR FAVORITES!\nfruits\n## [1] \"Banana\"     \"Mango\"      \"Strawberry\" \"Grapes\"\n\n\n\n\n8.2 Vector functions\n\nAccess specific items with [] (we practices above)\nlength(): Finds the number of elements in a vector.\n\n# Vector properties\nlength(fruits)       # Number of elements\n## [1] 4\nfruits * 2           # Produces an ERROR\n## Error in fruits * 2: non-numeric argument to binary operator\n1:length(fruits)     # Sequence from 1 to the vector length\n## [1] 1 2 3 4\nfruits[2:3]          # Access multiple items\n## [1] \"Mango\"      \"Strawberry\"\nfruits[-c(1, 4)]     # Exclude items 1 and 4\n## [1] \"Mango\"      \"Strawberry\"\nWhy does fruits * 2 produce an error?\n\n\n\n8.3 Vector Sequences\nWe can create sequences with vectors using :\nA &lt;- 11:20\nprint(A)\n##  [1] 11 12 13 14 15 16 17 18 19 20\nB &lt;- 0:10\nprint(B)\n##  [1]  0  1  2  3  4  5  6  7  8  9 10\nTRY: Vector functions with your number vectors\n# Vector properties\nlength(A)       # Number of elements\n## [1] 10\nB * 2           # This time no error\n##  [1]  0  2  4  6  8 10 12 14 16 18 20\n1:length(A)     # Sequence from 1 to the vector length\n##  [1]  1  2  3  4  5  6  7  8  9 10\nB[2:3]          # Access multiple items\n## [1] 1 2\nA[-c(1, 4)]     # Exclude items 1 and 4\n## [1] 12 13 15 16 17 18 19 20\n\n\n\n8.4 Vector Indexing\n\nAccess items with []\nExclude an item with -:\n\nfruits[3]  # Third item\n## [1] \"Strawberry\"\nfruits[c(1, 4)]  # First and fourth items\n## [1] \"Banana\" \"Grapes\"\nfruits[-2]  # Exclude the second item\n## [1] \"Banana\"     \"Strawberry\" \"Grapes\"\nPractice: - How do you select only items 1, 3, and 5? - How do you exclude items 2 and 4?\n# Selecting items:\nfruits[c(1, 3, 5)]\n## [1] \"Banana\"     \"Strawberry\" NA\n# Excluding items:\nfruits[-c(2, 4)]\n## [1] \"Banana\"     \"Strawberry\"\n# Saving a subset:\nfavorite_fruits &lt;- fruits[c(1, 3)]\nfavorite_fruits\n## [1] \"Banana\"     \"Strawberry\"\n\n\n\n8.5 Combining Vectors & Type Coercion\nCombine vectors of different types:\nnumbers &lt;- 1:5\ntext &lt;- \"Hello\"\ncombo &lt;- c(numbers, text)\ncombo\n## [1] \"1\"     \"2\"     \"3\"     \"4\"     \"5\"     \"Hello\"\nType Coercion:\n\nR converts all elements in a vector to the same type.\nOrder of precedence:\n\nCharacter &gt; Numeric &gt; Logical.\n\n\ntypeof(numbers)      # \"integer\"\n## [1] \"integer\"\ntypeof(text)         # \"character\"\n## [1] \"character\"\ntypeof(combo)        # \"character\"\n## [1] \"character\"\nPractice:\n\nCombine a vector of numbers and a vector of colors.\nCheck the resulting vector‚Äôs type using typeof().\n\n\n\nVectors class exercises\n\nCreate a vector of your favorite hobbies.\n\nAccess the first two items.\nExclude the last item.\n\nCombine vectors of numbers and character strings.\n\nWhat is the type of the resulting vector?\nWhat happens if you add TRUE to the vector?\n\n\nnumbers &lt;- 1:3\nstrings &lt;- c(\"One\", \"Two\", \"Three\")\ncombined &lt;- c(numbers, strings)\n\n\n\n\n8.6 Vector Arithmetic & Recycling\nWe can also do artihmetics with vectors!\nA + B\n##  [1] 11 13 15 17 19 21 23 25 27 29 21\nWhat will happen if we vectors of different sizes?\nD &lt;- 20:24\nE &lt;- 25:30\n\nD+E\n## [1] 45 47 49 51 53 50\nR applies recycling which means‚Ä¶\n\nD (length 5) is recycled to match the length of E (length 6).\nD becomes: 20, 21, 22, 23, 24, 20 (repeating the first element).\nThen, element-wise addition is performed:\n\n(20 + 25), (21 + 26), (22 + 27), (23 + 28), (24 + 29), (20 + 30) - 45, 47, 49, 51, 53, 50"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L1-github.html#data-frames",
    "href": "Spring2026/LectureSlides/bigdata_L1-github.html#data-frames",
    "title": "Why computational social science?",
    "section": "9. Data Frames",
    "text": "9. Data Frames\nA data frame is a table-like structure (multiple vectors combined into rows and column) in R where each column can have a different data type (numeric, character, logical, etc.). Think of it as a spreadsheet where each column is a variable, and each row is an observation.\n\n9.1 Create a dataframe\n\nUse the data.frame() function to create a data frame.\n\n# Create a data frame\ndf &lt;- data.frame(\n  Name = c(\"Ayse\", \"Jessy\", \"Chris\"),  # Character column\n  Age = c(25, 30, 35),  # Numeric column\n  Professor = c(TRUE, FALSE, TRUE)  # Logical column\n)\n\n# Print the data frame\nprint(df)\n##    Name Age Professor\n## 1  Ayse  25      TRUE\n## 2 Jessy  30     FALSE\n## 3 Chris  35      TRUE\n### OR\n\nName &lt;- c(\"Ayse\", \"Jessy\", \"Chris\")  # Character column\nAge &lt;- c(25, 30, 35)  # Numeric column\nProfessor &lt;- c(TRUE, FALSE, TRUE)  # Logical column\n\ndf &lt;- data.frame(Name, Age, Professor)\nprint(df)\n##    Name Age Professor\n## 1  Ayse  25      TRUE\n## 2 Jessy  30     FALSE\n## 3 Chris  35      TRUE\n\n\nVectors in a data frame must have the same length!!\n# Mismatched lengths will fail\nnames &lt;- c(\"Alice\", \"Bob\", \"Charlie\")\nages &lt;- c(25, 30, NA)  # Fewer elements\npeople &lt;- data.frame(names, ages)  # ERROR\nLet‚Äôs fix the error\n# Fix the error:\nages &lt;- c(25, 30, 35)\npeople &lt;- data.frame(names, ages)\npeople\n##     names ages\n## 1   Alice   25\n## 2     Bob   30\n## 3 Charlie   35\n\nTRY: Practice creating a dataframe\n\nCreate a vector of your favorite sports.\n\nSelect items 2 through 4.\nExclude item 1.\n\nCombining Vectors:\n\nCombine a vector of numbers (1:5) with a vector of shapes (\"Circle\", \"Square\", \"Triangle\").\nCheck the type of the resulting vector.\n\nBuilding Data Frames:\n\nCreate a data frame with us_df:\n\nus_cities &lt;- c(\"Boston\", \"Chicago\", \"Seattle\")\nus_populations &lt;- c(700000, 2700000, 750000)\n\nCreate another data frame with asia_df:\n\nasia_cities &lt;- c(\"Beijing\", \"Shanghai\", \"Taipei\", \"Kaohsiung\")\nasia_populations &lt;- c(21540000, 24280000, 2640000, 2775000)\n\n\n\n\nCreate a data frame with your own set of favorite cities (at least 3 cities and estimated populations).\nAdd more cities to the asia_df:\n\nInclude ‚ÄúGuangzhou‚Äù with a population of 18,810,000.\nInclude ‚ÄúNew Taipei City‚Äù with a population of 4,000,000.\n\nCombine all cities into one data frame:\n\nMerge US and Asian city data frames into one combined table.\n\n\n\n\n\n\n9.2 Exploring/Inspecting Data Frames\nBasic Functions to Explore a Data Frame:\n\nhead(df): Displays the first 6 rows of the data frame.\ntail(df): Displays the last 6 rows.\ndim(df): Returns the dimensions (rows, columns).\nstr(df): Shows the structure of the data frame, including data types.\nsummary(df): Provides summary statistics for each column.\n\nhead(df)        # First 6 rows\n##    Name Age Professor\n## 1  Ayse  25      TRUE\n## 2 Jessy  30     FALSE\n## 3 Chris  35      TRUE\ntail(df)        # Last 6 rows\n##    Name Age Professor\n## 1  Ayse  25      TRUE\n## 2 Jessy  30     FALSE\n## 3 Chris  35      TRUE\ndim(df)         # Dimensions: rows and columns\n## [1] 3 3\ndim(mtcars)\n## [1] 32 11\nstr(df)  \n## 'data.frame':    3 obs. of  3 variables:\n##  $ Name     : chr  \"Ayse\" \"Jessy\" \"Chris\"\n##  $ Age      : num  25 30 35\n##  $ Professor: logi  TRUE FALSE TRUE\nstr(mtcars) # Structure\n## 'data.frame':    32 obs. of  11 variables:\n##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n##  $ disp: num  160 160 108 258 360 ...\n##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n##  $ qsec: num  16.5 17 18.6 19.4 17 ...\n##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\nsummary(df)     # Summary statistics\n##      Name                Age       Professor      \n##  Length:3           Min.   :25.0   Mode :logical  \n##  Class :character   1st Qu.:27.5   FALSE:1        \n##  Mode  :character   Median :30.0   TRUE :2        \n##                     Mean   :30.0                  \n##                     3rd Qu.:32.5                  \n##                     Max.   :35.0\n\n\n\n9.3 Accessing Data\nAccessing Columns:\n\nUse the $ operator:df$ColumnName.\nUse bracket notation: df[ , \"ColumnName\"].\n\nprint(df$Name)  # Access the 'Name' column\n## [1] \"Ayse\"  \"Jessy\" \"Chris\"\nsummary(df$Name)\n##    Length     Class      Mode \n##         3 character character\nprint(df[, 1])\n## [1] \"Ayse\"  \"Jessy\" \"Chris\"\nprint(df[, \"Age\"])  # Access the 'Age' column\n## [1] 25 30 35\nAccessing Rows:\n\nUse bracket notation with a row index: df[row_number, ].\n\nprint(df[2, ])  # Access the first row\n##    Name Age Professor\n## 2 Jessy  30     FALSE\nAccessing Specific Elements:\n\nUse df[row, column].\n\nprint(df[2, 3])  # Access the element in the 2nd row and 3rd column\n## [1] FALSE\nprint(df[2, \"Professor\"])\n## [1] FALSE\n\n\n\n9.4 Adding and Removing Data\n Don‚Äôt forget [row, column]\nAdding Columns:\n\nUse the $ operator or bracket notation to add a new column.\n\ndf$Graduation &lt;- c(\"2021\", \"2026\", \"2019\")  # Add a new column 'Graduation'\nprint(df)\n##    Name Age Professor Graduation\n## 1  Ayse  25      TRUE       2021\n## 2 Jessy  30     FALSE       2026\n## 3 Chris  35      TRUE       2019\nAdding Rows:\n\nUse the rbind() function.\n\nnew_row &lt;- data.frame(Name = \"Donpeng\", Age = 28, Professor = NA, Graduation = \"2025\")\ndf &lt;- rbind(df, new_row)  # Add a new row\nprint(df)\n##      Name Age Professor Graduation\n## 1    Ayse  25      TRUE       2021\n## 2   Jessy  30     FALSE       2026\n## 3   Chris  35      TRUE       2019\n## 4 Donpeng  28        NA       2025\nRemoving Columns:\n\nUse the NULL assignment, - or subset the data frame.\n\ndf$Graduation &lt;- NULL  # Remove the 'Graduation' column\nprint(df)\n##      Name Age Professor\n## 1    Ayse  25      TRUE\n## 2   Jessy  30     FALSE\n## 3   Chris  35      TRUE\n## 4 Donpeng  28        NA\ndf2 &lt;- df[,-3]\nprint(df2)\n##      Name Age\n## 1    Ayse  25\n## 2   Jessy  30\n## 3   Chris  35\n## 4 Donpeng  28\nSame with rows, use -.\ndf3 &lt;- df[-2,]\nprint(df3)\n##      Name Age Professor\n## 1    Ayse  25      TRUE\n## 3   Chris  35      TRUE\n## 4 Donpeng  28        NA\n\n\n\n9.5 Filtering and Subsetting\nManipulating data is key to preparing it for analysis.\nCommon tasks:\n\nFiltering rows.\nSelecting columns.\nSorting data.\nAggregating data.\n\nFiltering Rows: Use logical conditions inside square brackets.\nTRY: Filter rows where Age &gt; 25\n# Filter rows where Age &gt; 25\ndf_filtered &lt;- df[df$Age &gt; 25, ]\nprint(df_filtered)\n##      Name Age Professor\n## 2   Jessy  30     FALSE\n## 3   Chris  35      TRUE\n## 4 Donpeng  28        NA\nSelecting Specific Columns: Use column indices or names.\nTRY: Select only ‚ÄòName‚Äô and ‚ÄòAge‚Äô columns\n# Select only 'Name' and 'Age' columns\ndf_subset &lt;- df[, c(\"Name\", \"Age\")]\nprint(df_subset)\n##      Name Age\n## 1    Ayse  25\n## 2   Jessy  30\n## 3   Chris  35\n## 4 Donpeng  28\ndf_subset &lt;- df[, c(1, 2)]\n\n\n\n9.6 Ordering Data\nRearrange rows based on column values.\nTRY: Sort by Age in ascending order\n# Sort by Age in ascending order\nsorted_df &lt;- df[order(df$Age), ]\n\n# Sort by Age in descending order\nsorted_df_desc &lt;- df[order(-df$Age), ]\nTRY: Sort rows by Name in alphabetical order.\n\n\n\n9.7 Common Data Frame Functions\n\nnrow(df): Returns the number of rows.\nncol(df): Returns the number of columns.\ncolnames(df): Returns column names.\nrownames(df): Returns row names.\nmerge(df1, df2): Combines two data frames by matching rows. We will learn to do this in Tidy next week\n\n\n\n\n9.8 Basic Aggregations\nCompute summary statistics (e.g., mean, sum) for groups of data.\nTRY:\n# Example: Calculate mean Age \nmean(df$Age)\n## [1] 29.5\nmax(df$Age)\n## [1] 35\nmin(df$Age)\n## [1] 25\nsum(df$Age)\n## [1] 118\nsd(df$Age)\n## [1] 4.203173\nTRY: Try with other functions such as max(), min(), sum(), and others\n\n\nClass Exercise\n\nCreate a Data Frame: Create a data frame of your favorite movies, including columns for Title, Year, and Rating.\nFilter Rows: Filter the data frame to show only movies released after 2010.\nAdd a New Column: Add a column indicating whether the movie has won an Oscar (TRUE or FALSE).\n\n\n\n\n\nLecture 1 Cheat Sheet\n\n\n\n\n\n\n\nTopic\nKey Points / Commands\n\n\n\n\nRStudio\nIntegrated Development Environment (IDE) for R. Main panes: Console, Source, Environment, Files/Plots/Packages/Help.\n\n\nConsole\nRuns R code line by line. Good for quick tests. Code is not saved unless written in a script or R Markdown file.\n\n\nTerminal\nSystem command line (not R). Used for OS-level commands (e.g., ls, dir). We will rarely use it.\n\n\nRStudio Projects\nOrganize files for one analysis. Automatically sets working directory. Always open work via the .Rproj file.\n\n\nProject Folder Structure\nRecommended folders: data/ (datasets), scripts/ (R scripts), .Rmd files for notes/analysis.\n\n\nScripts (.R)\nPlain text files for writing and saving R code. Run selected lines with Cmd/Ctrl + Enter.\n\n\nWorking Directory\nCheck with getwd(). Should point to your project folder. Enables relative paths like \"data/file.csv\".\n\n\nR Markdown (.Rmd)\nCombines text, code, and output in one reproducible document. Used for notes, assignments, and reports.\n\n\nRunning vs Knitting\nRun chunks to execute code interactively. Knit rebuilds the entire document from top to bottom in a clean session.\n\n\nCode Chunks\nR code lives inside chunks: {r} code . Output appears below the chunk.\n\n\nR Packages\nExtend R functionality. Install once with install.packages(), load every session with library().\n\n\nVariable Assignment\n&lt;- (preferred), = (allowed), -&gt; (rare). R is case-sensitive.\n\n\nComments\nUse # for comments. R only supports single-line comments.\n\n\nReserved Keywords\nCannot be used as variable names (e.g., if, for, TRUE, FALSE, NA, NULL, Inf).\n\n\nBasic Data Types\nNumeric, Character, Logical, Factor. Check with typeof().\n\n\nDates\nDate, POSIXct, POSIXlt. Use as.Date(), Sys.Date(), Sys.time().\n\n\nLubridate\nEasier date handling: ymd(), mdy(), year(), month(), day(), + days(), + months().\n\n\nBasic Arithmetic\n+, -, *, /, ^. Works on numbers and vectors.\n\n\nVectors\nOne-dimensional objects with same data type. Create with c().\n\n\nVector Indexing\nUse []. Select (x[1:3]), exclude (x[-2]), multiple indices (x[c(1,3)]).\n\n\nVector Functions\nlength(), sequences with :, recycling in vector arithmetic.\n\n\nType Coercion\nCombining types converts to one type: Character &gt; Numeric &gt; Logical.\n\n\nData Frames\nTable-like structure with columns of equal length. Create with data.frame().\n\n\nInspecting Data Frames\nhead(), tail(), str(), summary(), dim(), nrow(), ncol().\n\n\nAccessing Data\nColumns: df$col, df[, \"col\"]. Rows: df[row, ]. Elements: df[row, col].\n\n\nAdding/Removing Data\nAdd columns with $. Add rows with rbind(). Remove with NULL or negative indexing.\n\n\nFiltering & Subsetting\nUse logical conditions: df[df$Age &gt; 25, ]. Select columns by name or index.\n\n\nSorting Data\norder(df$col) for ascending, order(-df$col) for descending.\n\n\nBasic Aggregations\nmean(), max(), min(), sum(), sd() on vectors or data frame columns."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Dr.¬†Ayse D. Lokmanoglu Lecture 2, (B) Jan 28, (A) Feb 2\n\n\n\n\n\nSection\nTopic\n\n\n\n\n1\nData Import\n\n\n1.1\nUnderstanding File Paths\n\n\n1.2\nReading CSV Files with readr\n\n\n1.3\nReading Excel Files with readxl\n\n\n1.4\nLoading via RStudio Files Pane\n\n\n1.5\nColumn Specifications\n\n\n2\nTidy Data Principles\n\n\n2.1\nWhat is Tidy Data?\n\n\n2.2\nTibbles\n\n\n2.3\nThe Pipe Operator\n\n\n3\nDPLYR\n\n\n3.1\nCommon dplyr Functions Overview\n\n\n3.2\nfilter()\n\n\n3.3\nselect()\n\n\n3.4\nmutate()\n\n\n3.5\nsummarize()\n\n\n3.6\ngroup_by()\n\n\n3.7\narrange()\n\n\n3.8\nrename()\n\n\n4\nData Reshaping\n\n\n4.1\npivot_longer()\n\n\n4.2\npivot_wider()\n\n\n4.3\nseparate()\n\n\n4.4\nunite()\n\n\n5\nHandling Missing Values\n\n\n5.1\ndrop_na()\n\n\n5.2\nfill()\n\n\n5.3\nreplace_na()\n\n\n6\nExpanding Tables\n\n\n6.1\nexpand()\n\n\n6.2\ncomplete()\n\n\n7\nJoins in Tidyverse\n\n\n7.1\nleft_join()\n\n\n7.2\nright_join()\n\n\n7.3\ninner_join()\n\n\n7.4\nfull_join()\n\n\n7.5\nanti_join()\n\n\n8\nRegular Expressions\n\n\n8.1\nIntroduction to Regex\n\n\n8.2\nBasic Pattern Matching\n\n\n8.3\nstringr Functions\n\n\n8.4\nCommon Regex Patterns\n\n\n8.5\nPractical Examples\n\n\n\n\n\n\n\nOne of the first steps of any project is importing data into R. Data is often stored in tabular formats like CSV files, Excel spreadsheets, or databases.\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(readxl)\nlibrary(stringr)\n\n\n\nIn Lecture 1, we set up our project folder structure:\nEMS747_Project/\n  EMS747_Project.Rproj\n  data/\n  scripts/\n  bigdata_L2_github.Rmd\nWhen you open your project (by clicking the .Rproj file), R automatically sets your working directory to the project folder. This means you can use relative paths to access files.\n\n\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nAbsolute Path\nFull path from the root of your computer\n\"/Users/ayse/Documents/EMS747_Project/data/file.csv\"\n\n\nRelative Path\nPath relative to your working directory\n\"data/file.csv\"\n\n\n\nAlways use relative paths in your projects! They make your code portable and reproducible.\n\n\n\nThe ../ means ‚Äúgo up one folder level‚Äù (to the parent directory).\nExample project structure:\nEMS747_Project/\n  data/\n    Starbucks_User_Data.csv\n  scripts/\n    my_analysis.R          &lt;- If you're working here\n  bigdata_L2_github.Rmd    &lt;- Or working here\n\nIf your script is in the main project folder: use \"data/file.csv\"\nIf your script is in the scripts folder: use \"../data/file.csv\" (go up one level, then into data)\n\n# From the main project folder\nstarbucks_user_data &lt;- read_csv(\"data/Starbucks_User_Data.csv\")\n\n# From the scripts folder (need to go up one level first)\nstarbucks_user_data &lt;- read_csv(\"../data/Starbucks_User_Data.csv\")\nTRY: Check your working directory\ngetwd()  # This should show your project folder path\n## [1] \"/Users/alokman/Library/CloudStorage/GoogleDrive-alokman@bu.edu/My Drive/Teaching/S26_EMS747/ClassSlides\"\n\n\n\n\n\nThe readr package provides fast and friendly functions for reading rectangular data.\n\n\n\nFunction\nDescription\n\n\n\n\nread_csv()\nRead comma delimited files\n\n\nread_csv2()\nRead semicolon delimited files (European format)\n\n\nread_tsv()\nRead tab delimited files\n\n\nread_delim()\nRead files with any delimiter\n\n\n\nTRY: Load data from a URL\n# Load the data from the URL\nurl &lt;- \"https://raw.githubusercontent.com/aysedeniz09/Social_Media_Listening/refs/heads/main/MSC_social_media_list_data/Starbucks_User_Data.csv\"\nstarbucks_user_data &lt;- read_csv(url)\n\nhead(starbucks_user_data)\n## # A tibble: 6 √ó 16\n##   author_id conversation_id created_at          hashtag lang  like_count mention\n##       &lt;dbl&gt;           &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;  \n## 1     30973         1.61e18 2022-12-27 15:43:16 &lt;NA&gt;    en            10 &lt;NA&gt;   \n## 2     30973         1.60e18 2022-11-29 05:23:55 &lt;NA&gt;    en             9 Mo_sha‚Ä¶\n## 3     30973         1.59e18 2022-11-28 20:14:09 &lt;NA&gt;    en             2 Mixxed‚Ä¶\n## 4     30973         1.60e18 2022-11-28 12:51:28 &lt;NA&gt;    en             0 BihhKa‚Ä¶\n## 5     30973         1.60e18 2022-11-27 15:14:26 &lt;NA&gt;    en             0 BihhKa‚Ä¶\n## 6     30973         1.60e18 2022-11-24 17:47:24 &lt;NA&gt;    en             1 therea‚Ä¶\n## # ‚Ñπ 9 more variables: quote_count &lt;dbl&gt;, referenced_status_id &lt;dbl&gt;,\n## #   referenced_user_id &lt;dbl&gt;, reply_count &lt;dbl&gt;, retweet_count &lt;dbl&gt;,\n## #   row_id &lt;dbl&gt;, status_id &lt;dbl&gt;, text &lt;chr&gt;, type &lt;chr&gt;\nLoad from your computer\n# From the main project folder (data is a subfolder)\nstarbucks_user_data &lt;- read_csv(\"data/Starbucks_User_Data.csv\")\n\n# From a scripts folder (need to go up one level first)\nstarbucks_user_data &lt;- read_csv(\"../data/Starbucks_User_Data.csv\")\n\n\n\n\n\n\n\n\n\n\n\nArgument\nDescription\nExample\n\n\n\n\ncol_names\nUse first row as names or provide your own\ncol_names = FALSE or col_names = c(\"x\", \"y\", \"z\")\n\n\nskip\nNumber of lines to skip before reading\nskip = 1\n\n\nn_max\nMaximum number of rows to read\nn_max = 100\n\n\nna\nCharacter vector of strings to interpret as NA\nna = c(\"\", \"NA\", \"NULL\")\n\n\n\n# Skip header row and provide custom column names\nread_csv(\"data/file.csv\", col_names = c(\"x\", \"y\", \"z\"), skip = 1)\n\n# Read only first 100 rows\nread_csv(\"data/file.csv\", n_max = 100)\n\n# Treat \"NULL\" as missing values\nread_csv(\"data/file.csv\", na = c(\"\", \"NA\", \"NULL\"))\n\n\n\n\n\nThe readxl package reads both .xls and .xlsx files.\n# Install if needed\ninstall.packages(\"readxl\")\nlibrary(readxl)\n\n# Read an Excel file\ndata &lt;- read_excel(\"../data/Starbucks_User_Data.xlsx\")\n\n# Read a specific sheet by name or position\ndata &lt;- read_excel(\"../data/Starbucks_User_Data.xlsx\", sheet = \"Sheet2\")\ndata &lt;- read_excel(\"../data/Starbucks_User_Data.xlsx\", sheet = 2)\n\n# Get all sheet names\nexcel_sheets(\"../data/Starbucks_User_Data.xlsx\")\n\n# Read a specific range of cells\ndata &lt;- read_excel(\"../data/Starbucks_User_Data.xlsx\", range = \"B1:D10\")\n\n\n\n\nYou can also import data using RStudio‚Äôs point-and-click interface:\n\nClick on the Files pane\nNavigate to your data file\nClick Import Dataset\nConfigure import options\nClick Import\n\n \n\n\n\n\nColumn specifications define what data type each column will be imported as. By default, readr guesses column types based on the first 1000 rows.\nColumn Types:\n\n\n\nType\nFunction\nAbbreviation\n\n\n\n\nLogical\ncol_logical()\n‚Äúl‚Äù\n\n\nInteger\ncol_integer()\n‚Äúi‚Äù\n\n\nDouble\ncol_double()\n‚Äúd‚Äù\n\n\nCharacter\ncol_character()\n‚Äúc‚Äù\n\n\nFactor\ncol_factor()\n‚Äúf‚Äù\n\n\nDate\ncol_date()\n‚ÄúD‚Äù\n\n\nDateTime\ncol_datetime()\n‚ÄúT‚Äù\n\n\nSkip\ncol_skip()\n‚Äú-‚Äù or ‚Äú_‚Äù\n\n\n\n# Set specific column types\nread_csv(\"file.csv\", \n         col_types = list(\n           x = col_double(),\n           y = col_character(),\n           z = col_date()\n         ))\n\n# Use abbreviation string\nread_csv(\"file.csv\", col_types = \"dcD\")\n\n# Select specific columns to import\nread_csv(\"file.csv\", col_select = c(name, age, score))\n\n\n\nR comes with many built-in datasets for practice:\n# See all available datasets\ndata()\n\n# Load a built-in dataset\ndata(\"mtcars\")\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n\n\n\n\n\nTidy data is a consistent way to organize tabular data. A dataset is tidy if:\n\nEach variable is in its own column\nEach observation (case) is in its own row\nEach value is in its own cell\n\n\nImage from: Hassan, F. (2023, March 21). Tidy Data in Python. Medium.\nWhy Tidy Data?\n\nSimplifies data manipulation and visualization\nWorks seamlessly with tidyverse packages (dplyr, ggplot2, tidyr)\nMakes data analysis more reproducible\n\nTidy Cheat Sheet\n\n\n\n\nTibbles are a modern reimagining of data frames provided by the tibble package. They have improved behaviors:\n\nBetter printing (shows only first 10 rows and columns that fit on screen)\nNo partial matching when subsetting columns\nNever convert strings to factors automatically\nSubset with [] for a tibble, [[]] or $ for a vector\n\nlibrary(tibble)\n\n# Create a tibble by columns\nmy_tibble &lt;- tibble(\n  x = 1:3,\n  y = c(\"a\", \"b\", \"c\"),\n  z = c(TRUE, FALSE, TRUE)\n)\nmy_tibble\n## # A tibble: 3 √ó 3\n##       x y     z    \n##   &lt;int&gt; &lt;chr&gt; &lt;lgl&gt;\n## 1     1 a     TRUE \n## 2     2 b     FALSE\n## 3     3 c     TRUE\n# Create a tibble by rows (useful for small datasets)\nmy_tibble2 &lt;- tribble(\n  ~x, ~y, ~z,\n  1, \"a\", TRUE,\n  2, \"b\", FALSE,\n  3, \"c\", TRUE\n)\nmy_tibble2\n## # A tibble: 3 √ó 3\n##       x y     z    \n##   &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;\n## 1     1 a     TRUE \n## 2     2 b     FALSE\n## 3     3 c     TRUE\n# Convert data frame to tibble\nas_tibble(mtcars)\n## # A tibble: 32 √ó 11\n##      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n##  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n##  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n##  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n##  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n##  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n##  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n##  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n##  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n## 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n## # ‚Ñπ 22 more rows\n# Check if something is a tibble\nis_tibble(my_tibble)\n## [1] TRUE\nis_tibble(mtcars)\n## [1] FALSE\n\n\n\n\nThe pipe operator |&gt; (or %&gt;% from magrittr) allows you to chain operations together, making code more readable.\nShortcut: - Mac: Cmd + Shift + M - Windows: Ctrl + Shift + M\n# Without pipe (nested functions - hard to read)\nhead(arrange(filter(mtcars, mpg &gt; 20), desc(hp)), 3)\n##                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Lotus Europa  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Mazda RX4     21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n# With pipe (sequential - easy to read)\nmtcars |&gt; \n  filter(mpg &gt; 20) |&gt; \n  arrange(desc(hp)) |&gt; \n  head(3)\n##                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Lotus Europa  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Mazda RX4     21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nThe pipe takes the output of the left side and passes it as the first argument to the function on the right side.\n\n\n\n\n\nThe dplyr package provides a powerful toolkit for data manipulation with intuitive ‚Äúverb‚Äù functions.\nlibrary(dplyr)\nCreate a sample dataset:\ndf &lt;- data.frame(\n  Movie_Title = c(\n    \"YOLO\", \"Successor\", \"Pegasus 2\", \"Deadpool & Wolverine\", \"Moana 2\", \n    \"The Hidden Blade\", \"Avatar: The Spirit Returns\"\n  ),\n  Release_Date = c(\n    \"January 2024\", \"February 2024\", \"March 2024\", \"July 2024\", \n    \"November 2024\", \"April 2024\", \"December 2024\"\n  ),\n  China_Box_Office_Gross = c(\n    479597304, 469612890, 466930272, 450000000, 350000000, 320000000, 550000000\n  ),\n  US_Box_Office_Gross = c(\n    310000000, 280000000, 290000000, 438000000, 221000000, 75000000, 600000000\n  ),\n  Total_Worldwide_Gross = c(\n    479597304, 469612890, 466930272, 850000000, 421000000, 400000000, 1300000000\n  )\n)\n\nhead(df)\n##            Movie_Title  Release_Date China_Box_Office_Gross US_Box_Office_Gross\n## 1                 YOLO  January 2024              479597304            3.10e+08\n## 2            Successor February 2024              469612890            2.80e+08\n## 3            Pegasus 2    March 2024              466930272            2.90e+08\n## 4 Deadpool & Wolverine     July 2024              450000000            4.38e+08\n## 5              Moana 2 November 2024              350000000            2.21e+08\n## 6     The Hidden Blade    April 2024              320000000            7.50e+07\n##   Total_Worldwide_Gross\n## 1             479597304\n## 2             469612890\n## 3             466930272\n## 4             850000000\n## 5             421000000\n## 6             400000000\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nfilter()\nSelect rows based on conditions\n\n\nselect()\nChoose specific columns\n\n\nmutate()\nCreate or transform columns\n\n\nsummarize()\nCompute summary statistics\n\n\ngroup_by()\nGroup data by variables\n\n\narrange()\nSort rows\n\n\nrename()\nRename columns\n\n\n\n\n\n\n\nSelect rows based on specific conditions.\n# Filter movies with China gross &gt; 400 million\ndf |&gt; \n  filter(China_Box_Office_Gross &gt; 400000000)\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1                       YOLO  January 2024              479597304\n## 2                  Successor February 2024              469612890\n## 3                  Pegasus 2    March 2024              466930272\n## 4       Deadpool & Wolverine     July 2024              450000000\n## 5 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross Total_Worldwide_Gross\n## 1            3.10e+08             479597304\n## 2            2.80e+08             469612890\n## 3            2.90e+08             466930272\n## 4            4.38e+08             850000000\n## 5            6.00e+08            1300000000\n# Multiple conditions with AND (&)\nmtcars |&gt; \n  filter(mpg &gt; 20 & cyl == 6)\n##                 mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4      21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag  21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n# Multiple conditions with OR (|)\nmtcars |&gt; \n  filter(mpg &gt; 25 | hp &gt; 200)\n##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n\n\n\n\nChoose specific columns from a dataset.\n# Select specific columns by name\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross)\n##                  Movie_Title Total_Worldwide_Gross\n## 1                       YOLO             479597304\n## 2                  Successor             469612890\n## 3                  Pegasus 2             466930272\n## 4       Deadpool & Wolverine             850000000\n## 5                    Moana 2             421000000\n## 6           The Hidden Blade             400000000\n## 7 Avatar: The Spirit Returns            1300000000\n# Select a range of columns\ndf |&gt; \n  select(Movie_Title:US_Box_Office_Gross)\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1                       YOLO  January 2024              479597304\n## 2                  Successor February 2024              469612890\n## 3                  Pegasus 2    March 2024              466930272\n## 4       Deadpool & Wolverine     July 2024              450000000\n## 5                    Moana 2 November 2024              350000000\n## 6           The Hidden Blade    April 2024              320000000\n## 7 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross\n## 1            3.10e+08\n## 2            2.80e+08\n## 3            2.90e+08\n## 4            4.38e+08\n## 5            2.21e+08\n## 6            7.50e+07\n## 7            6.00e+08\n# Exclude columns with minus sign\ndf |&gt; \n  select(-Release_Date)\n##                  Movie_Title China_Box_Office_Gross US_Box_Office_Gross\n## 1                       YOLO              479597304            3.10e+08\n## 2                  Successor              469612890            2.80e+08\n## 3                  Pegasus 2              466930272            2.90e+08\n## 4       Deadpool & Wolverine              450000000            4.38e+08\n## 5                    Moana 2              350000000            2.21e+08\n## 6           The Hidden Blade              320000000            7.50e+07\n## 7 Avatar: The Spirit Returns              550000000            6.00e+08\n##   Total_Worldwide_Gross\n## 1             479597304\n## 2             469612890\n## 3             466930272\n## 4             850000000\n## 5             421000000\n## 6             400000000\n## 7            1300000000\nHelper functions for select():\n\n\n\nHelper\nDescription\n\n\n\n\nstarts_with(\"x\")\nColumns starting with ‚Äúx‚Äù\n\n\nends_with(\"x\")\nColumns ending with ‚Äúx‚Äù\n\n\ncontains(\"x\")\nColumns containing ‚Äúx‚Äù\n\n\neverything()\nAll columns\n\n\n\n# Select columns containing \"Gross\"\ndf |&gt; \n  select(Movie_Title, contains(\"Gross\"))\n##                  Movie_Title China_Box_Office_Gross US_Box_Office_Gross\n## 1                       YOLO              479597304            3.10e+08\n## 2                  Successor              469612890            2.80e+08\n## 3                  Pegasus 2              466930272            2.90e+08\n## 4       Deadpool & Wolverine              450000000            4.38e+08\n## 5                    Moana 2              350000000            2.21e+08\n## 6           The Hidden Blade              320000000            7.50e+07\n## 7 Avatar: The Spirit Returns              550000000            6.00e+08\n##   Total_Worldwide_Gross\n## 1             479597304\n## 2             469612890\n## 3             466930272\n## 4             850000000\n## 5             421000000\n## 6             400000000\n## 7            1300000000\n\n\n\n\nCreate new columns or transform existing ones.\n# Create a new column\ndf &lt;- df |&gt; \n  mutate(Profit = Total_Worldwide_Gross - US_Box_Office_Gross)\n\n# Multiple new columns at once\ndf |&gt; \n  mutate(\n    Profit = Total_Worldwide_Gross - US_Box_Office_Gross,\n    China_Pct = China_Box_Office_Gross / Total_Worldwide_Gross * 100\n  ) |&gt; \n  select(Movie_Title, Profit, China_Pct)\n##                  Movie_Title    Profit China_Pct\n## 1                       YOLO 169597304 100.00000\n## 2                  Successor 189612890 100.00000\n## 3                  Pegasus 2 176930272 100.00000\n## 4       Deadpool & Wolverine 412000000  52.94118\n## 5                    Moana 2 200000000  83.13539\n## 6           The Hidden Blade 325000000  80.00000\n## 7 Avatar: The Spirit Returns 700000000  42.30769\n\n\n\n\nCompute summary statistics. Often used with group_by().\n# Overall summary\ndf |&gt; \n  summarize(\n    Total_China = sum(China_Box_Office_Gross),\n    Avg_Worldwide = mean(Total_Worldwide_Gross),\n    Count = n()\n  )\n##   Total_China Avg_Worldwide Count\n## 1  3086140466     626734352     7\n# Summary with mtcars\nmtcars |&gt; \n  summarize(\n    mean_mpg = mean(mpg),\n    sd_mpg = sd(mpg),\n    median_hp = median(hp)\n  )\n##   mean_mpg   sd_mpg median_hp\n## 1 20.09062 6.026948       123\n\n\n\n\nGroup data by one or more variables for grouped operations.\n# Group by cylinders and summarize\nmtcars |&gt; \n  group_by(cyl) |&gt; \n  summarize(\n    mean_mpg = mean(mpg),\n    count = n()\n  )\n## # A tibble: 3 √ó 3\n##     cyl mean_mpg count\n##   &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n## 1     4     26.7    11\n## 2     6     19.7     7\n## 3     8     15.1    14\n# Group by multiple variables\nmtcars |&gt; \n  group_by(cyl, gear) |&gt; \n  summarize(\n    mean_mpg = mean(mpg),\n    count = n(),\n    .groups = \"drop\"  # Ungroup after summarizing\n  )\n## # A tibble: 8 √ó 4\n##     cyl  gear mean_mpg count\n##   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n## 1     4     3     21.5     1\n## 2     4     4     26.9     8\n## 3     4     5     28.2     2\n## 4     6     3     19.8     2\n## 5     6     4     19.8     4\n## 6     6     5     19.7     1\n## 7     8     3     15.0    12\n## 8     8     5     15.4     2\n\n\n\n\nSort rows by one or more variables.\n# Sort ascending (default)\ndf |&gt; \n  arrange(Total_Worldwide_Gross)\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1           The Hidden Blade    April 2024              320000000\n## 2                    Moana 2 November 2024              350000000\n## 3                  Pegasus 2    March 2024              466930272\n## 4                  Successor February 2024              469612890\n## 5                       YOLO  January 2024              479597304\n## 6       Deadpool & Wolverine     July 2024              450000000\n## 7 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross Total_Worldwide_Gross    Profit\n## 1            7.50e+07             400000000 325000000\n## 2            2.21e+08             421000000 200000000\n## 3            2.90e+08             466930272 176930272\n## 4            2.80e+08             469612890 189612890\n## 5            3.10e+08             479597304 169597304\n## 6            4.38e+08             850000000 412000000\n## 7            6.00e+08            1300000000 700000000\n# Sort descending\ndf |&gt; \n  arrange(desc(Total_Worldwide_Gross))\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1 Avatar: The Spirit Returns December 2024              550000000\n## 2       Deadpool & Wolverine     July 2024              450000000\n## 3                       YOLO  January 2024              479597304\n## 4                  Successor February 2024              469612890\n## 5                  Pegasus 2    March 2024              466930272\n## 6                    Moana 2 November 2024              350000000\n## 7           The Hidden Blade    April 2024              320000000\n##   US_Box_Office_Gross Total_Worldwide_Gross    Profit\n## 1            6.00e+08            1300000000 700000000\n## 2            4.38e+08             850000000 412000000\n## 3            3.10e+08             479597304 169597304\n## 4            2.80e+08             469612890 189612890\n## 5            2.90e+08             466930272 176930272\n## 6            2.21e+08             421000000 200000000\n## 7            7.50e+07             400000000 325000000\n# Sort by multiple columns\nmtcars |&gt; \n  arrange(cyl, desc(mpg)) |&gt; \n  head()\n##                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Toyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Fiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Lotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Fiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n\n\n\n\nRename columns. Syntax: rename(new_name = old_name)\ndf_renamed &lt;- df |&gt; \n  rename(\n    China_Gross = China_Box_Office_Gross,\n    US_Gross = US_Box_Office_Gross,\n    Worldwide_Gross = Total_Worldwide_Gross\n  )\n\nnames(df_renamed)\n## [1] \"Movie_Title\"     \"Release_Date\"    \"China_Gross\"     \"US_Gross\"       \n## [5] \"Worldwide_Gross\" \"Profit\"\n\n\n\nUse the movies dataset and perform the following:\n\nfilter(): Select movies with a total worldwide gross greater than $500M.\nselect(): Choose only Movie_Title and columns containing ‚ÄúGross‚Äù.\nmutate(): Add a new column for US percentage of worldwide gross.\nsummarize(): Compute the total and average gross for China.\ngroup_by() + summarize(): Calculate average gross by release month (hint: you‚Äôll need to extract month first).\narrange(): Sort movies by worldwide gross in descending order.\n\n### Your workspace\n\n\n\n\n\n\nData often needs to be reshaped between ‚Äúwide‚Äù and ‚Äúlong‚Äù formats for different analyses.\n\n\n\n\n\nTransform data from wide to long format by collapsing multiple columns into two: one for names and one for values.\nKey Arguments: - cols: Columns to pivot - names_to: Name for the new column holding original column names - values_to: Name for the new column holding values\n# Convert movie data to long format\nlong_df &lt;- df |&gt; \n  pivot_longer(\n    cols = China_Box_Office_Gross:Total_Worldwide_Gross,\n    names_to = \"Box_Office_Type\",\n    values_to = \"Gross\"\n  )\n\nprint(long_df)\n## # A tibble: 21 √ó 5\n##    Movie_Title          Release_Date     Profit Box_Office_Type            Gross\n##    &lt;chr&gt;                &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt;\n##  1 YOLO                 January 2024  169597304 China_Box_Office_Gross 479597304\n##  2 YOLO                 January 2024  169597304 US_Box_Office_Gross    310000000\n##  3 YOLO                 January 2024  169597304 Total_Worldwide_Gross  479597304\n##  4 Successor            February 2024 189612890 China_Box_Office_Gross 469612890\n##  5 Successor            February 2024 189612890 US_Box_Office_Gross    280000000\n##  6 Successor            February 2024 189612890 Total_Worldwide_Gross  469612890\n##  7 Pegasus 2            March 2024    176930272 China_Box_Office_Gross 466930272\n##  8 Pegasus 2            March 2024    176930272 US_Box_Office_Gross    290000000\n##  9 Pegasus 2            March 2024    176930272 Total_Worldwide_Gross  466930272\n## 10 Deadpool & Wolverine July 2024     412000000 China_Box_Office_Gross 450000000\n## # ‚Ñπ 11 more rows\n# Another example with mtcars\nmtcars_long &lt;- mtcars |&gt; \n  rownames_to_column(\"car\") |&gt; \n  pivot_longer(\n    cols = mpg:carb,\n    names_to = \"metric\",\n    values_to = \"value\"\n  )\n\nhead(mtcars_long)\n## # A tibble: 6 √ó 3\n##   car       metric  value\n##   &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;\n## 1 Mazda RX4 mpg     21   \n## 2 Mazda RX4 cyl      6   \n## 3 Mazda RX4 disp   160   \n## 4 Mazda RX4 hp     110   \n## 5 Mazda RX4 drat     3.9 \n## 6 Mazda RX4 wt       2.62\n\n\n\n\nTransform data from long to wide format by spreading values across multiple columns.\nKey Arguments: - names_from: Column whose values become new column names - values_from: Column whose values fill the new columns\n# Convert back to wide format\nwide_df &lt;- long_df |&gt; \n  pivot_wider(\n    names_from = Box_Office_Type,\n    values_from = Gross\n  )\n\nprint(wide_df)\n## # A tibble: 7 √ó 6\n##   Movie_Title     Release_Date Profit China_Box_Office_Gross US_Box_Office_Gross\n##   &lt;chr&gt;           &lt;chr&gt;         &lt;dbl&gt;                  &lt;dbl&gt;               &lt;dbl&gt;\n## 1 YOLO            January 2024 1.70e8              479597304           310000000\n## 2 Successor       February 20‚Ä¶ 1.90e8              469612890           280000000\n## 3 Pegasus 2       March 2024   1.77e8              466930272           290000000\n## 4 Deadpool & Wol‚Ä¶ July 2024    4.12e8              450000000           438000000\n## 5 Moana 2         November 20‚Ä¶ 2   e8              350000000           221000000\n## 6 The Hidden Bla‚Ä¶ April 2024   3.25e8              320000000            75000000\n## 7 Avatar: The Sp‚Ä¶ December 20‚Ä¶ 7   e8              550000000           600000000\n## # ‚Ñπ 1 more variable: Total_Worldwide_Gross &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\n\nFeature\npivot_longer()\npivot_wider()\n\n\n\n\nDirection\nWide ‚Üí Long\nLong ‚Üí Wide\n\n\nPurpose\nConsolidate columns\nSpread values across columns\n\n\nKey Arguments\ncols, names_to, values_to\nnames_from, values_from\n\n\nTypical Use\nTidying data for analysis\nSummarizing for presentation\n\n\n\n\n\n\n\n\nSplit the contents of a single column into multiple columns.\n# Separate Release_Date into Month and Year\ndf_separated &lt;- df |&gt; \n  separate(Release_Date, into = c(\"Month\", \"Year\"), sep = \" \")\n\nprint(df_separated)\n##                  Movie_Title    Month Year China_Box_Office_Gross\n## 1                       YOLO  January 2024              479597304\n## 2                  Successor February 2024              469612890\n## 3                  Pegasus 2    March 2024              466930272\n## 4       Deadpool & Wolverine     July 2024              450000000\n## 5                    Moana 2 November 2024              350000000\n## 6           The Hidden Blade    April 2024              320000000\n## 7 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross Total_Worldwide_Gross    Profit\n## 1            3.10e+08             479597304 169597304\n## 2            2.80e+08             469612890 189612890\n## 3            2.90e+08             466930272 176930272\n## 4            4.38e+08             850000000 412000000\n## 5            2.21e+08             421000000 200000000\n## 6            7.50e+07             400000000 325000000\n## 7            6.00e+08            1300000000 700000000\nRelated functions: - separate_wider_delim(): Separate by delimiter into columns - separate_wider_position(): Separate by position into columns - separate_longer_delim(): Separate into rows instead of columns\n\n\n\n\nCombine multiple columns into a single column.\n# Combine Month and Year back into Release_Date\ndf_united &lt;- df_separated |&gt; \n  unite(\"Release_Date\", Month, Year, sep = \" \")\n\nprint(df_united)\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1                       YOLO  January 2024              479597304\n## 2                  Successor February 2024              469612890\n## 3                  Pegasus 2    March 2024              466930272\n## 4       Deadpool & Wolverine     July 2024              450000000\n## 5                    Moana 2 November 2024              350000000\n## 6           The Hidden Blade    April 2024              320000000\n## 7 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross Total_Worldwide_Gross    Profit\n## 1            3.10e+08             479597304 169597304\n## 2            2.80e+08             469612890 189612890\n## 3            2.90e+08             466930272 176930272\n## 4            4.38e+08             850000000 412000000\n## 5            2.21e+08             421000000 200000000\n## 6            7.50e+07             400000000 325000000\n## 7            6.00e+08            1300000000 700000000\n\n\n\n\n\nMissing values (NA) are common in real-world data. The tidyr package provides functions to handle them.\n# Create sample data with missing values\nmissing_df &lt;- tibble(\n  x1 = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n  x2 = c(1, NA, NA, 3, NA)\n)\nmissing_df\n## # A tibble: 5 √ó 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B        NA\n## 3 C        NA\n## 4 D         3\n## 5 E        NA\n\n\n\nRemove rows containing NA values.\n# Drop rows with NA in any column\nmissing_df |&gt; \n  drop_na()\n## # A tibble: 2 √ó 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 D         3\n# Drop rows with NA only in specific columns\nmissing_df |&gt; \n  drop_na(x2)\n## # A tibble: 2 √ó 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 D         3\n\n\n\n\nFill in NA values using the previous or next value.\n# Fill down (default)\nmissing_df |&gt; \n  fill(x2, .direction = \"down\")\n## # A tibble: 5 √ó 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         1\n## 3 C         1\n## 4 D         3\n## 5 E         3\n# Fill up\nmissing_df |&gt; \n  fill(x2, .direction = \"up\")\n## # A tibble: 5 √ó 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         3\n## 3 C         3\n## 4 D         3\n## 5 E        NA\n# Fill in both directions (down first, then up)\nmissing_df |&gt; \n  fill(x2, .direction = \"downup\")\n## # A tibble: 5 √ó 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         1\n## 3 C         1\n## 4 D         3\n## 5 E         3\n\n\n\n\nReplace NA values with a specified value.\n# Replace NA with a specific value\nmissing_df |&gt; \n  replace_na(list(x2 = 0))\n## # A tibble: 5 √ó 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         0\n## 3 C         0\n## 4 D         3\n## 5 E         0\n# Replace NA with the mean (requires mutate)\nmissing_df |&gt; \n  mutate(x2 = replace_na(x2, mean(x2, na.rm = TRUE)))\n## # A tibble: 5 √ó 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         2\n## 3 C         2\n## 4 D         3\n## 5 E         2\n\n\n\n\n\nCreate new combinations of variables or identify implicit missing values.\n\n\n\nCreate a tibble with all possible combinations of specified variables.\n# All combinations of cyl and gear\nmtcars |&gt; \n  expand(cyl, gear)\n## # A tibble: 9 √ó 2\n##     cyl  gear\n##   &lt;dbl&gt; &lt;dbl&gt;\n## 1     4     3\n## 2     4     4\n## 3     4     5\n## 4     6     3\n## 5     6     4\n## 6     6     5\n## 7     8     3\n## 8     8     4\n## 9     8     5\n\n\n\n\nAdd missing combinations of values to a dataset, filling other variables with NA.\n# Sample data with implicit missing combinations\nsales_df &lt;- tibble(\n  store = c(\"A\", \"A\", \"B\"),\n  product = c(\"X\", \"Y\", \"X\"),\n  sales = c(100, 150, 200)\n)\nsales_df\n## # A tibble: 3 √ó 3\n##   store product sales\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 A     X         100\n## 2 A     Y         150\n## 3 B     X         200\n# Complete with all combinations\nsales_df |&gt; \n  complete(store, product)\n## # A tibble: 4 √ó 3\n##   store product sales\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 A     X         100\n## 2 A     Y         150\n## 3 B     X         200\n## 4 B     Y          NA\n# Complete and fill missing values with 0\nsales_df |&gt; \n  complete(store, product, fill = list(sales = 0))\n## # A tibble: 4 √ó 3\n##   store product sales\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 A     X         100\n## 2 A     Y         150\n## 3 B     X         200\n## 4 B     Y           0\n\n\n\n\n\nJoins combine two datasets based on a common key (column).\n\n\n\nJoin Type\nDescription\n\n\n\n\nleft_join()\nKeep all rows from the left dataset\n\n\nright_join()\nKeep all rows from the right dataset\n\n\ninner_join()\nKeep only rows that match in both datasets\n\n\nfull_join()\nKeep all rows from both datasets\n\n\nanti_join()\nKeep rows from left that don‚Äôt match right\n\n\n\n\n\nCreate datasets for joining:\n# Directors dataset\ndf_directors &lt;- tibble(\n  Movie_Title = c(\"YOLO\", \"Successor\", \"Deadpool & Wolverine\", \"Moana 2\"),\n  Director = c(\"Jia Ling\", \"Xu Zheng\", \"Shawn Levy\", \"David Derrick Jr.\")\n)\n\n# Ratings dataset  \ndf_ratings &lt;- tibble(\n  Movie_Title = c(\"YOLO\", \"Pegasus 2\", \"Moana 2\", \"Wicked\"),\n  Rating = c(8.1, 7.5, 7.8, 8.0)\n)\n\n\n\nKeep all rows from the left dataset, add matching data from the right.\n# Add directors to movies (keep all movies)\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross) |&gt; \n  left_join(df_directors, by = \"Movie_Title\")\n##                  Movie_Title Total_Worldwide_Gross          Director\n## 1                       YOLO             479597304          Jia Ling\n## 2                  Successor             469612890          Xu Zheng\n## 3                  Pegasus 2             466930272              &lt;NA&gt;\n## 4       Deadpool & Wolverine             850000000        Shawn Levy\n## 5                    Moana 2             421000000 David Derrick Jr.\n## 6           The Hidden Blade             400000000              &lt;NA&gt;\n## 7 Avatar: The Spirit Returns            1300000000              &lt;NA&gt;\n\n\n\n\nKeep all rows from the right dataset, add matching data from the left.\n# Keep all directors, add movie data\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross) |&gt; \n  right_join(df_directors, by = \"Movie_Title\")\n##            Movie_Title Total_Worldwide_Gross          Director\n## 1                 YOLO             479597304          Jia Ling\n## 2            Successor             469612890          Xu Zheng\n## 3 Deadpool & Wolverine             850000000        Shawn Levy\n## 4              Moana 2             421000000 David Derrick Jr.\n\n\n\n\nKeep only rows that have matches in both datasets.\n# Only movies that have both gross data AND ratings\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross) |&gt; \n  inner_join(df_ratings, by = \"Movie_Title\")\n##   Movie_Title Total_Worldwide_Gross Rating\n## 1        YOLO             479597304    8.1\n## 2   Pegasus 2             466930272    7.5\n## 3     Moana 2             421000000    7.8\n\n\n\n\nKeep all rows from both datasets, filling with NA where there‚Äôs no match.\n# Combine all movies and ratings\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross) |&gt; \n  full_join(df_ratings, by = \"Movie_Title\")\n##                  Movie_Title Total_Worldwide_Gross Rating\n## 1                       YOLO             479597304    8.1\n## 2                  Successor             469612890     NA\n## 3                  Pegasus 2             466930272    7.5\n## 4       Deadpool & Wolverine             850000000     NA\n## 5                    Moana 2             421000000    7.8\n## 6           The Hidden Blade             400000000     NA\n## 7 Avatar: The Spirit Returns            1300000000     NA\n## 8                     Wicked                    NA    8.0\n\n\n\n\nReturn rows from the left dataset that do NOT have a match in the right.\n# Movies WITHOUT directors\ndf |&gt; \n  select(Movie_Title) |&gt; \n  anti_join(df_directors, by = \"Movie_Title\")\n##                  Movie_Title\n## 1                  Pegasus 2\n## 2           The Hidden Blade\n## 3 Avatar: The Spirit Returns\n# Movies WITHOUT ratings\ndf |&gt; \n  select(Movie_Title) |&gt; \n  anti_join(df_ratings, by = \"Movie_Title\")\n##                  Movie_Title\n## 1                  Successor\n## 2       Deadpool & Wolverine\n## 3           The Hidden Blade\n## 4 Avatar: The Spirit Returns\n\n\n\nUse the provided datasets (df, df_directors, df_ratings):\n\nleft_join(): Merge directors into the main movie dataset.\ninner_join(): Find movies that have both director and rating information.\nanti_join(): Identify movies without ratings.\nfull_join(): Create a comprehensive dataset with all movies, directors, and ratings.\n\n### Your workspace\n\n\n\n\n\n\nRegular expressions (regex) are powerful patterns used to match, search, and manipulate text. They are essential for data cleaning and text processing.\nWe will use the stringr package, which is part of the tidyverse.\nlibrary(stringr)\n\n\n\nA regular expression is a sequence of characters that defines a search pattern. Think of it as a sophisticated ‚Äúfind and replace‚Äù tool.\nWhy use regex?\n\nClean messy text data (remove special characters, standardize formats)\nExtract specific patterns (emails, phone numbers, dates)\nFilter rows based on text patterns\nTransform text data\n\n\n\n\n\n\n\nThe simplest regex matches exact text:\nfruits &lt;- c(\"apple\", \"banana\", \"pineapple\", \"grape\", \"grapefruit\")\n\n# Find fruits containing \"apple\"\nstr_detect(fruits, \"apple\")\n## [1]  TRUE FALSE  TRUE FALSE FALSE\n# Extract matches\nstr_subset(fruits, \"apple\")\n## [1] \"apple\"     \"pineapple\"\n\n\n\n\nThese characters have special meanings in regex:\n\n\n\n\n\n\n\n\n\nCharacter\nMeaning\nExample\nMatches\n\n\n\n\n.\nAny single character\n\"a.c\"\n‚Äúabc‚Äù, ‚Äúa1c‚Äù, ‚Äúa c‚Äù\n\n\n^\nStart of string\n\"^The\"\n‚ÄúThe dog‚Äù but not ‚ÄúSee The dog‚Äù\n\n\n$\nEnd of string\n\"end$\"\n‚Äúthe end‚Äù but not ‚Äúendless‚Äù\n\n\n*\nZero or more of previous\n\"ab*c\"\n‚Äúac‚Äù, ‚Äúabc‚Äù, ‚Äúabbc‚Äù\n\n\n+\nOne or more of previous\n\"ab+c\"\n‚Äúabc‚Äù, ‚Äúabbc‚Äù but not ‚Äúac‚Äù\n\n\n?\nZero or one of previous\n\"colou?r\"\n‚Äúcolor‚Äù, ‚Äúcolour‚Äù\n\n\n\\\\\nEscape special character\n\"\\\\.\"\nLiteral period ‚Äú.‚Äù\n\n\n\ntext &lt;- c(\"cat\", \"car\", \"card\", \"care\", \"scar\")\n\n# Match \"ca\" followed by any character\nstr_subset(text, \"ca.\")\n## [1] \"cat\"  \"car\"  \"card\" \"care\" \"scar\"\n# Match words starting with \"ca\"\nstr_subset(text, \"^ca\")\n## [1] \"cat\"  \"car\"  \"card\" \"care\"\n# Match words ending with \"r\"\nstr_subset(text, \"r$\")\n## [1] \"car\"  \"scar\"\n\n\n\n\nUse square brackets [] to match any character in a set:\n\n\n\n\n\n\n\n\nPattern\nMeaning\nExample\n\n\n\n\n[abc]\nMatch a, b, or c\n\"[aeiou]\" matches vowels\n\n\n[a-z]\nMatch any lowercase letter\n\n\n\n[A-Z]\nMatch any uppercase letter\n\n\n\n[0-9]\nMatch any digit\nSame as \\\\d\n\n\n[^abc]\nMatch anything EXCEPT a, b, c\n[^0-9] matches non-digits\n\n\n\nwords &lt;- c(\"hello\", \"HELLO\", \"Hello\", \"h3llo\", \"12345\")\n\n# Match words with lowercase letters\nstr_subset(words, \"[a-z]\")\n## [1] \"hello\" \"Hello\" \"h3llo\"\n# Match words with digits\nstr_subset(words, \"[0-9]\")\n## [1] \"h3llo\" \"12345\"\n# Match words starting with uppercase\nstr_subset(words, \"^[A-Z]\")\n## [1] \"HELLO\" \"Hello\"\n\n\n\n\n\n\n\nShorthand\nMeaning\nEquivalent\n\n\n\n\n\\\\d\nAny digit\n[0-9]\n\n\n\\\\D\nAny non-digit\n[^0-9]\n\n\n\\\\w\nAny word character\n[a-zA-Z0-9_]\n\n\n\\\\W\nAny non-word character\n[^a-zA-Z0-9_]\n\n\n\\\\s\nAny whitespace\nSpace, tab, newline\n\n\n\\\\S\nAny non-whitespace\n\n\n\n\nmixed &lt;- c(\"abc123\", \"hello world\", \"test_case\", \"no-hyphens\")\n\n# Find strings with digits\nstr_subset(mixed, \"\\\\d\")\n## [1] \"abc123\"\n# Find strings with whitespace\nstr_subset(mixed, \"\\\\s\")\n## [1] \"hello world\"\n# Find strings with word characters only (no spaces or hyphens)\nstr_detect(mixed, \"^\\\\w+$\")\n## [1]  TRUE FALSE  TRUE FALSE\n\n\n\n\n\nThe stringr package provides consistent, easy-to-use functions for working with strings and regex.\n\n\n\n\n\n\n\n\nFunction\nPurpose\nExample\n\n\n\n\nstr_detect()\nDoes pattern exist? (returns TRUE/FALSE)\nstr_detect(x, \"pattern\")\n\n\nstr_subset()\nReturn elements that match\nstr_subset(x, \"pattern\")\n\n\nstr_extract()\nExtract first match\nstr_extract(x, \"pattern\")\n\n\nstr_extract_all()\nExtract all matches\nstr_extract_all(x, \"pattern\")\n\n\nstr_replace()\nReplace first match\nstr_replace(x, \"pattern\", \"replacement\")\n\n\nstr_replace_all()\nReplace all matches\nstr_replace_all(x, \"pattern\", \"replacement\")\n\n\nstr_match()\nExtract groups from first match\nstr_match(x, \"pattern\")\n\n\nstr_count()\nCount matches\nstr_count(x, \"pattern\")\n\n\nstr_split()\nSplit string by pattern\nstr_split(x, \"pattern\")\n\n\n\n\n\n\nReturns TRUE or FALSE for each element. Great for filtering with dplyr::filter().\nemails &lt;- c(\"user@gmail.com\", \"test@yahoo.com\", \"invalid-email\", \"admin@bu.edu\")\n\n# Check which are valid emails (simple check)\nstr_detect(emails, \"@\")\n## [1]  TRUE  TRUE FALSE  TRUE\n# Use with filter\ndata.frame(email = emails) |&gt;\n  filter(str_detect(email, \"\\\\.edu$\"))\n##          email\n## 1 admin@bu.edu\n\n\n\n\nPulls out the first matching pattern from each string.\nsentences &lt;- c(\"Call me at 555-1234\", \"My number is 555-5678\", \"No phone here\")\n\n# Extract phone numbers\nstr_extract(sentences, \"\\\\d{3}-\\\\d{4}\")\n## [1] \"555-1234\" \"555-5678\" NA\n# Extract first word\nstr_extract(sentences, \"^\\\\w+\")\n## [1] \"Call\" \"My\"   \"No\"\n\n\n\n\nmessy_text &lt;- c(\"Hello   World\", \"Too    many   spaces\", \"Normal text\")\n\n# Replace multiple spaces with single space\nstr_replace_all(messy_text, \"\\\\s+\", \" \")\n## [1] \"Hello World\"     \"Too many spaces\" \"Normal text\"\n# Remove all digits\nstr_replace_all(\"Phone: 555-1234\", \"\\\\d\", \"X\")\n## [1] \"Phone: XXX-XXXX\"\n\n\n\n\nUse parentheses () to create capture groups. str_match() returns a matrix with the full match and each group.\ndates &lt;- c(\"2024-01-15\", \"2023-12-25\", \"2025-06-30\")\n\n# Extract year, month, day separately\nstr_match(dates, \"(\\\\d{4})-(\\\\d{2})-(\\\\d{2})\")\n##      [,1]         [,2]   [,3] [,4]\n## [1,] \"2024-01-15\" \"2024\" \"01\" \"15\"\n## [2,] \"2023-12-25\" \"2023\" \"12\" \"25\"\n## [3,] \"2025-06-30\" \"2025\" \"06\" \"30\"\n\n\n\n\n\nHere are some useful patterns for common data cleaning tasks:\n\n\n\nTask\nPattern\nExample\n\n\n\n\nEmail\n[\\\\w.-]+@[\\\\w.-]+\\\\.\\\\w+\nuser@example.com\n\n\nPhone (US)\n\\\\d{3}[-.\\\\s]?\\\\d{3}[-.\\\\s]?\\\\d{4}\n555-123-4567\n\n\nURL\nhttps?://[\\\\w./]+\nhttps://example.com\n\n\nTwitter handle\n@\\\\w+\n@username\n\n\nHashtag\n#\\\\w+\n#DataScience\n\n\nDate (YYYY-MM-DD)\n\\\\d{4}-\\\\d{2}-\\\\d{2}\n2024-01-15\n\n\nTime (HH:MM)\n\\\\d{2}:\\\\d{2}\n14:30\n\n\n\n\n\n\n\n\n\ntweets &lt;- c(\n  \"@user1 Check out this link https://t.co/abc123 #DataScience\",\n  \"Hello @user2! Great post! #RStats #coding\",\n  \"Just a regular tweet with no mentions\"\n)\n\n# Extract mentions (@username)\nstr_extract_all(tweets, \"@\\\\w+\")\n## [[1]]\n## [1] \"@user1\"\n## \n## [[2]]\n## [1] \"@user2\"\n## \n## [[3]]\n## character(0)\n# Extract hashtags\nstr_extract_all(tweets, \"#\\\\w+\")\n## [[1]]\n## [1] \"#DataScience\"\n## \n## [[2]]\n## [1] \"#RStats\" \"#coding\"\n## \n## [[3]]\n## character(0)\n# Remove URLs\nstr_replace_all(tweets, \"https?://\\\\S+\", \"[URL]\")\n## [1] \"@user1 Check out this link [URL] #DataScience\"\n## [2] \"Hello @user2! Great post! #RStats #coding\"    \n## [3] \"Just a regular tweet with no mentions\"\n\n\n\n\n# Sample data\ncustomer_data &lt;- data.frame(\n  info = c(\n    \"John Smith, Email: john@gmail.com, Phone: 555-1234\",\n    \"Jane Doe, Email: jane@yahoo.com, Phone: 555-5678\",\n    \"Bob Wilson, Email: bob@bu.edu, Phone: 555-9999\"\n  )\n)\n\n# Extract emails and phones into new columns\ncustomer_data |&gt;\n  mutate(\n    email = str_extract(info, \"[\\\\w.-]+@[\\\\w.-]+\\\\.\\\\w+\"),\n    phone = str_extract(info, \"\\\\d{3}-\\\\d{4}\"),\n    name = str_extract(info, \"^[A-Za-z]+ [A-Za-z]+\")\n  )\n##                                                 info          email    phone\n## 1 John Smith, Email: john@gmail.com, Phone: 555-1234 john@gmail.com 555-1234\n## 2   Jane Doe, Email: jane@yahoo.com, Phone: 555-5678 jane@yahoo.com 555-5678\n## 3     Bob Wilson, Email: bob@bu.edu, Phone: 555-9999     bob@bu.edu 555-9999\n##         name\n## 1 John Smith\n## 2   Jane Doe\n## 3 Bob Wilson\n\n\n\n\nlibrary(janeaustenr)\n\n# Get all Jane Austen books\nbooks &lt;- austen_books()\n\n# Find lines mentioning \"Mr.\" followed by a name\nbooks |&gt;\n  filter(str_detect(text, \"Mr\\\\.\\\\s[A-Z][a-z]+\")) |&gt;\n  mutate(mr_name = str_extract(text, \"Mr\\\\.\\\\s[A-Z][a-z]+\")) |&gt;\n  count(mr_name, sort = TRUE) |&gt;\n  head(10)\n\n\n\n\nUsing the Starbucks Twitter data:\n\nExtract all Twitter usernames (mentions starting with @) from the text column\nCount how many tweets contain hashtags\nFind tweets that mention ‚Äúcoffee‚Äù (case insensitive - hint: use (?i) or str_to_lower())\nExtract any URLs from the tweets\nCreate a new column with the text cleaned of URLs and mentions\n\n### Your workspace\n# Load the data if needed\nurl &lt;- \"https://raw.githubusercontent.com/aysedeniz09/Social_Media_Listening/refs/heads/main/MSC_social_media_list_data/Starbucks_User_Data.csv\"\nstarbucks &lt;- read_csv(url)\n\n# 1. Extract mentions\n\n\n# 2. Count hashtag tweets\n\n\n# 3. Find coffee tweets\n\n\n# 4. Extract URLs\n\n\n# 5. Clean text\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic\nKey Points\n\n\n\n\nFile Paths\nUse relative paths (e.g., \"data/file.csv\"). Use ../ to go up one directory level. Always use projects to set working directory automatically.\n\n\nData Import\nread_csv(), read_excel(), read_delim() for different file types. Use col_types to specify column types.\n\n\nTibbles\nModern data frames with better printing and subsetting. Create with tibble() or tribble(). Convert with as_tibble().\n\n\nPipe Operator\n|&gt; or %&gt;% chains operations together. Shortcut: Cmd/Ctrl + Shift + M.\n\n\nTidy Data\nEach variable in a column, each observation in a row, each value in a cell.\n\n\nDPLYR Functions\nfilter(): rows by condition; select(): columns; mutate(): create/transform; summarize(): aggregate; group_by(): group operations; arrange(): sort; rename(): rename columns.\n\n\nData Reshaping\npivot_longer(): wide ‚Üí long; pivot_wider(): long ‚Üí wide.\n\n\nSplit/Combine\nseparate(): split column into multiple; unite(): combine columns into one.\n\n\nMissing Values\ndrop_na(): remove NA rows; fill(): fill NA with adjacent values; replace_na(): replace NA with specific value.\n\n\nExpand Tables\nexpand(): all combinations; complete(): add missing combinations with NA.\n\n\nJoins\nleft_join(): keep all left; right_join(): keep all right; inner_join(): only matches; full_join(): keep all; anti_join(): non-matches.\n\n\nRegex Basics\n. any char; ^ start; $ end; * zero+; + one+; ? zero/one; [] character class; \\\\d digit; \\\\w word char; \\\\s whitespace.\n\n\nstringr Functions\nstr_detect(): check pattern; str_extract(): get match; str_replace(): replace match; str_match(): extract groups; str_subset(): filter by pattern."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#lecture-2-table-of-contents",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#lecture-2-table-of-contents",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Section\nTopic\n\n\n\n\n1\nData Import\n\n\n1.1\nUnderstanding File Paths\n\n\n1.2\nReading CSV Files with readr\n\n\n1.3\nReading Excel Files with readxl\n\n\n1.4\nLoading via RStudio Files Pane\n\n\n1.5\nColumn Specifications\n\n\n2\nTidy Data Principles\n\n\n2.1\nWhat is Tidy Data?\n\n\n2.2\nTibbles\n\n\n2.3\nThe Pipe Operator\n\n\n3\nDPLYR\n\n\n3.1\nCommon dplyr Functions Overview\n\n\n3.2\nfilter()\n\n\n3.3\nselect()\n\n\n3.4\nmutate()\n\n\n3.5\nsummarize()\n\n\n3.6\ngroup_by()\n\n\n3.7\narrange()\n\n\n3.8\nrename()\n\n\n4\nData Reshaping\n\n\n4.1\npivot_longer()\n\n\n4.2\npivot_wider()\n\n\n4.3\nseparate()\n\n\n4.4\nunite()\n\n\n5\nHandling Missing Values\n\n\n5.1\ndrop_na()\n\n\n5.2\nfill()\n\n\n5.3\nreplace_na()\n\n\n6\nExpanding Tables\n\n\n6.1\nexpand()\n\n\n6.2\ncomplete()\n\n\n7\nJoins in Tidyverse\n\n\n7.1\nleft_join()\n\n\n7.2\nright_join()\n\n\n7.3\ninner_join()\n\n\n7.4\nfull_join()\n\n\n7.5\nanti_join()\n\n\n8\nRegular Expressions\n\n\n8.1\nIntroduction to Regex\n\n\n8.2\nBasic Pattern Matching\n\n\n8.3\nstringr Functions\n\n\n8.4\nCommon Regex Patterns\n\n\n8.5\nPractical Examples"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#data-import",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#data-import",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "One of the first steps of any project is importing data into R. Data is often stored in tabular formats like CSV files, Excel spreadsheets, or databases.\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(readxl)\nlibrary(stringr)\n\n\n\nIn Lecture 1, we set up our project folder structure:\nEMS747_Project/\n  EMS747_Project.Rproj\n  data/\n  scripts/\n  bigdata_L2_github.Rmd\nWhen you open your project (by clicking the .Rproj file), R automatically sets your working directory to the project folder. This means you can use relative paths to access files.\n\n\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nAbsolute Path\nFull path from the root of your computer\n\"/Users/ayse/Documents/EMS747_Project/data/file.csv\"\n\n\nRelative Path\nPath relative to your working directory\n\"data/file.csv\"\n\n\n\nAlways use relative paths in your projects! They make your code portable and reproducible.\n\n\n\nThe ../ means ‚Äúgo up one folder level‚Äù (to the parent directory).\nExample project structure:\nEMS747_Project/\n  data/\n    Starbucks_User_Data.csv\n  scripts/\n    my_analysis.R          &lt;- If you're working here\n  bigdata_L2_github.Rmd    &lt;- Or working here\n\nIf your script is in the main project folder: use \"data/file.csv\"\nIf your script is in the scripts folder: use \"../data/file.csv\" (go up one level, then into data)\n\n# From the main project folder\nstarbucks_user_data &lt;- read_csv(\"data/Starbucks_User_Data.csv\")\n\n# From the scripts folder (need to go up one level first)\nstarbucks_user_data &lt;- read_csv(\"../data/Starbucks_User_Data.csv\")\nTRY: Check your working directory\ngetwd()  # This should show your project folder path\n## [1] \"/Users/alokman/Library/CloudStorage/GoogleDrive-alokman@bu.edu/My Drive/Teaching/S26_EMS747/ClassSlides\"\n\n\n\n\n\nThe readr package provides fast and friendly functions for reading rectangular data.\n\n\n\nFunction\nDescription\n\n\n\n\nread_csv()\nRead comma delimited files\n\n\nread_csv2()\nRead semicolon delimited files (European format)\n\n\nread_tsv()\nRead tab delimited files\n\n\nread_delim()\nRead files with any delimiter\n\n\n\nTRY: Load data from a URL\n# Load the data from the URL\nurl &lt;- \"https://raw.githubusercontent.com/aysedeniz09/Social_Media_Listening/refs/heads/main/MSC_social_media_list_data/Starbucks_User_Data.csv\"\nstarbucks_user_data &lt;- read_csv(url)\n\nhead(starbucks_user_data)\n## # A tibble: 6 √ó 16\n##   author_id conversation_id created_at          hashtag lang  like_count mention\n##       &lt;dbl&gt;           &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;  \n## 1     30973         1.61e18 2022-12-27 15:43:16 &lt;NA&gt;    en            10 &lt;NA&gt;   \n## 2     30973         1.60e18 2022-11-29 05:23:55 &lt;NA&gt;    en             9 Mo_sha‚Ä¶\n## 3     30973         1.59e18 2022-11-28 20:14:09 &lt;NA&gt;    en             2 Mixxed‚Ä¶\n## 4     30973         1.60e18 2022-11-28 12:51:28 &lt;NA&gt;    en             0 BihhKa‚Ä¶\n## 5     30973         1.60e18 2022-11-27 15:14:26 &lt;NA&gt;    en             0 BihhKa‚Ä¶\n## 6     30973         1.60e18 2022-11-24 17:47:24 &lt;NA&gt;    en             1 therea‚Ä¶\n## # ‚Ñπ 9 more variables: quote_count &lt;dbl&gt;, referenced_status_id &lt;dbl&gt;,\n## #   referenced_user_id &lt;dbl&gt;, reply_count &lt;dbl&gt;, retweet_count &lt;dbl&gt;,\n## #   row_id &lt;dbl&gt;, status_id &lt;dbl&gt;, text &lt;chr&gt;, type &lt;chr&gt;\nLoad from your computer\n# From the main project folder (data is a subfolder)\nstarbucks_user_data &lt;- read_csv(\"data/Starbucks_User_Data.csv\")\n\n# From a scripts folder (need to go up one level first)\nstarbucks_user_data &lt;- read_csv(\"../data/Starbucks_User_Data.csv\")\n\n\n\n\n\n\n\n\n\n\n\nArgument\nDescription\nExample\n\n\n\n\ncol_names\nUse first row as names or provide your own\ncol_names = FALSE or col_names = c(\"x\", \"y\", \"z\")\n\n\nskip\nNumber of lines to skip before reading\nskip = 1\n\n\nn_max\nMaximum number of rows to read\nn_max = 100\n\n\nna\nCharacter vector of strings to interpret as NA\nna = c(\"\", \"NA\", \"NULL\")\n\n\n\n# Skip header row and provide custom column names\nread_csv(\"data/file.csv\", col_names = c(\"x\", \"y\", \"z\"), skip = 1)\n\n# Read only first 100 rows\nread_csv(\"data/file.csv\", n_max = 100)\n\n# Treat \"NULL\" as missing values\nread_csv(\"data/file.csv\", na = c(\"\", \"NA\", \"NULL\"))\n\n\n\n\n\nThe readxl package reads both .xls and .xlsx files.\n# Install if needed\ninstall.packages(\"readxl\")\nlibrary(readxl)\n\n# Read an Excel file\ndata &lt;- read_excel(\"../data/Starbucks_User_Data.xlsx\")\n\n# Read a specific sheet by name or position\ndata &lt;- read_excel(\"../data/Starbucks_User_Data.xlsx\", sheet = \"Sheet2\")\ndata &lt;- read_excel(\"../data/Starbucks_User_Data.xlsx\", sheet = 2)\n\n# Get all sheet names\nexcel_sheets(\"../data/Starbucks_User_Data.xlsx\")\n\n# Read a specific range of cells\ndata &lt;- read_excel(\"../data/Starbucks_User_Data.xlsx\", range = \"B1:D10\")\n\n\n\n\nYou can also import data using RStudio‚Äôs point-and-click interface:\n\nClick on the Files pane\nNavigate to your data file\nClick Import Dataset\nConfigure import options\nClick Import\n\n \n\n\n\n\nColumn specifications define what data type each column will be imported as. By default, readr guesses column types based on the first 1000 rows.\nColumn Types:\n\n\n\nType\nFunction\nAbbreviation\n\n\n\n\nLogical\ncol_logical()\n‚Äúl‚Äù\n\n\nInteger\ncol_integer()\n‚Äúi‚Äù\n\n\nDouble\ncol_double()\n‚Äúd‚Äù\n\n\nCharacter\ncol_character()\n‚Äúc‚Äù\n\n\nFactor\ncol_factor()\n‚Äúf‚Äù\n\n\nDate\ncol_date()\n‚ÄúD‚Äù\n\n\nDateTime\ncol_datetime()\n‚ÄúT‚Äù\n\n\nSkip\ncol_skip()\n‚Äú-‚Äù or ‚Äú_‚Äù\n\n\n\n# Set specific column types\nread_csv(\"file.csv\", \n         col_types = list(\n           x = col_double(),\n           y = col_character(),\n           z = col_date()\n         ))\n\n# Use abbreviation string\nread_csv(\"file.csv\", col_types = \"dcD\")\n\n# Select specific columns to import\nread_csv(\"file.csv\", col_select = c(name, age, score))\n\n\n\nR comes with many built-in datasets for practice:\n# See all available datasets\ndata()\n\n# Load a built-in dataset\ndata(\"mtcars\")\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#tidy-data-principles",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#tidy-data-principles",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Tidy data is a consistent way to organize tabular data. A dataset is tidy if:\n\nEach variable is in its own column\nEach observation (case) is in its own row\nEach value is in its own cell\n\n\nImage from: Hassan, F. (2023, March 21). Tidy Data in Python. Medium.\nWhy Tidy Data?\n\nSimplifies data manipulation and visualization\nWorks seamlessly with tidyverse packages (dplyr, ggplot2, tidyr)\nMakes data analysis more reproducible\n\nTidy Cheat Sheet\n\n\n\n\nTibbles are a modern reimagining of data frames provided by the tibble package. They have improved behaviors:\n\nBetter printing (shows only first 10 rows and columns that fit on screen)\nNo partial matching when subsetting columns\nNever convert strings to factors automatically\nSubset with [] for a tibble, [[]] or $ for a vector\n\nlibrary(tibble)\n\n# Create a tibble by columns\nmy_tibble &lt;- tibble(\n  x = 1:3,\n  y = c(\"a\", \"b\", \"c\"),\n  z = c(TRUE, FALSE, TRUE)\n)\nmy_tibble\n## # A tibble: 3 √ó 3\n##       x y     z    \n##   &lt;int&gt; &lt;chr&gt; &lt;lgl&gt;\n## 1     1 a     TRUE \n## 2     2 b     FALSE\n## 3     3 c     TRUE\n# Create a tibble by rows (useful for small datasets)\nmy_tibble2 &lt;- tribble(\n  ~x, ~y, ~z,\n  1, \"a\", TRUE,\n  2, \"b\", FALSE,\n  3, \"c\", TRUE\n)\nmy_tibble2\n## # A tibble: 3 √ó 3\n##       x y     z    \n##   &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;\n## 1     1 a     TRUE \n## 2     2 b     FALSE\n## 3     3 c     TRUE\n# Convert data frame to tibble\nas_tibble(mtcars)\n## # A tibble: 32 √ó 11\n##      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n##  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n##  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n##  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n##  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n##  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n##  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n##  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n##  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n## 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n## # ‚Ñπ 22 more rows\n# Check if something is a tibble\nis_tibble(my_tibble)\n## [1] TRUE\nis_tibble(mtcars)\n## [1] FALSE\n\n\n\n\nThe pipe operator |&gt; (or %&gt;% from magrittr) allows you to chain operations together, making code more readable.\nShortcut: - Mac: Cmd + Shift + M - Windows: Ctrl + Shift + M\n# Without pipe (nested functions - hard to read)\nhead(arrange(filter(mtcars, mpg &gt; 20), desc(hp)), 3)\n##                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Lotus Europa  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Mazda RX4     21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n# With pipe (sequential - easy to read)\nmtcars |&gt; \n  filter(mpg &gt; 20) |&gt; \n  arrange(desc(hp)) |&gt; \n  head(3)\n##                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Lotus Europa  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Mazda RX4     21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nThe pipe takes the output of the left side and passes it as the first argument to the function on the right side."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#dplyr",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#dplyr",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "The dplyr package provides a powerful toolkit for data manipulation with intuitive ‚Äúverb‚Äù functions.\nlibrary(dplyr)\nCreate a sample dataset:\ndf &lt;- data.frame(\n  Movie_Title = c(\n    \"YOLO\", \"Successor\", \"Pegasus 2\", \"Deadpool & Wolverine\", \"Moana 2\", \n    \"The Hidden Blade\", \"Avatar: The Spirit Returns\"\n  ),\n  Release_Date = c(\n    \"January 2024\", \"February 2024\", \"March 2024\", \"July 2024\", \n    \"November 2024\", \"April 2024\", \"December 2024\"\n  ),\n  China_Box_Office_Gross = c(\n    479597304, 469612890, 466930272, 450000000, 350000000, 320000000, 550000000\n  ),\n  US_Box_Office_Gross = c(\n    310000000, 280000000, 290000000, 438000000, 221000000, 75000000, 600000000\n  ),\n  Total_Worldwide_Gross = c(\n    479597304, 469612890, 466930272, 850000000, 421000000, 400000000, 1300000000\n  )\n)\n\nhead(df)\n##            Movie_Title  Release_Date China_Box_Office_Gross US_Box_Office_Gross\n## 1                 YOLO  January 2024              479597304            3.10e+08\n## 2            Successor February 2024              469612890            2.80e+08\n## 3            Pegasus 2    March 2024              466930272            2.90e+08\n## 4 Deadpool & Wolverine     July 2024              450000000            4.38e+08\n## 5              Moana 2 November 2024              350000000            2.21e+08\n## 6     The Hidden Blade    April 2024              320000000            7.50e+07\n##   Total_Worldwide_Gross\n## 1             479597304\n## 2             469612890\n## 3             466930272\n## 4             850000000\n## 5             421000000\n## 6             400000000\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nfilter()\nSelect rows based on conditions\n\n\nselect()\nChoose specific columns\n\n\nmutate()\nCreate or transform columns\n\n\nsummarize()\nCompute summary statistics\n\n\ngroup_by()\nGroup data by variables\n\n\narrange()\nSort rows\n\n\nrename()\nRename columns\n\n\n\n\n\n\n\nSelect rows based on specific conditions.\n# Filter movies with China gross &gt; 400 million\ndf |&gt; \n  filter(China_Box_Office_Gross &gt; 400000000)\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1                       YOLO  January 2024              479597304\n## 2                  Successor February 2024              469612890\n## 3                  Pegasus 2    March 2024              466930272\n## 4       Deadpool & Wolverine     July 2024              450000000\n## 5 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross Total_Worldwide_Gross\n## 1            3.10e+08             479597304\n## 2            2.80e+08             469612890\n## 3            2.90e+08             466930272\n## 4            4.38e+08             850000000\n## 5            6.00e+08            1300000000\n# Multiple conditions with AND (&)\nmtcars |&gt; \n  filter(mpg &gt; 20 & cyl == 6)\n##                 mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4      21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag  21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n# Multiple conditions with OR (|)\nmtcars |&gt; \n  filter(mpg &gt; 25 | hp &gt; 200)\n##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n\n\n\n\nChoose specific columns from a dataset.\n# Select specific columns by name\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross)\n##                  Movie_Title Total_Worldwide_Gross\n## 1                       YOLO             479597304\n## 2                  Successor             469612890\n## 3                  Pegasus 2             466930272\n## 4       Deadpool & Wolverine             850000000\n## 5                    Moana 2             421000000\n## 6           The Hidden Blade             400000000\n## 7 Avatar: The Spirit Returns            1300000000\n# Select a range of columns\ndf |&gt; \n  select(Movie_Title:US_Box_Office_Gross)\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1                       YOLO  January 2024              479597304\n## 2                  Successor February 2024              469612890\n## 3                  Pegasus 2    March 2024              466930272\n## 4       Deadpool & Wolverine     July 2024              450000000\n## 5                    Moana 2 November 2024              350000000\n## 6           The Hidden Blade    April 2024              320000000\n## 7 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross\n## 1            3.10e+08\n## 2            2.80e+08\n## 3            2.90e+08\n## 4            4.38e+08\n## 5            2.21e+08\n## 6            7.50e+07\n## 7            6.00e+08\n# Exclude columns with minus sign\ndf |&gt; \n  select(-Release_Date)\n##                  Movie_Title China_Box_Office_Gross US_Box_Office_Gross\n## 1                       YOLO              479597304            3.10e+08\n## 2                  Successor              469612890            2.80e+08\n## 3                  Pegasus 2              466930272            2.90e+08\n## 4       Deadpool & Wolverine              450000000            4.38e+08\n## 5                    Moana 2              350000000            2.21e+08\n## 6           The Hidden Blade              320000000            7.50e+07\n## 7 Avatar: The Spirit Returns              550000000            6.00e+08\n##   Total_Worldwide_Gross\n## 1             479597304\n## 2             469612890\n## 3             466930272\n## 4             850000000\n## 5             421000000\n## 6             400000000\n## 7            1300000000\nHelper functions for select():\n\n\n\nHelper\nDescription\n\n\n\n\nstarts_with(\"x\")\nColumns starting with ‚Äúx‚Äù\n\n\nends_with(\"x\")\nColumns ending with ‚Äúx‚Äù\n\n\ncontains(\"x\")\nColumns containing ‚Äúx‚Äù\n\n\neverything()\nAll columns\n\n\n\n# Select columns containing \"Gross\"\ndf |&gt; \n  select(Movie_Title, contains(\"Gross\"))\n##                  Movie_Title China_Box_Office_Gross US_Box_Office_Gross\n## 1                       YOLO              479597304            3.10e+08\n## 2                  Successor              469612890            2.80e+08\n## 3                  Pegasus 2              466930272            2.90e+08\n## 4       Deadpool & Wolverine              450000000            4.38e+08\n## 5                    Moana 2              350000000            2.21e+08\n## 6           The Hidden Blade              320000000            7.50e+07\n## 7 Avatar: The Spirit Returns              550000000            6.00e+08\n##   Total_Worldwide_Gross\n## 1             479597304\n## 2             469612890\n## 3             466930272\n## 4             850000000\n## 5             421000000\n## 6             400000000\n## 7            1300000000\n\n\n\n\nCreate new columns or transform existing ones.\n# Create a new column\ndf &lt;- df |&gt; \n  mutate(Profit = Total_Worldwide_Gross - US_Box_Office_Gross)\n\n# Multiple new columns at once\ndf |&gt; \n  mutate(\n    Profit = Total_Worldwide_Gross - US_Box_Office_Gross,\n    China_Pct = China_Box_Office_Gross / Total_Worldwide_Gross * 100\n  ) |&gt; \n  select(Movie_Title, Profit, China_Pct)\n##                  Movie_Title    Profit China_Pct\n## 1                       YOLO 169597304 100.00000\n## 2                  Successor 189612890 100.00000\n## 3                  Pegasus 2 176930272 100.00000\n## 4       Deadpool & Wolverine 412000000  52.94118\n## 5                    Moana 2 200000000  83.13539\n## 6           The Hidden Blade 325000000  80.00000\n## 7 Avatar: The Spirit Returns 700000000  42.30769\n\n\n\n\nCompute summary statistics. Often used with group_by().\n# Overall summary\ndf |&gt; \n  summarize(\n    Total_China = sum(China_Box_Office_Gross),\n    Avg_Worldwide = mean(Total_Worldwide_Gross),\n    Count = n()\n  )\n##   Total_China Avg_Worldwide Count\n## 1  3086140466     626734352     7\n# Summary with mtcars\nmtcars |&gt; \n  summarize(\n    mean_mpg = mean(mpg),\n    sd_mpg = sd(mpg),\n    median_hp = median(hp)\n  )\n##   mean_mpg   sd_mpg median_hp\n## 1 20.09062 6.026948       123\n\n\n\n\nGroup data by one or more variables for grouped operations.\n# Group by cylinders and summarize\nmtcars |&gt; \n  group_by(cyl) |&gt; \n  summarize(\n    mean_mpg = mean(mpg),\n    count = n()\n  )\n## # A tibble: 3 √ó 3\n##     cyl mean_mpg count\n##   &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n## 1     4     26.7    11\n## 2     6     19.7     7\n## 3     8     15.1    14\n# Group by multiple variables\nmtcars |&gt; \n  group_by(cyl, gear) |&gt; \n  summarize(\n    mean_mpg = mean(mpg),\n    count = n(),\n    .groups = \"drop\"  # Ungroup after summarizing\n  )\n## # A tibble: 8 √ó 4\n##     cyl  gear mean_mpg count\n##   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n## 1     4     3     21.5     1\n## 2     4     4     26.9     8\n## 3     4     5     28.2     2\n## 4     6     3     19.8     2\n## 5     6     4     19.8     4\n## 6     6     5     19.7     1\n## 7     8     3     15.0    12\n## 8     8     5     15.4     2\n\n\n\n\nSort rows by one or more variables.\n# Sort ascending (default)\ndf |&gt; \n  arrange(Total_Worldwide_Gross)\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1           The Hidden Blade    April 2024              320000000\n## 2                    Moana 2 November 2024              350000000\n## 3                  Pegasus 2    March 2024              466930272\n## 4                  Successor February 2024              469612890\n## 5                       YOLO  January 2024              479597304\n## 6       Deadpool & Wolverine     July 2024              450000000\n## 7 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross Total_Worldwide_Gross    Profit\n## 1            7.50e+07             400000000 325000000\n## 2            2.21e+08             421000000 200000000\n## 3            2.90e+08             466930272 176930272\n## 4            2.80e+08             469612890 189612890\n## 5            3.10e+08             479597304 169597304\n## 6            4.38e+08             850000000 412000000\n## 7            6.00e+08            1300000000 700000000\n# Sort descending\ndf |&gt; \n  arrange(desc(Total_Worldwide_Gross))\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1 Avatar: The Spirit Returns December 2024              550000000\n## 2       Deadpool & Wolverine     July 2024              450000000\n## 3                       YOLO  January 2024              479597304\n## 4                  Successor February 2024              469612890\n## 5                  Pegasus 2    March 2024              466930272\n## 6                    Moana 2 November 2024              350000000\n## 7           The Hidden Blade    April 2024              320000000\n##   US_Box_Office_Gross Total_Worldwide_Gross    Profit\n## 1            6.00e+08            1300000000 700000000\n## 2            4.38e+08             850000000 412000000\n## 3            3.10e+08             479597304 169597304\n## 4            2.80e+08             469612890 189612890\n## 5            2.90e+08             466930272 176930272\n## 6            2.21e+08             421000000 200000000\n## 7            7.50e+07             400000000 325000000\n# Sort by multiple columns\nmtcars |&gt; \n  arrange(cyl, desc(mpg)) |&gt; \n  head()\n##                 mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## Toyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## Fiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## Honda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## Lotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## Fiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## Porsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n\n\n\n\nRename columns. Syntax: rename(new_name = old_name)\ndf_renamed &lt;- df |&gt; \n  rename(\n    China_Gross = China_Box_Office_Gross,\n    US_Gross = US_Box_Office_Gross,\n    Worldwide_Gross = Total_Worldwide_Gross\n  )\n\nnames(df_renamed)\n## [1] \"Movie_Title\"     \"Release_Date\"    \"China_Gross\"     \"US_Gross\"       \n## [5] \"Worldwide_Gross\" \"Profit\"\n\n\n\nUse the movies dataset and perform the following:\n\nfilter(): Select movies with a total worldwide gross greater than $500M.\nselect(): Choose only Movie_Title and columns containing ‚ÄúGross‚Äù.\nmutate(): Add a new column for US percentage of worldwide gross.\nsummarize(): Compute the total and average gross for China.\ngroup_by() + summarize(): Calculate average gross by release month (hint: you‚Äôll need to extract month first).\narrange(): Sort movies by worldwide gross in descending order.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#data-reshaping",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#data-reshaping",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Data often needs to be reshaped between ‚Äúwide‚Äù and ‚Äúlong‚Äù formats for different analyses.\n\n\n\n\n\nTransform data from wide to long format by collapsing multiple columns into two: one for names and one for values.\nKey Arguments: - cols: Columns to pivot - names_to: Name for the new column holding original column names - values_to: Name for the new column holding values\n# Convert movie data to long format\nlong_df &lt;- df |&gt; \n  pivot_longer(\n    cols = China_Box_Office_Gross:Total_Worldwide_Gross,\n    names_to = \"Box_Office_Type\",\n    values_to = \"Gross\"\n  )\n\nprint(long_df)\n## # A tibble: 21 √ó 5\n##    Movie_Title          Release_Date     Profit Box_Office_Type            Gross\n##    &lt;chr&gt;                &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt;\n##  1 YOLO                 January 2024  169597304 China_Box_Office_Gross 479597304\n##  2 YOLO                 January 2024  169597304 US_Box_Office_Gross    310000000\n##  3 YOLO                 January 2024  169597304 Total_Worldwide_Gross  479597304\n##  4 Successor            February 2024 189612890 China_Box_Office_Gross 469612890\n##  5 Successor            February 2024 189612890 US_Box_Office_Gross    280000000\n##  6 Successor            February 2024 189612890 Total_Worldwide_Gross  469612890\n##  7 Pegasus 2            March 2024    176930272 China_Box_Office_Gross 466930272\n##  8 Pegasus 2            March 2024    176930272 US_Box_Office_Gross    290000000\n##  9 Pegasus 2            March 2024    176930272 Total_Worldwide_Gross  466930272\n## 10 Deadpool & Wolverine July 2024     412000000 China_Box_Office_Gross 450000000\n## # ‚Ñπ 11 more rows\n# Another example with mtcars\nmtcars_long &lt;- mtcars |&gt; \n  rownames_to_column(\"car\") |&gt; \n  pivot_longer(\n    cols = mpg:carb,\n    names_to = \"metric\",\n    values_to = \"value\"\n  )\n\nhead(mtcars_long)\n## # A tibble: 6 √ó 3\n##   car       metric  value\n##   &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;\n## 1 Mazda RX4 mpg     21   \n## 2 Mazda RX4 cyl      6   \n## 3 Mazda RX4 disp   160   \n## 4 Mazda RX4 hp     110   \n## 5 Mazda RX4 drat     3.9 \n## 6 Mazda RX4 wt       2.62\n\n\n\n\nTransform data from long to wide format by spreading values across multiple columns.\nKey Arguments: - names_from: Column whose values become new column names - values_from: Column whose values fill the new columns\n# Convert back to wide format\nwide_df &lt;- long_df |&gt; \n  pivot_wider(\n    names_from = Box_Office_Type,\n    values_from = Gross\n  )\n\nprint(wide_df)\n## # A tibble: 7 √ó 6\n##   Movie_Title     Release_Date Profit China_Box_Office_Gross US_Box_Office_Gross\n##   &lt;chr&gt;           &lt;chr&gt;         &lt;dbl&gt;                  &lt;dbl&gt;               &lt;dbl&gt;\n## 1 YOLO            January 2024 1.70e8              479597304           310000000\n## 2 Successor       February 20‚Ä¶ 1.90e8              469612890           280000000\n## 3 Pegasus 2       March 2024   1.77e8              466930272           290000000\n## 4 Deadpool & Wol‚Ä¶ July 2024    4.12e8              450000000           438000000\n## 5 Moana 2         November 20‚Ä¶ 2   e8              350000000           221000000\n## 6 The Hidden Bla‚Ä¶ April 2024   3.25e8              320000000            75000000\n## 7 Avatar: The Sp‚Ä¶ December 20‚Ä¶ 7   e8              550000000           600000000\n## # ‚Ñπ 1 more variable: Total_Worldwide_Gross &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\n\nFeature\npivot_longer()\npivot_wider()\n\n\n\n\nDirection\nWide ‚Üí Long\nLong ‚Üí Wide\n\n\nPurpose\nConsolidate columns\nSpread values across columns\n\n\nKey Arguments\ncols, names_to, values_to\nnames_from, values_from\n\n\nTypical Use\nTidying data for analysis\nSummarizing for presentation\n\n\n\n\n\n\n\n\nSplit the contents of a single column into multiple columns.\n# Separate Release_Date into Month and Year\ndf_separated &lt;- df |&gt; \n  separate(Release_Date, into = c(\"Month\", \"Year\"), sep = \" \")\n\nprint(df_separated)\n##                  Movie_Title    Month Year China_Box_Office_Gross\n## 1                       YOLO  January 2024              479597304\n## 2                  Successor February 2024              469612890\n## 3                  Pegasus 2    March 2024              466930272\n## 4       Deadpool & Wolverine     July 2024              450000000\n## 5                    Moana 2 November 2024              350000000\n## 6           The Hidden Blade    April 2024              320000000\n## 7 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross Total_Worldwide_Gross    Profit\n## 1            3.10e+08             479597304 169597304\n## 2            2.80e+08             469612890 189612890\n## 3            2.90e+08             466930272 176930272\n## 4            4.38e+08             850000000 412000000\n## 5            2.21e+08             421000000 200000000\n## 6            7.50e+07             400000000 325000000\n## 7            6.00e+08            1300000000 700000000\nRelated functions: - separate_wider_delim(): Separate by delimiter into columns - separate_wider_position(): Separate by position into columns - separate_longer_delim(): Separate into rows instead of columns\n\n\n\n\nCombine multiple columns into a single column.\n# Combine Month and Year back into Release_Date\ndf_united &lt;- df_separated |&gt; \n  unite(\"Release_Date\", Month, Year, sep = \" \")\n\nprint(df_united)\n##                  Movie_Title  Release_Date China_Box_Office_Gross\n## 1                       YOLO  January 2024              479597304\n## 2                  Successor February 2024              469612890\n## 3                  Pegasus 2    March 2024              466930272\n## 4       Deadpool & Wolverine     July 2024              450000000\n## 5                    Moana 2 November 2024              350000000\n## 6           The Hidden Blade    April 2024              320000000\n## 7 Avatar: The Spirit Returns December 2024              550000000\n##   US_Box_Office_Gross Total_Worldwide_Gross    Profit\n## 1            3.10e+08             479597304 169597304\n## 2            2.80e+08             469612890 189612890\n## 3            2.90e+08             466930272 176930272\n## 4            4.38e+08             850000000 412000000\n## 5            2.21e+08             421000000 200000000\n## 6            7.50e+07             400000000 325000000\n## 7            6.00e+08            1300000000 700000000"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#handling-missing-values",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#handling-missing-values",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Missing values (NA) are common in real-world data. The tidyr package provides functions to handle them.\n# Create sample data with missing values\nmissing_df &lt;- tibble(\n  x1 = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n  x2 = c(1, NA, NA, 3, NA)\n)\nmissing_df\n## # A tibble: 5 √ó 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B        NA\n## 3 C        NA\n## 4 D         3\n## 5 E        NA\n\n\n\nRemove rows containing NA values.\n# Drop rows with NA in any column\nmissing_df |&gt; \n  drop_na()\n## # A tibble: 2 √ó 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 D         3\n# Drop rows with NA only in specific columns\nmissing_df |&gt; \n  drop_na(x2)\n## # A tibble: 2 √ó 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 D         3\n\n\n\n\nFill in NA values using the previous or next value.\n# Fill down (default)\nmissing_df |&gt; \n  fill(x2, .direction = \"down\")\n## # A tibble: 5 √ó 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         1\n## 3 C         1\n## 4 D         3\n## 5 E         3\n# Fill up\nmissing_df |&gt; \n  fill(x2, .direction = \"up\")\n## # A tibble: 5 √ó 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         3\n## 3 C         3\n## 4 D         3\n## 5 E        NA\n# Fill in both directions (down first, then up)\nmissing_df |&gt; \n  fill(x2, .direction = \"downup\")\n## # A tibble: 5 √ó 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         1\n## 3 C         1\n## 4 D         3\n## 5 E         3\n\n\n\n\nReplace NA values with a specified value.\n# Replace NA with a specific value\nmissing_df |&gt; \n  replace_na(list(x2 = 0))\n## # A tibble: 5 √ó 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         0\n## 3 C         0\n## 4 D         3\n## 5 E         0\n# Replace NA with the mean (requires mutate)\nmissing_df |&gt; \n  mutate(x2 = replace_na(x2, mean(x2, na.rm = TRUE)))\n## # A tibble: 5 √ó 2\n##   x1       x2\n##   &lt;chr&gt; &lt;dbl&gt;\n## 1 A         1\n## 2 B         2\n## 3 C         2\n## 4 D         3\n## 5 E         2"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#expanding-tables",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#expanding-tables",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Create new combinations of variables or identify implicit missing values.\n\n\n\nCreate a tibble with all possible combinations of specified variables.\n# All combinations of cyl and gear\nmtcars |&gt; \n  expand(cyl, gear)\n## # A tibble: 9 √ó 2\n##     cyl  gear\n##   &lt;dbl&gt; &lt;dbl&gt;\n## 1     4     3\n## 2     4     4\n## 3     4     5\n## 4     6     3\n## 5     6     4\n## 6     6     5\n## 7     8     3\n## 8     8     4\n## 9     8     5\n\n\n\n\nAdd missing combinations of values to a dataset, filling other variables with NA.\n# Sample data with implicit missing combinations\nsales_df &lt;- tibble(\n  store = c(\"A\", \"A\", \"B\"),\n  product = c(\"X\", \"Y\", \"X\"),\n  sales = c(100, 150, 200)\n)\nsales_df\n## # A tibble: 3 √ó 3\n##   store product sales\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 A     X         100\n## 2 A     Y         150\n## 3 B     X         200\n# Complete with all combinations\nsales_df |&gt; \n  complete(store, product)\n## # A tibble: 4 √ó 3\n##   store product sales\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 A     X         100\n## 2 A     Y         150\n## 3 B     X         200\n## 4 B     Y          NA\n# Complete and fill missing values with 0\nsales_df |&gt; \n  complete(store, product, fill = list(sales = 0))\n## # A tibble: 4 √ó 3\n##   store product sales\n##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n## 1 A     X         100\n## 2 A     Y         150\n## 3 B     X         200\n## 4 B     Y           0"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#joins-in-tidyverse",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#joins-in-tidyverse",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Joins combine two datasets based on a common key (column).\n\n\n\nJoin Type\nDescription\n\n\n\n\nleft_join()\nKeep all rows from the left dataset\n\n\nright_join()\nKeep all rows from the right dataset\n\n\ninner_join()\nKeep only rows that match in both datasets\n\n\nfull_join()\nKeep all rows from both datasets\n\n\nanti_join()\nKeep rows from left that don‚Äôt match right\n\n\n\n\n\nCreate datasets for joining:\n# Directors dataset\ndf_directors &lt;- tibble(\n  Movie_Title = c(\"YOLO\", \"Successor\", \"Deadpool & Wolverine\", \"Moana 2\"),\n  Director = c(\"Jia Ling\", \"Xu Zheng\", \"Shawn Levy\", \"David Derrick Jr.\")\n)\n\n# Ratings dataset  \ndf_ratings &lt;- tibble(\n  Movie_Title = c(\"YOLO\", \"Pegasus 2\", \"Moana 2\", \"Wicked\"),\n  Rating = c(8.1, 7.5, 7.8, 8.0)\n)\n\n\n\nKeep all rows from the left dataset, add matching data from the right.\n# Add directors to movies (keep all movies)\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross) |&gt; \n  left_join(df_directors, by = \"Movie_Title\")\n##                  Movie_Title Total_Worldwide_Gross          Director\n## 1                       YOLO             479597304          Jia Ling\n## 2                  Successor             469612890          Xu Zheng\n## 3                  Pegasus 2             466930272              &lt;NA&gt;\n## 4       Deadpool & Wolverine             850000000        Shawn Levy\n## 5                    Moana 2             421000000 David Derrick Jr.\n## 6           The Hidden Blade             400000000              &lt;NA&gt;\n## 7 Avatar: The Spirit Returns            1300000000              &lt;NA&gt;\n\n\n\n\nKeep all rows from the right dataset, add matching data from the left.\n# Keep all directors, add movie data\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross) |&gt; \n  right_join(df_directors, by = \"Movie_Title\")\n##            Movie_Title Total_Worldwide_Gross          Director\n## 1                 YOLO             479597304          Jia Ling\n## 2            Successor             469612890          Xu Zheng\n## 3 Deadpool & Wolverine             850000000        Shawn Levy\n## 4              Moana 2             421000000 David Derrick Jr.\n\n\n\n\nKeep only rows that have matches in both datasets.\n# Only movies that have both gross data AND ratings\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross) |&gt; \n  inner_join(df_ratings, by = \"Movie_Title\")\n##   Movie_Title Total_Worldwide_Gross Rating\n## 1        YOLO             479597304    8.1\n## 2   Pegasus 2             466930272    7.5\n## 3     Moana 2             421000000    7.8\n\n\n\n\nKeep all rows from both datasets, filling with NA where there‚Äôs no match.\n# Combine all movies and ratings\ndf |&gt; \n  select(Movie_Title, Total_Worldwide_Gross) |&gt; \n  full_join(df_ratings, by = \"Movie_Title\")\n##                  Movie_Title Total_Worldwide_Gross Rating\n## 1                       YOLO             479597304    8.1\n## 2                  Successor             469612890     NA\n## 3                  Pegasus 2             466930272    7.5\n## 4       Deadpool & Wolverine             850000000     NA\n## 5                    Moana 2             421000000    7.8\n## 6           The Hidden Blade             400000000     NA\n## 7 Avatar: The Spirit Returns            1300000000     NA\n## 8                     Wicked                    NA    8.0\n\n\n\n\nReturn rows from the left dataset that do NOT have a match in the right.\n# Movies WITHOUT directors\ndf |&gt; \n  select(Movie_Title) |&gt; \n  anti_join(df_directors, by = \"Movie_Title\")\n##                  Movie_Title\n## 1                  Pegasus 2\n## 2           The Hidden Blade\n## 3 Avatar: The Spirit Returns\n# Movies WITHOUT ratings\ndf |&gt; \n  select(Movie_Title) |&gt; \n  anti_join(df_ratings, by = \"Movie_Title\")\n##                  Movie_Title\n## 1                  Successor\n## 2       Deadpool & Wolverine\n## 3           The Hidden Blade\n## 4 Avatar: The Spirit Returns\n\n\n\nUse the provided datasets (df, df_directors, df_ratings):\n\nleft_join(): Merge directors into the main movie dataset.\ninner_join(): Find movies that have both director and rating information.\nanti_join(): Identify movies without ratings.\nfull_join(): Create a comprehensive dataset with all movies, directors, and ratings.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#regular-expressions",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#regular-expressions",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Regular expressions (regex) are powerful patterns used to match, search, and manipulate text. They are essential for data cleaning and text processing.\nWe will use the stringr package, which is part of the tidyverse.\nlibrary(stringr)\n\n\n\nA regular expression is a sequence of characters that defines a search pattern. Think of it as a sophisticated ‚Äúfind and replace‚Äù tool.\nWhy use regex?\n\nClean messy text data (remove special characters, standardize formats)\nExtract specific patterns (emails, phone numbers, dates)\nFilter rows based on text patterns\nTransform text data\n\n\n\n\n\n\n\nThe simplest regex matches exact text:\nfruits &lt;- c(\"apple\", \"banana\", \"pineapple\", \"grape\", \"grapefruit\")\n\n# Find fruits containing \"apple\"\nstr_detect(fruits, \"apple\")\n## [1]  TRUE FALSE  TRUE FALSE FALSE\n# Extract matches\nstr_subset(fruits, \"apple\")\n## [1] \"apple\"     \"pineapple\"\n\n\n\n\nThese characters have special meanings in regex:\n\n\n\n\n\n\n\n\n\nCharacter\nMeaning\nExample\nMatches\n\n\n\n\n.\nAny single character\n\"a.c\"\n‚Äúabc‚Äù, ‚Äúa1c‚Äù, ‚Äúa c‚Äù\n\n\n^\nStart of string\n\"^The\"\n‚ÄúThe dog‚Äù but not ‚ÄúSee The dog‚Äù\n\n\n$\nEnd of string\n\"end$\"\n‚Äúthe end‚Äù but not ‚Äúendless‚Äù\n\n\n*\nZero or more of previous\n\"ab*c\"\n‚Äúac‚Äù, ‚Äúabc‚Äù, ‚Äúabbc‚Äù\n\n\n+\nOne or more of previous\n\"ab+c\"\n‚Äúabc‚Äù, ‚Äúabbc‚Äù but not ‚Äúac‚Äù\n\n\n?\nZero or one of previous\n\"colou?r\"\n‚Äúcolor‚Äù, ‚Äúcolour‚Äù\n\n\n\\\\\nEscape special character\n\"\\\\.\"\nLiteral period ‚Äú.‚Äù\n\n\n\ntext &lt;- c(\"cat\", \"car\", \"card\", \"care\", \"scar\")\n\n# Match \"ca\" followed by any character\nstr_subset(text, \"ca.\")\n## [1] \"cat\"  \"car\"  \"card\" \"care\" \"scar\"\n# Match words starting with \"ca\"\nstr_subset(text, \"^ca\")\n## [1] \"cat\"  \"car\"  \"card\" \"care\"\n# Match words ending with \"r\"\nstr_subset(text, \"r$\")\n## [1] \"car\"  \"scar\"\n\n\n\n\nUse square brackets [] to match any character in a set:\n\n\n\n\n\n\n\n\nPattern\nMeaning\nExample\n\n\n\n\n[abc]\nMatch a, b, or c\n\"[aeiou]\" matches vowels\n\n\n[a-z]\nMatch any lowercase letter\n\n\n\n[A-Z]\nMatch any uppercase letter\n\n\n\n[0-9]\nMatch any digit\nSame as \\\\d\n\n\n[^abc]\nMatch anything EXCEPT a, b, c\n[^0-9] matches non-digits\n\n\n\nwords &lt;- c(\"hello\", \"HELLO\", \"Hello\", \"h3llo\", \"12345\")\n\n# Match words with lowercase letters\nstr_subset(words, \"[a-z]\")\n## [1] \"hello\" \"Hello\" \"h3llo\"\n# Match words with digits\nstr_subset(words, \"[0-9]\")\n## [1] \"h3llo\" \"12345\"\n# Match words starting with uppercase\nstr_subset(words, \"^[A-Z]\")\n## [1] \"HELLO\" \"Hello\"\n\n\n\n\n\n\n\nShorthand\nMeaning\nEquivalent\n\n\n\n\n\\\\d\nAny digit\n[0-9]\n\n\n\\\\D\nAny non-digit\n[^0-9]\n\n\n\\\\w\nAny word character\n[a-zA-Z0-9_]\n\n\n\\\\W\nAny non-word character\n[^a-zA-Z0-9_]\n\n\n\\\\s\nAny whitespace\nSpace, tab, newline\n\n\n\\\\S\nAny non-whitespace\n\n\n\n\nmixed &lt;- c(\"abc123\", \"hello world\", \"test_case\", \"no-hyphens\")\n\n# Find strings with digits\nstr_subset(mixed, \"\\\\d\")\n## [1] \"abc123\"\n# Find strings with whitespace\nstr_subset(mixed, \"\\\\s\")\n## [1] \"hello world\"\n# Find strings with word characters only (no spaces or hyphens)\nstr_detect(mixed, \"^\\\\w+$\")\n## [1]  TRUE FALSE  TRUE FALSE\n\n\n\n\n\nThe stringr package provides consistent, easy-to-use functions for working with strings and regex.\n\n\n\n\n\n\n\n\nFunction\nPurpose\nExample\n\n\n\n\nstr_detect()\nDoes pattern exist? (returns TRUE/FALSE)\nstr_detect(x, \"pattern\")\n\n\nstr_subset()\nReturn elements that match\nstr_subset(x, \"pattern\")\n\n\nstr_extract()\nExtract first match\nstr_extract(x, \"pattern\")\n\n\nstr_extract_all()\nExtract all matches\nstr_extract_all(x, \"pattern\")\n\n\nstr_replace()\nReplace first match\nstr_replace(x, \"pattern\", \"replacement\")\n\n\nstr_replace_all()\nReplace all matches\nstr_replace_all(x, \"pattern\", \"replacement\")\n\n\nstr_match()\nExtract groups from first match\nstr_match(x, \"pattern\")\n\n\nstr_count()\nCount matches\nstr_count(x, \"pattern\")\n\n\nstr_split()\nSplit string by pattern\nstr_split(x, \"pattern\")\n\n\n\n\n\n\nReturns TRUE or FALSE for each element. Great for filtering with dplyr::filter().\nemails &lt;- c(\"user@gmail.com\", \"test@yahoo.com\", \"invalid-email\", \"admin@bu.edu\")\n\n# Check which are valid emails (simple check)\nstr_detect(emails, \"@\")\n## [1]  TRUE  TRUE FALSE  TRUE\n# Use with filter\ndata.frame(email = emails) |&gt;\n  filter(str_detect(email, \"\\\\.edu$\"))\n##          email\n## 1 admin@bu.edu\n\n\n\n\nPulls out the first matching pattern from each string.\nsentences &lt;- c(\"Call me at 555-1234\", \"My number is 555-5678\", \"No phone here\")\n\n# Extract phone numbers\nstr_extract(sentences, \"\\\\d{3}-\\\\d{4}\")\n## [1] \"555-1234\" \"555-5678\" NA\n# Extract first word\nstr_extract(sentences, \"^\\\\w+\")\n## [1] \"Call\" \"My\"   \"No\"\n\n\n\n\nmessy_text &lt;- c(\"Hello   World\", \"Too    many   spaces\", \"Normal text\")\n\n# Replace multiple spaces with single space\nstr_replace_all(messy_text, \"\\\\s+\", \" \")\n## [1] \"Hello World\"     \"Too many spaces\" \"Normal text\"\n# Remove all digits\nstr_replace_all(\"Phone: 555-1234\", \"\\\\d\", \"X\")\n## [1] \"Phone: XXX-XXXX\"\n\n\n\n\nUse parentheses () to create capture groups. str_match() returns a matrix with the full match and each group.\ndates &lt;- c(\"2024-01-15\", \"2023-12-25\", \"2025-06-30\")\n\n# Extract year, month, day separately\nstr_match(dates, \"(\\\\d{4})-(\\\\d{2})-(\\\\d{2})\")\n##      [,1]         [,2]   [,3] [,4]\n## [1,] \"2024-01-15\" \"2024\" \"01\" \"15\"\n## [2,] \"2023-12-25\" \"2023\" \"12\" \"25\"\n## [3,] \"2025-06-30\" \"2025\" \"06\" \"30\"\n\n\n\n\n\nHere are some useful patterns for common data cleaning tasks:\n\n\n\nTask\nPattern\nExample\n\n\n\n\nEmail\n[\\\\w.-]+@[\\\\w.-]+\\\\.\\\\w+\nuser@example.com\n\n\nPhone (US)\n\\\\d{3}[-.\\\\s]?\\\\d{3}[-.\\\\s]?\\\\d{4}\n555-123-4567\n\n\nURL\nhttps?://[\\\\w./]+\nhttps://example.com\n\n\nTwitter handle\n@\\\\w+\n@username\n\n\nHashtag\n#\\\\w+\n#DataScience\n\n\nDate (YYYY-MM-DD)\n\\\\d{4}-\\\\d{2}-\\\\d{2}\n2024-01-15\n\n\nTime (HH:MM)\n\\\\d{2}:\\\\d{2}\n14:30\n\n\n\n\n\n\n\n\n\ntweets &lt;- c(\n  \"@user1 Check out this link https://t.co/abc123 #DataScience\",\n  \"Hello @user2! Great post! #RStats #coding\",\n  \"Just a regular tweet with no mentions\"\n)\n\n# Extract mentions (@username)\nstr_extract_all(tweets, \"@\\\\w+\")\n## [[1]]\n## [1] \"@user1\"\n## \n## [[2]]\n## [1] \"@user2\"\n## \n## [[3]]\n## character(0)\n# Extract hashtags\nstr_extract_all(tweets, \"#\\\\w+\")\n## [[1]]\n## [1] \"#DataScience\"\n## \n## [[2]]\n## [1] \"#RStats\" \"#coding\"\n## \n## [[3]]\n## character(0)\n# Remove URLs\nstr_replace_all(tweets, \"https?://\\\\S+\", \"[URL]\")\n## [1] \"@user1 Check out this link [URL] #DataScience\"\n## [2] \"Hello @user2! Great post! #RStats #coding\"    \n## [3] \"Just a regular tweet with no mentions\"\n\n\n\n\n# Sample data\ncustomer_data &lt;- data.frame(\n  info = c(\n    \"John Smith, Email: john@gmail.com, Phone: 555-1234\",\n    \"Jane Doe, Email: jane@yahoo.com, Phone: 555-5678\",\n    \"Bob Wilson, Email: bob@bu.edu, Phone: 555-9999\"\n  )\n)\n\n# Extract emails and phones into new columns\ncustomer_data |&gt;\n  mutate(\n    email = str_extract(info, \"[\\\\w.-]+@[\\\\w.-]+\\\\.\\\\w+\"),\n    phone = str_extract(info, \"\\\\d{3}-\\\\d{4}\"),\n    name = str_extract(info, \"^[A-Za-z]+ [A-Za-z]+\")\n  )\n##                                                 info          email    phone\n## 1 John Smith, Email: john@gmail.com, Phone: 555-1234 john@gmail.com 555-1234\n## 2   Jane Doe, Email: jane@yahoo.com, Phone: 555-5678 jane@yahoo.com 555-5678\n## 3     Bob Wilson, Email: bob@bu.edu, Phone: 555-9999     bob@bu.edu 555-9999\n##         name\n## 1 John Smith\n## 2   Jane Doe\n## 3 Bob Wilson\n\n\n\n\nlibrary(janeaustenr)\n\n# Get all Jane Austen books\nbooks &lt;- austen_books()\n\n# Find lines mentioning \"Mr.\" followed by a name\nbooks |&gt;\n  filter(str_detect(text, \"Mr\\\\.\\\\s[A-Z][a-z]+\")) |&gt;\n  mutate(mr_name = str_extract(text, \"Mr\\\\.\\\\s[A-Z][a-z]+\")) |&gt;\n  count(mr_name, sort = TRUE) |&gt;\n  head(10)\n\n\n\n\nUsing the Starbucks Twitter data:\n\nExtract all Twitter usernames (mentions starting with @) from the text column\nCount how many tweets contain hashtags\nFind tweets that mention ‚Äúcoffee‚Äù (case insensitive - hint: use (?i) or str_to_lower())\nExtract any URLs from the tweets\nCreate a new column with the text cleaned of URLs and mentions\n\n### Your workspace\n# Load the data if needed\nurl &lt;- \"https://raw.githubusercontent.com/aysedeniz09/Social_Media_Listening/refs/heads/main/MSC_social_media_list_data/Starbucks_User_Data.csv\"\nstarbucks &lt;- read_csv(url)\n\n# 1. Extract mentions\n\n\n# 2. Count hashtag tweets\n\n\n# 3. Find coffee tweets\n\n\n# 4. Extract URLs\n\n\n# 5. Clean text"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L2-github.html#lecture-2-cheat-sheet",
    "href": "Spring2026/LectureSlides/bigdata_L2-github.html#lecture-2-cheat-sheet",
    "title": "Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "Topic\nKey Points\n\n\n\n\nFile Paths\nUse relative paths (e.g., \"data/file.csv\"). Use ../ to go up one directory level. Always use projects to set working directory automatically.\n\n\nData Import\nread_csv(), read_excel(), read_delim() for different file types. Use col_types to specify column types.\n\n\nTibbles\nModern data frames with better printing and subsetting. Create with tibble() or tribble(). Convert with as_tibble().\n\n\nPipe Operator\n|&gt; or %&gt;% chains operations together. Shortcut: Cmd/Ctrl + Shift + M.\n\n\nTidy Data\nEach variable in a column, each observation in a row, each value in a cell.\n\n\nDPLYR Functions\nfilter(): rows by condition; select(): columns; mutate(): create/transform; summarize(): aggregate; group_by(): group operations; arrange(): sort; rename(): rename columns.\n\n\nData Reshaping\npivot_longer(): wide ‚Üí long; pivot_wider(): long ‚Üí wide.\n\n\nSplit/Combine\nseparate(): split column into multiple; unite(): combine columns into one.\n\n\nMissing Values\ndrop_na(): remove NA rows; fill(): fill NA with adjacent values; replace_na(): replace NA with specific value.\n\n\nExpand Tables\nexpand(): all combinations; complete(): add missing combinations with NA.\n\n\nJoins\nleft_join(): keep all left; right_join(): keep all right; inner_join(): only matches; full_join(): keep all; anti_join(): non-matches.\n\n\nRegex Basics\n. any char; ^ start; $ end; * zero+; + one+; ? zero/one; [] character class; \\\\d digit; \\\\w word char; \\\\s whitespace.\n\n\nstringr Functions\nstr_detect(): check pattern; str_extract(): get match; str_replace(): replace match; str_match(): extract groups; str_subset(): filter by pattern."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html",
    "title": "Computational Research",
    "section": "",
    "text": "Dr.¬†Ayse D. Lokmanoglu Lecture 4, (B) Feb 11, (A) Feb 17\n\n\n\n\n\nSection\nTopic\n\n\n\n\n1\nControl Structures\n\n\n1.1\nif-else Statements\n\n\n1.2\nfor Loops\n\n\n1.3\nLooping Over Vectors\n\n\n1.4\nNested Loops\n\n\n1.5\nwhile Loops\n\n\n1.6\nbreak and next\n\n\n2\nFunctions\n\n\n2.1\nWhy Write Functions?\n\n\n2.2\nCreating Your First Function\n\n\n2.3\nFunction Arguments\n\n\n2.4\nDefault Arguments\n\n\n2.5\nReturn Values\n\n\n2.6\nArgument Matching\n\n\n3\nText Analysis with TidyText\n\n\n3.1\nIntroduction to TidyText\n\n\n3.2\nTokenizing Text with unnest_tokens()\n\n\n3.3\nRemoving Stopwords\n\n\n4\nKeyword in Context (KWIC)\n\n\n4.1\nWhat is KWIC?\n\n\n4.2\nExtracting Keywords\n\n\n4.3\nExtracting Surrounding Context\n\n\n5\nDocument-Term Matrix (DTM)\n\n\n5.1\nWhat is a DTM?\n\n\n5.2\nFrom Text to DTM: The Pipeline\n\n\n5.3\nCreating the DTM\n\n\n5.4\nVisualizing the DTM as a Heatmap\n\n\n5.5\nVisualizing Top Words per Document\n\n\n5.6\nConverting DTM Back to Tidy Format\n\n\n6\nTF-IDF\n\n\n6.1\nUnderstanding TF-IDF\n\n\n6.2\nCalculating TF-IDF\n\n\n6.3\nVisualizing TF-IDF\n\n\n7\nWord Clouds\n\n\n7.1\nCreating Word Clouds\n\n\n7.2\nCustomizing Word Clouds\n\n\n\n\nALWAYS load our libraries first\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(ggplot2)\n\n\n\n\nControl structures allow you to control the flow of execution in your R code. Instead of running the same code every time, you can add logic to respond to different inputs or data conditions.\nCommon control structures include:\n\n\n\nStructure\nPurpose\n\n\n\n\nif, else\nTest a condition and act on it\n\n\nfor\nExecute a loop a fixed number of times\n\n\nwhile\nExecute a loop while a condition is true\n\n\nbreak\nExit a loop immediately\n\n\nnext\nSkip to the next iteration of a loop\n\n\n\n\n\n\nThe if-else combination tests a condition and executes different code depending on whether it‚Äôs TRUE or FALSE.\nBasic if statement:\nx &lt;- 7\n\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n}\n## [1] \"x is greater than 5\"\nif-else statement:\nx &lt;- 3\n\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n} else {\n  print(\"x is NOT greater than 5\")\n}\n## [1] \"x is NOT greater than 5\"\nMultiple conditions with else if:\nx &lt;- 5\n\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n} else if (x == 5) {\n  print(\"x is exactly 5\")\n} else {\n  print(\"x is less than 5\")\n}\n## [1] \"x is exactly 5\"\nCompact if-else (single line):\nx &lt;- 8\ny &lt;- if (x &gt; 5) \"big\" else \"small\"\nprint(y)\n## [1] \"big\"\n\n\n\n\nfor loops iterate over elements in a sequence (vector, list, etc.) and execute code for each element.\nBasic for loop:\nfor (i in 1:5) {\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\nLooping over a character vector:\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\")\n\nfor (fruit in fruits) {\n  print(paste(\"I like\", fruit))\n}\n## [1] \"I like apple\"\n## [1] \"I like banana\"\n## [1] \"I like cherry\"\n\n\n\n\nThere are multiple ways to loop over vectors. The seq_along() function is particularly useful:\ncolors &lt;- c(\"red\", \"green\", \"blue\", \"yellow\")\n\n# Method 1: Loop over indices\nfor (i in 1:length(colors)) {\n  print(paste(\"Color\", i, \"is\", colors[i]))\n}\n## [1] \"Color 1 is red\"\n## [1] \"Color 2 is green\"\n## [1] \"Color 3 is blue\"\n## [1] \"Color 4 is yellow\"\n# Method 2: Using seq_along() - safer!\nfor (i in seq_along(colors)) {\n  print(paste(\"Color\", i, \"is\", colors[i]))\n}\n## [1] \"Color 1 is red\"\n## [1] \"Color 2 is green\"\n## [1] \"Color 3 is blue\"\n## [1] \"Color 4 is yellow\"\n# Method 3: Loop directly over elements\nfor (color in colors) {\n  print(color)\n}\n## [1] \"red\"\n## [1] \"green\"\n## [1] \"blue\"\n## [1] \"yellow\"\nWhy use seq_along()? It‚Äôs safer because if the vector is empty, 1:length(x) would give 1:0 which creates c(1, 0), but seq_along(x) correctly returns an empty sequence.\n\n\n\n\nLoops can be nested inside each other. This is useful for working with matrices or multidimensional data:\n# Create a 3x3 matrix\nmat &lt;- matrix(1:9, nrow = 3, ncol = 3)\nprint(mat)\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n# Loop through rows and columns\nfor (i in 1:nrow(mat)) {\n  for (j in 1:ncol(mat)) {\n    print(paste(\"Row\", i, \"Col\", j, \"=\", mat[i, j]))\n  }\n}\n## [1] \"Row 1 Col 1 = 1\"\n## [1] \"Row 1 Col 2 = 4\"\n## [1] \"Row 1 Col 3 = 7\"\n## [1] \"Row 2 Col 1 = 2\"\n## [1] \"Row 2 Col 2 = 5\"\n## [1] \"Row 2 Col 3 = 8\"\n## [1] \"Row 3 Col 1 = 3\"\n## [1] \"Row 3 Col 2 = 6\"\n## [1] \"Row 3 Col 3 = 9\"\nWarning: Avoid nesting more than 2-3 levels deep. If you need more, consider using functions to break up the code.\n\n\n\n\nwhile loops execute as long as a condition is TRUE:\ncount &lt;- 1\n\nwhile (count &lt;= 5) {\n  print(paste(\"Count is:\", count))\n  count &lt;- count + 1\n}\n## [1] \"Count is: 1\"\n## [1] \"Count is: 2\"\n## [1] \"Count is: 3\"\n## [1] \"Count is: 4\"\n## [1] \"Count is: 5\"\nCaution: while loops can run forever if the condition never becomes FALSE. Always make sure your loop has a way to exit!\nExample with multiple conditions:\nset.seed(123)  # For reproducibility\nvalue &lt;- 5\n\nwhile (value &gt;= 2 && value &lt;= 8) {\n  # Random walk: add or subtract 1\n  coin &lt;- sample(c(-1, 1), 1)\n  value &lt;- value + coin\n  print(paste(\"Value is now:\", value))\n}\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 2\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 2\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 6\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 6\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 6\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 2\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 2\"\n## [1] \"Value is now: 1\"\n\n\n\n\n\nbreak: Exit the loop immediately\nnext: Skip the current iteration and continue to the next\n\nUsing break:\nfor (i in 1:10) {\n  if (i &gt; 5) {\n    print(\"Breaking out of loop!\")\n    break\n  }\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n## [1] \"Breaking out of loop!\"\nUsing next:\n# Print only odd numbers\nfor (i in 1:10) {\n  if (i %% 2 == 0) {  # If i is even, skip it\n    next\n  }\n  print(i)\n}\n## [1] 1\n## [1] 3\n## [1] 5\n## [1] 7\n## [1] 9\n\n\n\n\nWrite an if-else statement that checks if a number is positive, negative, or zero.\nCreate a for loop that prints the squares of numbers 1 through 10.\nWrite a while loop that starts at 100 and divides by 2 until the value is less than 1.\nUse a for loop with next to print only numbers divisible by 3 from 1 to 20.\n\n### Your workspace\n\n\n\n\n\n\nFunctions allow you to encapsulate code that you want to reuse. Instead of copying and pasting code, you write it once as a function and call it whenever needed.\n\n\n\n\nReusability: Write code once, use it many times\nReadability: Give meaningful names to complex operations\nMaintainability: Fix bugs in one place instead of many\nAbstraction: Hide implementation details from users\n\nRule of thumb: If you find yourself copying and pasting code more than twice, write a function!\n\n\n\n\nFunctions are created using the function() keyword:\n# A simple function that prints a greeting\nsay_hello &lt;- function() {\n  print(\"Hello, world!\")\n}\n\n# Call the function\nsay_hello()\n## [1] \"Hello, world!\"\nA function with a body that does computation:\n# Function to calculate the area of a circle\ncircle_area &lt;- function(radius) {\n  area &lt;- pi * radius^2\n  return(area)\n}\n\n# Use the function\ncircle_area(5)\n## [1] 78.53982\ncircle_area(10)\n## [1] 314.1593\n\n\n\n\nArguments are the inputs to your function. They let users customize the function‚Äôs behavior:\n# Function with multiple arguments\ngreet_person &lt;- function(name, greeting) {\n  message &lt;- paste(greeting, name)\n  print(message)\n}\n\ngreet_person(\"Alice\", \"Hello\")\n## [1] \"Hello Alice\"\ngreet_person(\"Bob\", \"Good morning\")\n## [1] \"Good morning Bob\"\n\n\n\n\nYou can set default values for arguments. This makes the function easier to use for common cases:\n# Function with default argument\ngreet_person &lt;- function(name, greeting = \"Hello\") {\n  message &lt;- paste(greeting, name)\n  print(message)\n}\n\n# Using default\ngreet_person(\"Alice\")\n## [1] \"Hello Alice\"\n# Overriding default\ngreet_person(\"Bob\", \"Good evening\")\n## [1] \"Good evening Bob\"\nAnother example:\n# Function to repeat a message\nrepeat_message &lt;- function(msg, times = 3) {\n  for (i in seq_len(times)) {\n    print(msg)\n  }\n}\n\nrepeat_message(\"R is fun!\")\n## [1] \"R is fun!\"\n## [1] \"R is fun!\"\n## [1] \"R is fun!\"\nrepeat_message(\"Learning loops!\", times = 2)\n## [1] \"Learning loops!\"\n## [1] \"Learning loops!\"\n\n\n\n\nFunctions return the last expression evaluated, or you can use return() explicitly:\n# Implicit return (last expression)\nadd_numbers &lt;- function(a, b) {\n  a + b  # This is returned\n}\n\nresult &lt;- add_numbers(3, 5)\nprint(result)\n## [1] 8\n# Explicit return\ncalculate_stats &lt;- function(numbers) {\n  if (length(numbers) == 0) {\n    return(NULL)  # Early return for edge case\n  }\n  \n  stats &lt;- list(\n    mean = mean(numbers),\n    sd = sd(numbers),\n    min = min(numbers),\n    max = max(numbers)\n  )\n  \n  return(stats)\n}\n\nmy_stats &lt;- calculate_stats(c(10, 20, 30, 40, 50))\nprint(my_stats)\n## $mean\n## [1] 30\n## \n## $sd\n## [1] 15.81139\n## \n## $min\n## [1] 10\n## \n## $max\n## [1] 50\n\n\n\n\nR matches arguments by position or by name:\n# Define a function\npower_calc &lt;- function(base, exponent) {\n  base^exponent\n}\n\n# Positional matching\npower_calc(2, 3)  # 2^3 = 8\n## [1] 8\n# Named matching\npower_calc(exponent = 3, base = 2)  # Same result\n## [1] 8\n# Mixed matching\npower_calc(2, exponent = 3)  # Same result\n## [1] 8\nTip: For functions with many arguments, use named arguments for clarity!\n\n\n\n\nWrite a function called fahrenheit_to_celsius that converts temperature from Fahrenheit to Celsius. Formula: C = (F - 32) * 5/9\nWrite a function called word_count that takes a text string and returns the number of words.\nWrite a function that takes a vector of numbers and returns a named list with the sum, mean, and length.\nModify your word_count function to have a default argument remove_punct = TRUE that removes punctuation before counting.\n\n### Your workspace\n\n\n\n\n\n\nNow let‚Äôs apply what we‚Äôve learned to text analysis! The tidytext package provides tools for working with text in a tidy data format.\n\n\n\nTidy text format means having one token per row. A token can be:\n\nA word\nA sentence\nAn n-gram (sequence of n words)\nA paragraph\n\nThis format works seamlessly with tidyverse tools like dplyr and ggplot2.\nLoad the Jane Austen books dataset:\n# install.packages(\"janeaustenr\")\nlibrary(janeaustenr)\n\n# Combine all books into a single dataframe\noriginal_books &lt;- austen_books() |&gt; \n  group_by(book) |&gt; \n  mutate(\n    linenumber = row_number(),\n    chapter = cumsum(str_detect(text, \n                                regex(\"^chapter [\\\\divxlc]\",\n                                      ignore_case = TRUE)))\n  ) |&gt; \n  ungroup()\n\nhead(original_books)\n## # A tibble: 6 √ó 4\n##   text                    book                linenumber chapter\n##   &lt;chr&gt;                   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt;\n## 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility          1       0\n## 2 \"\"                      Sense & Sensibility          2       0\n## 3 \"by Jane Austen\"        Sense & Sensibility          3       0\n## 4 \"\"                      Sense & Sensibility          4       0\n## 5 \"(1811)\"                Sense & Sensibility          5       0\n## 6 \"\"                      Sense & Sensibility          6       0\n\n\n\n\nunnest_tokens() breaks text into individual tokens (usually words):\nSyntax:\nunnest_tokens(tbl, output, input, token = \"words\", ...)\n\n\n\nArgument\nDescription\n\n\n\n\ntbl\nThe data frame\n\n\noutput\nName of the new column for tokens\n\n\ninput\nName of the column containing text\n\n\ntoken\nType: ‚Äúwords‚Äù, ‚Äúsentences‚Äù, ‚Äúngrams‚Äù, etc.\n\n\n\nTokenize the Jane Austen books:\ntidy_books &lt;- original_books |&gt; \n  unnest_tokens(word, text)\n\nhead(tidy_books)\n## # A tibble: 6 √ó 4\n##   book                linenumber chapter word       \n##   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      \n## 1 Sense & Sensibility          1       0 sense      \n## 2 Sense & Sensibility          1       0 and        \n## 3 Sense & Sensibility          1       0 sensibility\n## 4 Sense & Sensibility          3       0 by         \n## 5 Sense & Sensibility          3       0 jane       \n## 6 Sense & Sensibility          3       0 austen\nNotice that: - Punctuation is removed - Text is converted to lowercase - Each word is now its own row\nCount words:\ntidy_books |&gt; \n  count(word, sort = TRUE)\n## # A tibble: 14,520 √ó 2\n##    word      n\n##    &lt;chr&gt; &lt;int&gt;\n##  1 the   26351\n##  2 to    24044\n##  3 and   22515\n##  4 of    21178\n##  5 a     13408\n##  6 her   13055\n##  7 i     12006\n##  8 in    11217\n##  9 was   11204\n## 10 it    10234\n## # ‚Ñπ 14,510 more rows\n\n\n\n\nStopwords are common words like ‚Äúthe‚Äù, ‚Äúand‚Äù, ‚Äúof‚Äù that don‚Äôt carry much meaning. The tidytext package includes a stopwords dataset:\n# View stopwords\nhead(stop_words)\n## # A tibble: 6 √ó 2\n##   word      lexicon\n##   &lt;chr&gt;     &lt;chr&gt;  \n## 1 a         SMART  \n## 2 a's       SMART  \n## 3 able      SMART  \n## 4 about     SMART  \n## 5 above     SMART  \n## 6 according SMART\n# How many stopwords?\nnrow(stop_words)\n## [1] 1149\nRemove stopwords using anti_join():\ntidy_books_clean &lt;- tidy_books |&gt; \n  anti_join(stop_words, by = \"word\")\n\n# Compare counts\nnrow(tidy_books)        # Before\n## [1] 725055\nnrow(tidy_books_clean)  # After\n## [1] 217609\n# Most common words without stopwords\ntidy_books_clean |&gt; \n  count(word, sort = TRUE) |&gt; \n  head(15)\n## # A tibble: 15 √ó 2\n##    word          n\n##    &lt;chr&gt;     &lt;int&gt;\n##  1 miss       1855\n##  2 time       1337\n##  3 fanny       862\n##  4 dear        822\n##  5 lady        817\n##  6 sir         806\n##  7 day         797\n##  8 emma        787\n##  9 sister      727\n## 10 house       699\n## 11 elizabeth   687\n## 12 elinor      623\n## 13 hope        601\n## 14 friend      593\n## 15 family      578\n\n\n\nLoad the Starbucks Twitter data and practice tokenization:\nlibrary(readr)\n\nurl &lt;- \"https://raw.githubusercontent.com/aysedeniz09/Social_Media_Listening/refs/heads/main/MSC_social_media_list_data/Starbucks_User_Data.csv\"\nstarbucks_data &lt;- read_csv(url)\n\nhead(starbucks_data)\n## # A tibble: 6 √ó 16\n##   author_id conversation_id created_at          hashtag lang  like_count mention\n##       &lt;dbl&gt;           &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;  \n## 1     30973         1.61e18 2022-12-27 15:43:16 &lt;NA&gt;    en            10 &lt;NA&gt;   \n## 2     30973         1.60e18 2022-11-29 05:23:55 &lt;NA&gt;    en             9 Mo_sha‚Ä¶\n## 3     30973         1.59e18 2022-11-28 20:14:09 &lt;NA&gt;    en             2 Mixxed‚Ä¶\n## 4     30973         1.60e18 2022-11-28 12:51:28 &lt;NA&gt;    en             0 BihhKa‚Ä¶\n## 5     30973         1.60e18 2022-11-27 15:14:26 &lt;NA&gt;    en             0 BihhKa‚Ä¶\n## 6     30973         1.60e18 2022-11-24 17:47:24 &lt;NA&gt;    en             1 therea‚Ä¶\n## # ‚Ñπ 9 more variables: quote_count &lt;dbl&gt;, referenced_status_id &lt;dbl&gt;,\n## #   referenced_user_id &lt;dbl&gt;, reply_count &lt;dbl&gt;, retweet_count &lt;dbl&gt;,\n## #   row_id &lt;dbl&gt;, status_id &lt;dbl&gt;, text &lt;chr&gt;, type &lt;chr&gt;\n\nTokenize the text column into words.\nRemove stopwords.\nFind the 20 most common words.\nHow many total words are there before and after removing stopwords?\n\n### Your workspace\n\n\n\n\n\n\n\n\nKeyword in Context (KWIC) extracts and analyzes how specific keywords appear in text along with their surrounding context.\nWhy use KWIC?\n\nIdentify patterns in how words are used\nUnderstand the context around specific terms\nUseful for sentiment analysis and content understanding\n\n\n\n\n\nUse str_detect() to filter text containing a keyword:\n# Find all lines containing \"family\"\nfiltered_text &lt;- original_books |&gt; \n  filter(str_detect(text, \"family\"))\n\nhead(filtered_text)\n## # A tibble: 6 √ó 4\n##   text                                                  book  linenumber chapter\n##   &lt;chr&gt;                                                 &lt;fct&gt;      &lt;int&gt;   &lt;int&gt;\n## 1 The family of Dashwood had long been settled in Suss‚Ä¶ Sens‚Ä¶         13       1\n## 2 into his house the family of his nephew Mr. Henry Da‚Ä¶ Sens‚Ä¶         22       1\n## 3 family; but he was affected by a recommendation of s‚Ä¶ Sens‚Ä¶         79       1\n## 4 any of her husband's family; but she had had no oppo‚Ä¶ Sens‚Ä¶        116       1\n## 5 large a sum was parted with.  If he should have a nu‚Ä¶ Sens‚Ä¶        223       2\n## 6 of her character, which half a year's residence in h‚Ä¶ Sens‚Ä¶        398       3\nnrow(filtered_text)\n## [1] 572\n\n\n\n\nUse regex to extract characters before and after a keyword:\n# Extract 15 characters before and after \"family\"\ncontext &lt;- original_books |&gt;\n  filter(str_detect(text, \"family\")) |&gt;\n  mutate(context = str_extract(text, \".{0,15}family.{0,15}\"))\n\nhead(context$context, 10)\n##  [1] \"The family of Dashwood ha\"           \n##  [2] \" his house the family of his nephew \"\n##  [3] \"family; but he was af\"               \n##  [4] \" her husband's family; but she had h\"\n##  [5] \"ave a numerous family, for\"          \n##  [6] \"sidence in her family afforded;\"     \n##  [7] \" small for our family,\"              \n##  [8] \"terms with his family, and pressed\"  \n##  [9] \"than any other family in the neighbo\"\n## [10] \"rival of a new family in the country\"\nCreate a function for KWIC:\n# KWIC function\nkwic &lt;- function(data, text_col, keyword, window = 10) {\n  pattern &lt;- paste0(\".{0,\", window, \"}\", keyword, \".{0,\", window, \"}\")\n  \n  data |&gt;\n    filter(str_detect({{ text_col }}, keyword)) |&gt;\n    mutate(context = str_extract({{ text_col }}, pattern)) |&gt;\n    select(context)\n}\n\n# Use the function\nkwic(original_books, text, \"love\", window = 20) |&gt; \n  head(10)\n## # A tibble: 10 √ó 1\n##    context                                       \n##    &lt;chr&gt;                                         \n##  1 \" and her own tender love for all her three c\"\n##  2 \"e you are right, my love; it will be better \"\n##  3 \"ove far from that beloved spot was impossibl\"\n##  4 \"that he loved her daughter, and \"            \n##  5 \"rything amiable.  I love him already.\\\"\"     \n##  6 \"obation inferior to love.\\\"\"                 \n##  7 \"separate esteem and love.\\\"\"                 \n##  8 \"eive any symptom of love in his behaviour to\"\n##  9 \"\\\"My love, it will be scarcel\"               \n## 10 \"very amiable, and I love him tenderly.  But \"\n\n\n\nUsing the Starbucks data:\n\nFind all tweets containing ‚Äúcoffee‚Äù.\nExtract 20 characters of context around ‚Äúcoffee‚Äù.\nFind tweets containing mentions (@username) using regex.\nCreate a KWIC analysis for the word ‚ÄúStarbucks‚Äù.\n\n### Your workspace\n\n\n\n\n\n\n\n\nA Document-Term Matrix (DTM) is a mathematical representation of text where:\n\nRows represent documents (e.g., books, tweets, articles)\nColumns represent terms (words)\nValues indicate the frequency of each term in each document\n\nExample DTM:\n\n\n\n\nlove\nfamily\nmoney\nmarriage\nhappy\n\n\n\n\nBook 1\n45\n23\n12\n67\n34\n\n\nBook 2\n32\n45\n56\n23\n12\n\n\nBook 3\n67\n12\n8\n89\n45\n\n\n\nNote: A Term-Document Matrix (TDM) is simply the transpose - terms as rows, documents as columns. In R‚Äôs tidytext, we typically create DTMs.\n\n\n\n\nThe transformation from raw text to DTM follows these steps:\nRaw Text ‚Üí Tokenize ‚Üí Remove Stopwords ‚Üí Count ‚Üí DTM\nStep-by-step visualization:\n# STEP 1: Start with raw text\noriginal_books |&gt; \n  select(book, text) |&gt; \n  head(3)\n## # A tibble: 3 √ó 2\n##   book                text                   \n##   &lt;fct&gt;               &lt;chr&gt;                  \n## 1 Sense & Sensibility \"SENSE AND SENSIBILITY\"\n## 2 Sense & Sensibility \"\"                     \n## 3 Sense & Sensibility \"by Jane Austen\"\n# STEP 2: Tokenize (one word per row)\ntokenized &lt;- original_books |&gt; \n  unnest_tokens(word, text)\n\ntokenized |&gt; \n  select(book, word) |&gt; \n  head(10)\n## # A tibble: 10 √ó 2\n##    book                word       \n##    &lt;fct&gt;               &lt;chr&gt;      \n##  1 Sense & Sensibility sense      \n##  2 Sense & Sensibility and        \n##  3 Sense & Sensibility sensibility\n##  4 Sense & Sensibility by         \n##  5 Sense & Sensibility jane       \n##  6 Sense & Sensibility austen     \n##  7 Sense & Sensibility 1811       \n##  8 Sense & Sensibility chapter    \n##  9 Sense & Sensibility 1          \n## 10 Sense & Sensibility the\n# STEP 3: Remove stopwords\ncleaned &lt;- tokenized |&gt; \n  anti_join(stop_words, by = \"word\")\n\ncleaned |&gt; \n  select(book, word) |&gt; \n  head(10)\n## # A tibble: 10 √ó 2\n##    book                word       \n##    &lt;fct&gt;               &lt;chr&gt;      \n##  1 Sense & Sensibility sense      \n##  2 Sense & Sensibility sensibility\n##  3 Sense & Sensibility jane       \n##  4 Sense & Sensibility austen     \n##  5 Sense & Sensibility 1811       \n##  6 Sense & Sensibility chapter    \n##  7 Sense & Sensibility 1          \n##  8 Sense & Sensibility family     \n##  9 Sense & Sensibility dashwood   \n## 10 Sense & Sensibility settled\n# STEP 4: Count words per document\nword_counts &lt;- cleaned |&gt; \n  count(book, word, sort = TRUE)\n\nhead(word_counts, 10)\n## # A tibble: 10 √ó 3\n##    book                word          n\n##    &lt;fct&gt;               &lt;chr&gt;     &lt;int&gt;\n##  1 Mansfield Park      fanny       816\n##  2 Emma                emma        786\n##  3 Sense & Sensibility elinor      623\n##  4 Emma                miss        599\n##  5 Pride & Prejudice   elizabeth   597\n##  6 Mansfield Park      crawford    493\n##  7 Sense & Sensibility marianne    492\n##  8 Persuasion          anne        447\n##  9 Mansfield Park      miss        432\n## 10 Northanger Abbey    catherine   428\n\n\n\n\nUse cast_dtm() to convert tidy word counts to a DTM:\n# Create DTM\nbook_dtm &lt;- word_counts |&gt; \n  cast_dtm(document = book, term = word, value = n)\n\n# Inspect the DTM\nbook_dtm\n## &lt;&lt;DocumentTermMatrix (documents: 6, terms: 13914)&gt;&gt;\n## Non-/sparse entries: 37224/46260\n## Sparsity           : 55%\n## Maximal term length: 19\n## Weighting          : term frequency (tf)\n# View dimensions: documents x terms\ndim(book_dtm)\n## [1]     6 13914\nUnderstanding the output: - 6 documents (the 6 Jane Austen books) - 13,914 terms (unique words across all books) - 99% sparse means 99% of the cells are zeros (most words don‚Äôt appear in most books)\n\n\n\n\nLet‚Äôs visualize a small portion of the DTM to understand its structure:\n# Get top 10 words overall\ntop_10_words &lt;- word_counts |&gt; \n  group_by(word) |&gt; \n  summarize(total = sum(n)) |&gt; \n  slice_max(total, n = 10) |&gt; \n  pull(word)\n\n# Filter to just these words\ndtm_subset &lt;- word_counts |&gt; \n  filter(word %in% top_10_words)\n\n# Create heatmap\nggplot(dtm_subset, aes(x = word, y = book, fill = n)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = n), color = \"white\", size = 3) +\n  scale_fill_gradient(low = \"steelblue\", high = \"darkred\") +\n  labs(\n    title = \"Document-Term Matrix Heatmap\",\n    subtitle = \"Top 10 words across Jane Austen books\",\n    x = \"Term\",\n    y = \"Document\",\n    fill = \"Frequency\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nReading the heatmap: - Darker colors = higher frequency - Each row is a book (document) - Each column is a word (term) - The numbers show exact word counts\n\n\n\n\nTop words per book:\ntop_words &lt;- word_counts |&gt; \n  group_by(book) |&gt; \n  slice_max(n, n = 5) |&gt; \n  ungroup()\n\nhead(top_words, 12)\n## # A tibble: 12 √ó 3\n##    book                word          n\n##    &lt;fct&gt;               &lt;chr&gt;     &lt;int&gt;\n##  1 Sense & Sensibility elinor      623\n##  2 Sense & Sensibility marianne    492\n##  3 Sense & Sensibility time        239\n##  4 Sense & Sensibility dashwood    231\n##  5 Sense & Sensibility sister      229\n##  6 Pride & Prejudice   elizabeth   597\n##  7 Pride & Prejudice   darcy       373\n##  8 Pride & Prejudice   bennet      294\n##  9 Pride & Prejudice   miss        283\n## 10 Pride & Prejudice   jane        264\n## 11 Mansfield Park      fanny       816\n## 12 Mansfield Park      crawford    493\nBar chart visualization:\nggplot(top_words, aes(x = reorder(word, n), y = n, fill = book)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  facet_wrap(~ book, scales = \"free_y\") +\n  labs(\n    title = \"Top 5 Words in Each Jane Austen Book\",\n    x = \"Word\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nYou can convert a DTM back to tidy format using tidy():\n# Convert DTM back to tidy\ntidy_dtm &lt;- tidy(book_dtm)\n\nhead(tidy_dtm)\n## # A tibble: 6 √ó 3\n##   document            term   count\n##   &lt;chr&gt;               &lt;chr&gt;  &lt;dbl&gt;\n## 1 Sense & Sensibility fanny     42\n## 2 Mansfield Park      fanny    816\n## 3 Persuasion          fanny      4\n## 4 Emma                emma     786\n## 5 Persuasion          emma       1\n## 6 Sense & Sensibility elinor   623\nThis is useful when you receive a DTM from another package and want to use tidyverse tools.\n\n\n\n\n\n\n\nTF-IDF (Term Frequency - Inverse Document Frequency) measures how important a word is to a document within a collection.\n\nTF (Term Frequency): How often a term appears in a document\nIDF (Inverse Document Frequency): How rare a term is across all documents\n\n\\[\\text{TF-IDF} = \\text{TF} \\times \\log\\left(\\frac{\\text{Total Documents}}{\\text{Documents containing term}}\\right)\\]\nInterpretation:\n\nHigh TF-IDF: Word is frequent in this document but rare overall ‚Üí important/distinctive\nLow TF-IDF: Word is common everywhere ‚Üí less distinctive\n\n\n\n\n\nUse bind_tf_idf() from tidytext:\n# Calculate TF-IDF\nbook_tfidf &lt;- original_books |&gt; \n  unnest_tokens(word, text) |&gt; \n  count(book, word, sort = TRUE) |&gt; \n  bind_tf_idf(word, book, n)\n\nhead(book_tfidf)\n## # A tibble: 6 √ó 6\n##   book           word      n     tf   idf tf_idf\n##   &lt;fct&gt;          &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n## 1 Mansfield Park the    6206 0.0387     0      0\n## 2 Mansfield Park to     5475 0.0341     0      0\n## 3 Mansfield Park and    5438 0.0339     0      0\n## 4 Emma           to     5239 0.0325     0      0\n## 5 Emma           the    5201 0.0323     0      0\n## 6 Emma           and    4896 0.0304     0      0\nFind distinctive words for each book:\n# Top TF-IDF words per book\ntop_tfidf &lt;- book_tfidf |&gt; \n  group_by(book) |&gt; \n  slice_max(tf_idf, n = 5) |&gt; \n  ungroup()\n\nhead(top_tfidf, 12)\n## # A tibble: 12 √ó 6\n##    book                word           n      tf   idf  tf_idf\n##    &lt;fct&gt;               &lt;chr&gt;      &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n##  1 Sense & Sensibility elinor       623 0.00519 1.79  0.00931\n##  2 Sense & Sensibility marianne     492 0.00410 1.79  0.00735\n##  3 Sense & Sensibility dashwood     231 0.00193 1.79  0.00345\n##  4 Sense & Sensibility jennings     199 0.00166 1.79  0.00297\n##  5 Sense & Sensibility willoughby   181 0.00151 1.79  0.00270\n##  6 Pride & Prejudice   darcy        373 0.00305 1.79  0.00547\n##  7 Pride & Prejudice   bennet       294 0.00241 1.79  0.00431\n##  8 Pride & Prejudice   bingley      257 0.00210 1.79  0.00377\n##  9 Pride & Prejudice   elizabeth    597 0.00489 0.693 0.00339\n## 10 Pride & Prejudice   wickham      162 0.00133 1.79  0.00238\n## 11 Mansfield Park      crawford     493 0.00307 1.79  0.00551\n## 12 Mansfield Park      edmund       364 0.00227 1.79  0.00406\n\n\n\n\nggplot(top_tfidf, aes(x = reorder(word, tf_idf), y = tf_idf, fill = book)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  facet_wrap(~ book, scales = \"free\") +\n  labs(\n    title = \"Most Distinctive Words by TF-IDF\",\n    x = \"Word\",\n    y = \"TF-IDF Score\"\n  ) +\n  theme_minimal()\n\nNotice that TF-IDF highlights character names and distinctive terms for each book, rather than common words!\n\n\n\nUsing the Starbucks data:\n\nCreate word counts grouped by mention (the user being replied to).\nCalculate TF-IDF scores.\nFind the top 5 distinctive words for the 3 most active mentions.\nVisualize the results.\n\n### Your workspace\n\n\n\n\n\n\n\n\nWord clouds display words with size proportional to their frequency. Install the wordcloud2 package:\ninstall.packages(\"wordcloud2\")\nlibrary(wordcloud2)\n\n# Prepare word frequencies\nword_freq &lt;- original_books |&gt;\n  unnest_tokens(word, text) |&gt;\n  anti_join(stop_words, by = \"word\") |&gt;\n  count(word, sort = TRUE)\n\nhead(word_freq)\n## # A tibble: 6 √ó 2\n##   word      n\n##   &lt;chr&gt; &lt;int&gt;\n## 1 miss   1855\n## 2 time   1337\n## 3 fanny   862\n## 4 dear    822\n## 5 lady    817\n## 6 sir     806\nCreate a basic word cloud:\nwordcloud2(data = word_freq, size = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nDescription\nExample\n\n\n\n\nsize\nScale factor for word sizes\nsize = 0.5\n\n\ncolor\nColor scheme\ncolor = \"random-light\"\n\n\nbackgroundColor\nBackground color\nbackgroundColor = \"black\"\n\n\nshape\nShape of cloud\nshape = \"circle\" or \"star\"\n\n\nminRotation, maxRotation\nWord rotation angles\nminRotation = -pi/4\n\n\n\nCustom colors:\nwordcloud2(data = word_freq, size = 0.5, color = \"random-light\")\n\n\n\n\nChange shape and background:\nwordcloud2(data = word_freq, size = 0.4, shape = \"star\", backgroundColor = \"black\", color = \"random-light\")\n\n\n\n\nWith rotation:\nwordcloud2(data = word_freq, size = 0.5, \n           minRotation = -pi/4, maxRotation = pi/4, rotateRatio = 0.3)\n\n\n\n\n\n\n\nlibrary(htmlwidgets)\nlibrary(webshot)\nwebshot::install_phantomjs()\n\n# Create word cloud\nmy_cloud &lt;- wordcloud2(word_freq, size = 1)\n\n# Save as HTML\nsaveWidget(my_cloud, \"wordcloud.html\", selfcontained = FALSE)\n\n# Save as image\nwebshot(\"wordcloud.html\", \"wordcloud.png\", delay = 5)\n\n\n\n\n\n\n\n\nUsing either the Jane Austen or Starbucks dataset:\n\nWrite a function that takes a data frame and text column, then returns:\n\nTotal word count (after removing stopwords)\nTop 10 most frequent words\nA word cloud object\n\nUse a loop to analyze each book (or group) separately and store the results in a list.\nCreate a KWIC analysis for a keyword of your choice.\nCalculate TF-IDF and identify what makes each document/group distinctive.\nVisualize your findings with at least two different plot types.\n\n### Your workspace\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic\nDescription\nCode Example\n\n\n\n\nif-else\nConditional execution\nif (x &gt; 5) { ... } else { ... }\n\n\nfor loop\nFixed iterations\nfor (i in 1:10) { print(i) }\n\n\nwhile loop\nConditional iterations\nwhile (x &lt; 10) { x &lt;- x + 1 }\n\n\nseq_along()\nSafe sequence for loops\nfor (i in seq_along(vec)) { ... }\n\n\nbreak\nExit loop\nif (cond) break\n\n\nnext\nSkip iteration\nif (cond) next\n\n\nfunction()\nCreate function\nmy_func &lt;- function(arg) { ... }\n\n\nDefault arguments\nSet defaults\nfunction(x, y = 10) { ... }\n\n\nreturn()\nExplicit return\nreturn(result)\n\n\nunnest_tokens()\nTokenize text\nunnest_tokens(word, text)\n\n\nanti_join(stop_words)\nRemove stopwords\ndata |&gt; anti_join(stop_words)\n\n\nstr_detect()\nFind pattern\nfilter(str_detect(text, \"word\"))\n\n\nstr_extract()\nExtract pattern\nmutate(x = str_extract(text, \".{10}word.{10}\"))\n\n\nKWIC\nKeyword in context\nExtract surrounding text for keywords\n\n\ncast_dtm()\nCreate DTM from tidy\ncast_dtm(document, term, value)\n\n\ntidy()\nConvert DTM to tidy\ntidy(dtm_object)\n\n\ngeom_tile()\nCreate heatmap\ngeom_tile(aes(x, y, fill = value))\n\n\nbind_tf_idf()\nCalculate TF-IDF\nbind_tf_idf(word, document, n)\n\n\nwordcloud2()\nCreate word cloud\nwordcloud2(data = freq_df, size = 0.5)\n\n\ncount()\nCount occurrences\ncount(word, sort = TRUE)\n\n\nslice_max()\nTop n by value\nslice_max(n, n = 10)"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#lecture-4-table-of-contents",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#lecture-4-table-of-contents",
    "title": "Computational Research",
    "section": "",
    "text": "Section\nTopic\n\n\n\n\n1\nControl Structures\n\n\n1.1\nif-else Statements\n\n\n1.2\nfor Loops\n\n\n1.3\nLooping Over Vectors\n\n\n1.4\nNested Loops\n\n\n1.5\nwhile Loops\n\n\n1.6\nbreak and next\n\n\n2\nFunctions\n\n\n2.1\nWhy Write Functions?\n\n\n2.2\nCreating Your First Function\n\n\n2.3\nFunction Arguments\n\n\n2.4\nDefault Arguments\n\n\n2.5\nReturn Values\n\n\n2.6\nArgument Matching\n\n\n3\nText Analysis with TidyText\n\n\n3.1\nIntroduction to TidyText\n\n\n3.2\nTokenizing Text with unnest_tokens()\n\n\n3.3\nRemoving Stopwords\n\n\n4\nKeyword in Context (KWIC)\n\n\n4.1\nWhat is KWIC?\n\n\n4.2\nExtracting Keywords\n\n\n4.3\nExtracting Surrounding Context\n\n\n5\nDocument-Term Matrix (DTM)\n\n\n5.1\nWhat is a DTM?\n\n\n5.2\nFrom Text to DTM: The Pipeline\n\n\n5.3\nCreating the DTM\n\n\n5.4\nVisualizing the DTM as a Heatmap\n\n\n5.5\nVisualizing Top Words per Document\n\n\n5.6\nConverting DTM Back to Tidy Format\n\n\n6\nTF-IDF\n\n\n6.1\nUnderstanding TF-IDF\n\n\n6.2\nCalculating TF-IDF\n\n\n6.3\nVisualizing TF-IDF\n\n\n7\nWord Clouds\n\n\n7.1\nCreating Word Clouds\n\n\n7.2\nCustomizing Word Clouds\n\n\n\n\nALWAYS load our libraries first\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(ggplot2)"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#control-structures",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#control-structures",
    "title": "Computational Research",
    "section": "",
    "text": "Control structures allow you to control the flow of execution in your R code. Instead of running the same code every time, you can add logic to respond to different inputs or data conditions.\nCommon control structures include:\n\n\n\nStructure\nPurpose\n\n\n\n\nif, else\nTest a condition and act on it\n\n\nfor\nExecute a loop a fixed number of times\n\n\nwhile\nExecute a loop while a condition is true\n\n\nbreak\nExit a loop immediately\n\n\nnext\nSkip to the next iteration of a loop\n\n\n\n\n\n\nThe if-else combination tests a condition and executes different code depending on whether it‚Äôs TRUE or FALSE.\nBasic if statement:\nx &lt;- 7\n\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n}\n## [1] \"x is greater than 5\"\nif-else statement:\nx &lt;- 3\n\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n} else {\n  print(\"x is NOT greater than 5\")\n}\n## [1] \"x is NOT greater than 5\"\nMultiple conditions with else if:\nx &lt;- 5\n\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n} else if (x == 5) {\n  print(\"x is exactly 5\")\n} else {\n  print(\"x is less than 5\")\n}\n## [1] \"x is exactly 5\"\nCompact if-else (single line):\nx &lt;- 8\ny &lt;- if (x &gt; 5) \"big\" else \"small\"\nprint(y)\n## [1] \"big\"\n\n\n\n\nfor loops iterate over elements in a sequence (vector, list, etc.) and execute code for each element.\nBasic for loop:\nfor (i in 1:5) {\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\nLooping over a character vector:\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\")\n\nfor (fruit in fruits) {\n  print(paste(\"I like\", fruit))\n}\n## [1] \"I like apple\"\n## [1] \"I like banana\"\n## [1] \"I like cherry\"\n\n\n\n\nThere are multiple ways to loop over vectors. The seq_along() function is particularly useful:\ncolors &lt;- c(\"red\", \"green\", \"blue\", \"yellow\")\n\n# Method 1: Loop over indices\nfor (i in 1:length(colors)) {\n  print(paste(\"Color\", i, \"is\", colors[i]))\n}\n## [1] \"Color 1 is red\"\n## [1] \"Color 2 is green\"\n## [1] \"Color 3 is blue\"\n## [1] \"Color 4 is yellow\"\n# Method 2: Using seq_along() - safer!\nfor (i in seq_along(colors)) {\n  print(paste(\"Color\", i, \"is\", colors[i]))\n}\n## [1] \"Color 1 is red\"\n## [1] \"Color 2 is green\"\n## [1] \"Color 3 is blue\"\n## [1] \"Color 4 is yellow\"\n# Method 3: Loop directly over elements\nfor (color in colors) {\n  print(color)\n}\n## [1] \"red\"\n## [1] \"green\"\n## [1] \"blue\"\n## [1] \"yellow\"\nWhy use seq_along()? It‚Äôs safer because if the vector is empty, 1:length(x) would give 1:0 which creates c(1, 0), but seq_along(x) correctly returns an empty sequence.\n\n\n\n\nLoops can be nested inside each other. This is useful for working with matrices or multidimensional data:\n# Create a 3x3 matrix\nmat &lt;- matrix(1:9, nrow = 3, ncol = 3)\nprint(mat)\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n# Loop through rows and columns\nfor (i in 1:nrow(mat)) {\n  for (j in 1:ncol(mat)) {\n    print(paste(\"Row\", i, \"Col\", j, \"=\", mat[i, j]))\n  }\n}\n## [1] \"Row 1 Col 1 = 1\"\n## [1] \"Row 1 Col 2 = 4\"\n## [1] \"Row 1 Col 3 = 7\"\n## [1] \"Row 2 Col 1 = 2\"\n## [1] \"Row 2 Col 2 = 5\"\n## [1] \"Row 2 Col 3 = 8\"\n## [1] \"Row 3 Col 1 = 3\"\n## [1] \"Row 3 Col 2 = 6\"\n## [1] \"Row 3 Col 3 = 9\"\nWarning: Avoid nesting more than 2-3 levels deep. If you need more, consider using functions to break up the code.\n\n\n\n\nwhile loops execute as long as a condition is TRUE:\ncount &lt;- 1\n\nwhile (count &lt;= 5) {\n  print(paste(\"Count is:\", count))\n  count &lt;- count + 1\n}\n## [1] \"Count is: 1\"\n## [1] \"Count is: 2\"\n## [1] \"Count is: 3\"\n## [1] \"Count is: 4\"\n## [1] \"Count is: 5\"\nCaution: while loops can run forever if the condition never becomes FALSE. Always make sure your loop has a way to exit!\nExample with multiple conditions:\nset.seed(123)  # For reproducibility\nvalue &lt;- 5\n\nwhile (value &gt;= 2 && value &lt;= 8) {\n  # Random walk: add or subtract 1\n  coin &lt;- sample(c(-1, 1), 1)\n  value &lt;- value + coin\n  print(paste(\"Value is now:\", value))\n}\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 2\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 2\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 6\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 6\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 6\"\n## [1] \"Value is now: 5\"\n## [1] \"Value is now: 4\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 2\"\n## [1] \"Value is now: 3\"\n## [1] \"Value is now: 2\"\n## [1] \"Value is now: 1\"\n\n\n\n\n\nbreak: Exit the loop immediately\nnext: Skip the current iteration and continue to the next\n\nUsing break:\nfor (i in 1:10) {\n  if (i &gt; 5) {\n    print(\"Breaking out of loop!\")\n    break\n  }\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n## [1] \"Breaking out of loop!\"\nUsing next:\n# Print only odd numbers\nfor (i in 1:10) {\n  if (i %% 2 == 0) {  # If i is even, skip it\n    next\n  }\n  print(i)\n}\n## [1] 1\n## [1] 3\n## [1] 5\n## [1] 7\n## [1] 9\n\n\n\n\nWrite an if-else statement that checks if a number is positive, negative, or zero.\nCreate a for loop that prints the squares of numbers 1 through 10.\nWrite a while loop that starts at 100 and divides by 2 until the value is less than 1.\nUse a for loop with next to print only numbers divisible by 3 from 1 to 20.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#functions",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#functions",
    "title": "Computational Research",
    "section": "",
    "text": "Functions allow you to encapsulate code that you want to reuse. Instead of copying and pasting code, you write it once as a function and call it whenever needed.\n\n\n\n\nReusability: Write code once, use it many times\nReadability: Give meaningful names to complex operations\nMaintainability: Fix bugs in one place instead of many\nAbstraction: Hide implementation details from users\n\nRule of thumb: If you find yourself copying and pasting code more than twice, write a function!\n\n\n\n\nFunctions are created using the function() keyword:\n# A simple function that prints a greeting\nsay_hello &lt;- function() {\n  print(\"Hello, world!\")\n}\n\n# Call the function\nsay_hello()\n## [1] \"Hello, world!\"\nA function with a body that does computation:\n# Function to calculate the area of a circle\ncircle_area &lt;- function(radius) {\n  area &lt;- pi * radius^2\n  return(area)\n}\n\n# Use the function\ncircle_area(5)\n## [1] 78.53982\ncircle_area(10)\n## [1] 314.1593\n\n\n\n\nArguments are the inputs to your function. They let users customize the function‚Äôs behavior:\n# Function with multiple arguments\ngreet_person &lt;- function(name, greeting) {\n  message &lt;- paste(greeting, name)\n  print(message)\n}\n\ngreet_person(\"Alice\", \"Hello\")\n## [1] \"Hello Alice\"\ngreet_person(\"Bob\", \"Good morning\")\n## [1] \"Good morning Bob\"\n\n\n\n\nYou can set default values for arguments. This makes the function easier to use for common cases:\n# Function with default argument\ngreet_person &lt;- function(name, greeting = \"Hello\") {\n  message &lt;- paste(greeting, name)\n  print(message)\n}\n\n# Using default\ngreet_person(\"Alice\")\n## [1] \"Hello Alice\"\n# Overriding default\ngreet_person(\"Bob\", \"Good evening\")\n## [1] \"Good evening Bob\"\nAnother example:\n# Function to repeat a message\nrepeat_message &lt;- function(msg, times = 3) {\n  for (i in seq_len(times)) {\n    print(msg)\n  }\n}\n\nrepeat_message(\"R is fun!\")\n## [1] \"R is fun!\"\n## [1] \"R is fun!\"\n## [1] \"R is fun!\"\nrepeat_message(\"Learning loops!\", times = 2)\n## [1] \"Learning loops!\"\n## [1] \"Learning loops!\"\n\n\n\n\nFunctions return the last expression evaluated, or you can use return() explicitly:\n# Implicit return (last expression)\nadd_numbers &lt;- function(a, b) {\n  a + b  # This is returned\n}\n\nresult &lt;- add_numbers(3, 5)\nprint(result)\n## [1] 8\n# Explicit return\ncalculate_stats &lt;- function(numbers) {\n  if (length(numbers) == 0) {\n    return(NULL)  # Early return for edge case\n  }\n  \n  stats &lt;- list(\n    mean = mean(numbers),\n    sd = sd(numbers),\n    min = min(numbers),\n    max = max(numbers)\n  )\n  \n  return(stats)\n}\n\nmy_stats &lt;- calculate_stats(c(10, 20, 30, 40, 50))\nprint(my_stats)\n## $mean\n## [1] 30\n## \n## $sd\n## [1] 15.81139\n## \n## $min\n## [1] 10\n## \n## $max\n## [1] 50\n\n\n\n\nR matches arguments by position or by name:\n# Define a function\npower_calc &lt;- function(base, exponent) {\n  base^exponent\n}\n\n# Positional matching\npower_calc(2, 3)  # 2^3 = 8\n## [1] 8\n# Named matching\npower_calc(exponent = 3, base = 2)  # Same result\n## [1] 8\n# Mixed matching\npower_calc(2, exponent = 3)  # Same result\n## [1] 8\nTip: For functions with many arguments, use named arguments for clarity!\n\n\n\n\nWrite a function called fahrenheit_to_celsius that converts temperature from Fahrenheit to Celsius. Formula: C = (F - 32) * 5/9\nWrite a function called word_count that takes a text string and returns the number of words.\nWrite a function that takes a vector of numbers and returns a named list with the sum, mean, and length.\nModify your word_count function to have a default argument remove_punct = TRUE that removes punctuation before counting.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#text-analysis-with-tidytext",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#text-analysis-with-tidytext",
    "title": "Computational Research",
    "section": "",
    "text": "Now let‚Äôs apply what we‚Äôve learned to text analysis! The tidytext package provides tools for working with text in a tidy data format.\n\n\n\nTidy text format means having one token per row. A token can be:\n\nA word\nA sentence\nAn n-gram (sequence of n words)\nA paragraph\n\nThis format works seamlessly with tidyverse tools like dplyr and ggplot2.\nLoad the Jane Austen books dataset:\n# install.packages(\"janeaustenr\")\nlibrary(janeaustenr)\n\n# Combine all books into a single dataframe\noriginal_books &lt;- austen_books() |&gt; \n  group_by(book) |&gt; \n  mutate(\n    linenumber = row_number(),\n    chapter = cumsum(str_detect(text, \n                                regex(\"^chapter [\\\\divxlc]\",\n                                      ignore_case = TRUE)))\n  ) |&gt; \n  ungroup()\n\nhead(original_books)\n## # A tibble: 6 √ó 4\n##   text                    book                linenumber chapter\n##   &lt;chr&gt;                   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt;\n## 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility          1       0\n## 2 \"\"                      Sense & Sensibility          2       0\n## 3 \"by Jane Austen\"        Sense & Sensibility          3       0\n## 4 \"\"                      Sense & Sensibility          4       0\n## 5 \"(1811)\"                Sense & Sensibility          5       0\n## 6 \"\"                      Sense & Sensibility          6       0\n\n\n\n\nunnest_tokens() breaks text into individual tokens (usually words):\nSyntax:\nunnest_tokens(tbl, output, input, token = \"words\", ...)\n\n\n\nArgument\nDescription\n\n\n\n\ntbl\nThe data frame\n\n\noutput\nName of the new column for tokens\n\n\ninput\nName of the column containing text\n\n\ntoken\nType: ‚Äúwords‚Äù, ‚Äúsentences‚Äù, ‚Äúngrams‚Äù, etc.\n\n\n\nTokenize the Jane Austen books:\ntidy_books &lt;- original_books |&gt; \n  unnest_tokens(word, text)\n\nhead(tidy_books)\n## # A tibble: 6 √ó 4\n##   book                linenumber chapter word       \n##   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      \n## 1 Sense & Sensibility          1       0 sense      \n## 2 Sense & Sensibility          1       0 and        \n## 3 Sense & Sensibility          1       0 sensibility\n## 4 Sense & Sensibility          3       0 by         \n## 5 Sense & Sensibility          3       0 jane       \n## 6 Sense & Sensibility          3       0 austen\nNotice that: - Punctuation is removed - Text is converted to lowercase - Each word is now its own row\nCount words:\ntidy_books |&gt; \n  count(word, sort = TRUE)\n## # A tibble: 14,520 √ó 2\n##    word      n\n##    &lt;chr&gt; &lt;int&gt;\n##  1 the   26351\n##  2 to    24044\n##  3 and   22515\n##  4 of    21178\n##  5 a     13408\n##  6 her   13055\n##  7 i     12006\n##  8 in    11217\n##  9 was   11204\n## 10 it    10234\n## # ‚Ñπ 14,510 more rows\n\n\n\n\nStopwords are common words like ‚Äúthe‚Äù, ‚Äúand‚Äù, ‚Äúof‚Äù that don‚Äôt carry much meaning. The tidytext package includes a stopwords dataset:\n# View stopwords\nhead(stop_words)\n## # A tibble: 6 √ó 2\n##   word      lexicon\n##   &lt;chr&gt;     &lt;chr&gt;  \n## 1 a         SMART  \n## 2 a's       SMART  \n## 3 able      SMART  \n## 4 about     SMART  \n## 5 above     SMART  \n## 6 according SMART\n# How many stopwords?\nnrow(stop_words)\n## [1] 1149\nRemove stopwords using anti_join():\ntidy_books_clean &lt;- tidy_books |&gt; \n  anti_join(stop_words, by = \"word\")\n\n# Compare counts\nnrow(tidy_books)        # Before\n## [1] 725055\nnrow(tidy_books_clean)  # After\n## [1] 217609\n# Most common words without stopwords\ntidy_books_clean |&gt; \n  count(word, sort = TRUE) |&gt; \n  head(15)\n## # A tibble: 15 √ó 2\n##    word          n\n##    &lt;chr&gt;     &lt;int&gt;\n##  1 miss       1855\n##  2 time       1337\n##  3 fanny       862\n##  4 dear        822\n##  5 lady        817\n##  6 sir         806\n##  7 day         797\n##  8 emma        787\n##  9 sister      727\n## 10 house       699\n## 11 elizabeth   687\n## 12 elinor      623\n## 13 hope        601\n## 14 friend      593\n## 15 family      578\n\n\n\nLoad the Starbucks Twitter data and practice tokenization:\nlibrary(readr)\n\nurl &lt;- \"https://raw.githubusercontent.com/aysedeniz09/Social_Media_Listening/refs/heads/main/MSC_social_media_list_data/Starbucks_User_Data.csv\"\nstarbucks_data &lt;- read_csv(url)\n\nhead(starbucks_data)\n## # A tibble: 6 √ó 16\n##   author_id conversation_id created_at          hashtag lang  like_count mention\n##       &lt;dbl&gt;           &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;  \n## 1     30973         1.61e18 2022-12-27 15:43:16 &lt;NA&gt;    en            10 &lt;NA&gt;   \n## 2     30973         1.60e18 2022-11-29 05:23:55 &lt;NA&gt;    en             9 Mo_sha‚Ä¶\n## 3     30973         1.59e18 2022-11-28 20:14:09 &lt;NA&gt;    en             2 Mixxed‚Ä¶\n## 4     30973         1.60e18 2022-11-28 12:51:28 &lt;NA&gt;    en             0 BihhKa‚Ä¶\n## 5     30973         1.60e18 2022-11-27 15:14:26 &lt;NA&gt;    en             0 BihhKa‚Ä¶\n## 6     30973         1.60e18 2022-11-24 17:47:24 &lt;NA&gt;    en             1 therea‚Ä¶\n## # ‚Ñπ 9 more variables: quote_count &lt;dbl&gt;, referenced_status_id &lt;dbl&gt;,\n## #   referenced_user_id &lt;dbl&gt;, reply_count &lt;dbl&gt;, retweet_count &lt;dbl&gt;,\n## #   row_id &lt;dbl&gt;, status_id &lt;dbl&gt;, text &lt;chr&gt;, type &lt;chr&gt;\n\nTokenize the text column into words.\nRemove stopwords.\nFind the 20 most common words.\nHow many total words are there before and after removing stopwords?\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#keyword-in-context-kwic",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#keyword-in-context-kwic",
    "title": "Computational Research",
    "section": "",
    "text": "Keyword in Context (KWIC) extracts and analyzes how specific keywords appear in text along with their surrounding context.\nWhy use KWIC?\n\nIdentify patterns in how words are used\nUnderstand the context around specific terms\nUseful for sentiment analysis and content understanding\n\n\n\n\n\nUse str_detect() to filter text containing a keyword:\n# Find all lines containing \"family\"\nfiltered_text &lt;- original_books |&gt; \n  filter(str_detect(text, \"family\"))\n\nhead(filtered_text)\n## # A tibble: 6 √ó 4\n##   text                                                  book  linenumber chapter\n##   &lt;chr&gt;                                                 &lt;fct&gt;      &lt;int&gt;   &lt;int&gt;\n## 1 The family of Dashwood had long been settled in Suss‚Ä¶ Sens‚Ä¶         13       1\n## 2 into his house the family of his nephew Mr. Henry Da‚Ä¶ Sens‚Ä¶         22       1\n## 3 family; but he was affected by a recommendation of s‚Ä¶ Sens‚Ä¶         79       1\n## 4 any of her husband's family; but she had had no oppo‚Ä¶ Sens‚Ä¶        116       1\n## 5 large a sum was parted with.  If he should have a nu‚Ä¶ Sens‚Ä¶        223       2\n## 6 of her character, which half a year's residence in h‚Ä¶ Sens‚Ä¶        398       3\nnrow(filtered_text)\n## [1] 572\n\n\n\n\nUse regex to extract characters before and after a keyword:\n# Extract 15 characters before and after \"family\"\ncontext &lt;- original_books |&gt;\n  filter(str_detect(text, \"family\")) |&gt;\n  mutate(context = str_extract(text, \".{0,15}family.{0,15}\"))\n\nhead(context$context, 10)\n##  [1] \"The family of Dashwood ha\"           \n##  [2] \" his house the family of his nephew \"\n##  [3] \"family; but he was af\"               \n##  [4] \" her husband's family; but she had h\"\n##  [5] \"ave a numerous family, for\"          \n##  [6] \"sidence in her family afforded;\"     \n##  [7] \" small for our family,\"              \n##  [8] \"terms with his family, and pressed\"  \n##  [9] \"than any other family in the neighbo\"\n## [10] \"rival of a new family in the country\"\nCreate a function for KWIC:\n# KWIC function\nkwic &lt;- function(data, text_col, keyword, window = 10) {\n  pattern &lt;- paste0(\".{0,\", window, \"}\", keyword, \".{0,\", window, \"}\")\n  \n  data |&gt;\n    filter(str_detect({{ text_col }}, keyword)) |&gt;\n    mutate(context = str_extract({{ text_col }}, pattern)) |&gt;\n    select(context)\n}\n\n# Use the function\nkwic(original_books, text, \"love\", window = 20) |&gt; \n  head(10)\n## # A tibble: 10 √ó 1\n##    context                                       \n##    &lt;chr&gt;                                         \n##  1 \" and her own tender love for all her three c\"\n##  2 \"e you are right, my love; it will be better \"\n##  3 \"ove far from that beloved spot was impossibl\"\n##  4 \"that he loved her daughter, and \"            \n##  5 \"rything amiable.  I love him already.\\\"\"     \n##  6 \"obation inferior to love.\\\"\"                 \n##  7 \"separate esteem and love.\\\"\"                 \n##  8 \"eive any symptom of love in his behaviour to\"\n##  9 \"\\\"My love, it will be scarcel\"               \n## 10 \"very amiable, and I love him tenderly.  But \"\n\n\n\nUsing the Starbucks data:\n\nFind all tweets containing ‚Äúcoffee‚Äù.\nExtract 20 characters of context around ‚Äúcoffee‚Äù.\nFind tweets containing mentions (@username) using regex.\nCreate a KWIC analysis for the word ‚ÄúStarbucks‚Äù.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#document-term-matrix-dtm",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#document-term-matrix-dtm",
    "title": "Computational Research",
    "section": "",
    "text": "A Document-Term Matrix (DTM) is a mathematical representation of text where:\n\nRows represent documents (e.g., books, tweets, articles)\nColumns represent terms (words)\nValues indicate the frequency of each term in each document\n\nExample DTM:\n\n\n\n\nlove\nfamily\nmoney\nmarriage\nhappy\n\n\n\n\nBook 1\n45\n23\n12\n67\n34\n\n\nBook 2\n32\n45\n56\n23\n12\n\n\nBook 3\n67\n12\n8\n89\n45\n\n\n\nNote: A Term-Document Matrix (TDM) is simply the transpose - terms as rows, documents as columns. In R‚Äôs tidytext, we typically create DTMs.\n\n\n\n\nThe transformation from raw text to DTM follows these steps:\nRaw Text ‚Üí Tokenize ‚Üí Remove Stopwords ‚Üí Count ‚Üí DTM\nStep-by-step visualization:\n# STEP 1: Start with raw text\noriginal_books |&gt; \n  select(book, text) |&gt; \n  head(3)\n## # A tibble: 3 √ó 2\n##   book                text                   \n##   &lt;fct&gt;               &lt;chr&gt;                  \n## 1 Sense & Sensibility \"SENSE AND SENSIBILITY\"\n## 2 Sense & Sensibility \"\"                     \n## 3 Sense & Sensibility \"by Jane Austen\"\n# STEP 2: Tokenize (one word per row)\ntokenized &lt;- original_books |&gt; \n  unnest_tokens(word, text)\n\ntokenized |&gt; \n  select(book, word) |&gt; \n  head(10)\n## # A tibble: 10 √ó 2\n##    book                word       \n##    &lt;fct&gt;               &lt;chr&gt;      \n##  1 Sense & Sensibility sense      \n##  2 Sense & Sensibility and        \n##  3 Sense & Sensibility sensibility\n##  4 Sense & Sensibility by         \n##  5 Sense & Sensibility jane       \n##  6 Sense & Sensibility austen     \n##  7 Sense & Sensibility 1811       \n##  8 Sense & Sensibility chapter    \n##  9 Sense & Sensibility 1          \n## 10 Sense & Sensibility the\n# STEP 3: Remove stopwords\ncleaned &lt;- tokenized |&gt; \n  anti_join(stop_words, by = \"word\")\n\ncleaned |&gt; \n  select(book, word) |&gt; \n  head(10)\n## # A tibble: 10 √ó 2\n##    book                word       \n##    &lt;fct&gt;               &lt;chr&gt;      \n##  1 Sense & Sensibility sense      \n##  2 Sense & Sensibility sensibility\n##  3 Sense & Sensibility jane       \n##  4 Sense & Sensibility austen     \n##  5 Sense & Sensibility 1811       \n##  6 Sense & Sensibility chapter    \n##  7 Sense & Sensibility 1          \n##  8 Sense & Sensibility family     \n##  9 Sense & Sensibility dashwood   \n## 10 Sense & Sensibility settled\n# STEP 4: Count words per document\nword_counts &lt;- cleaned |&gt; \n  count(book, word, sort = TRUE)\n\nhead(word_counts, 10)\n## # A tibble: 10 √ó 3\n##    book                word          n\n##    &lt;fct&gt;               &lt;chr&gt;     &lt;int&gt;\n##  1 Mansfield Park      fanny       816\n##  2 Emma                emma        786\n##  3 Sense & Sensibility elinor      623\n##  4 Emma                miss        599\n##  5 Pride & Prejudice   elizabeth   597\n##  6 Mansfield Park      crawford    493\n##  7 Sense & Sensibility marianne    492\n##  8 Persuasion          anne        447\n##  9 Mansfield Park      miss        432\n## 10 Northanger Abbey    catherine   428\n\n\n\n\nUse cast_dtm() to convert tidy word counts to a DTM:\n# Create DTM\nbook_dtm &lt;- word_counts |&gt; \n  cast_dtm(document = book, term = word, value = n)\n\n# Inspect the DTM\nbook_dtm\n## &lt;&lt;DocumentTermMatrix (documents: 6, terms: 13914)&gt;&gt;\n## Non-/sparse entries: 37224/46260\n## Sparsity           : 55%\n## Maximal term length: 19\n## Weighting          : term frequency (tf)\n# View dimensions: documents x terms\ndim(book_dtm)\n## [1]     6 13914\nUnderstanding the output: - 6 documents (the 6 Jane Austen books) - 13,914 terms (unique words across all books) - 99% sparse means 99% of the cells are zeros (most words don‚Äôt appear in most books)\n\n\n\n\nLet‚Äôs visualize a small portion of the DTM to understand its structure:\n# Get top 10 words overall\ntop_10_words &lt;- word_counts |&gt; \n  group_by(word) |&gt; \n  summarize(total = sum(n)) |&gt; \n  slice_max(total, n = 10) |&gt; \n  pull(word)\n\n# Filter to just these words\ndtm_subset &lt;- word_counts |&gt; \n  filter(word %in% top_10_words)\n\n# Create heatmap\nggplot(dtm_subset, aes(x = word, y = book, fill = n)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = n), color = \"white\", size = 3) +\n  scale_fill_gradient(low = \"steelblue\", high = \"darkred\") +\n  labs(\n    title = \"Document-Term Matrix Heatmap\",\n    subtitle = \"Top 10 words across Jane Austen books\",\n    x = \"Term\",\n    y = \"Document\",\n    fill = \"Frequency\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nReading the heatmap: - Darker colors = higher frequency - Each row is a book (document) - Each column is a word (term) - The numbers show exact word counts\n\n\n\n\nTop words per book:\ntop_words &lt;- word_counts |&gt; \n  group_by(book) |&gt; \n  slice_max(n, n = 5) |&gt; \n  ungroup()\n\nhead(top_words, 12)\n## # A tibble: 12 √ó 3\n##    book                word          n\n##    &lt;fct&gt;               &lt;chr&gt;     &lt;int&gt;\n##  1 Sense & Sensibility elinor      623\n##  2 Sense & Sensibility marianne    492\n##  3 Sense & Sensibility time        239\n##  4 Sense & Sensibility dashwood    231\n##  5 Sense & Sensibility sister      229\n##  6 Pride & Prejudice   elizabeth   597\n##  7 Pride & Prejudice   darcy       373\n##  8 Pride & Prejudice   bennet      294\n##  9 Pride & Prejudice   miss        283\n## 10 Pride & Prejudice   jane        264\n## 11 Mansfield Park      fanny       816\n## 12 Mansfield Park      crawford    493\nBar chart visualization:\nggplot(top_words, aes(x = reorder(word, n), y = n, fill = book)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  facet_wrap(~ book, scales = \"free_y\") +\n  labs(\n    title = \"Top 5 Words in Each Jane Austen Book\",\n    x = \"Word\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nYou can convert a DTM back to tidy format using tidy():\n# Convert DTM back to tidy\ntidy_dtm &lt;- tidy(book_dtm)\n\nhead(tidy_dtm)\n## # A tibble: 6 √ó 3\n##   document            term   count\n##   &lt;chr&gt;               &lt;chr&gt;  &lt;dbl&gt;\n## 1 Sense & Sensibility fanny     42\n## 2 Mansfield Park      fanny    816\n## 3 Persuasion          fanny      4\n## 4 Emma                emma     786\n## 5 Persuasion          emma       1\n## 6 Sense & Sensibility elinor   623\nThis is useful when you receive a DTM from another package and want to use tidyverse tools."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#tf-idf",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#tf-idf",
    "title": "Computational Research",
    "section": "",
    "text": "TF-IDF (Term Frequency - Inverse Document Frequency) measures how important a word is to a document within a collection.\n\nTF (Term Frequency): How often a term appears in a document\nIDF (Inverse Document Frequency): How rare a term is across all documents\n\n\\[\\text{TF-IDF} = \\text{TF} \\times \\log\\left(\\frac{\\text{Total Documents}}{\\text{Documents containing term}}\\right)\\]\nInterpretation:\n\nHigh TF-IDF: Word is frequent in this document but rare overall ‚Üí important/distinctive\nLow TF-IDF: Word is common everywhere ‚Üí less distinctive\n\n\n\n\n\nUse bind_tf_idf() from tidytext:\n# Calculate TF-IDF\nbook_tfidf &lt;- original_books |&gt; \n  unnest_tokens(word, text) |&gt; \n  count(book, word, sort = TRUE) |&gt; \n  bind_tf_idf(word, book, n)\n\nhead(book_tfidf)\n## # A tibble: 6 √ó 6\n##   book           word      n     tf   idf tf_idf\n##   &lt;fct&gt;          &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n## 1 Mansfield Park the    6206 0.0387     0      0\n## 2 Mansfield Park to     5475 0.0341     0      0\n## 3 Mansfield Park and    5438 0.0339     0      0\n## 4 Emma           to     5239 0.0325     0      0\n## 5 Emma           the    5201 0.0323     0      0\n## 6 Emma           and    4896 0.0304     0      0\nFind distinctive words for each book:\n# Top TF-IDF words per book\ntop_tfidf &lt;- book_tfidf |&gt; \n  group_by(book) |&gt; \n  slice_max(tf_idf, n = 5) |&gt; \n  ungroup()\n\nhead(top_tfidf, 12)\n## # A tibble: 12 √ó 6\n##    book                word           n      tf   idf  tf_idf\n##    &lt;fct&gt;               &lt;chr&gt;      &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n##  1 Sense & Sensibility elinor       623 0.00519 1.79  0.00931\n##  2 Sense & Sensibility marianne     492 0.00410 1.79  0.00735\n##  3 Sense & Sensibility dashwood     231 0.00193 1.79  0.00345\n##  4 Sense & Sensibility jennings     199 0.00166 1.79  0.00297\n##  5 Sense & Sensibility willoughby   181 0.00151 1.79  0.00270\n##  6 Pride & Prejudice   darcy        373 0.00305 1.79  0.00547\n##  7 Pride & Prejudice   bennet       294 0.00241 1.79  0.00431\n##  8 Pride & Prejudice   bingley      257 0.00210 1.79  0.00377\n##  9 Pride & Prejudice   elizabeth    597 0.00489 0.693 0.00339\n## 10 Pride & Prejudice   wickham      162 0.00133 1.79  0.00238\n## 11 Mansfield Park      crawford     493 0.00307 1.79  0.00551\n## 12 Mansfield Park      edmund       364 0.00227 1.79  0.00406\n\n\n\n\nggplot(top_tfidf, aes(x = reorder(word, tf_idf), y = tf_idf, fill = book)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  facet_wrap(~ book, scales = \"free\") +\n  labs(\n    title = \"Most Distinctive Words by TF-IDF\",\n    x = \"Word\",\n    y = \"TF-IDF Score\"\n  ) +\n  theme_minimal()\n\nNotice that TF-IDF highlights character names and distinctive terms for each book, rather than common words!\n\n\n\nUsing the Starbucks data:\n\nCreate word counts grouped by mention (the user being replied to).\nCalculate TF-IDF scores.\nFind the top 5 distinctive words for the 3 most active mentions.\nVisualize the results.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#word-clouds",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#word-clouds",
    "title": "Computational Research",
    "section": "",
    "text": "Word clouds display words with size proportional to their frequency. Install the wordcloud2 package:\ninstall.packages(\"wordcloud2\")\nlibrary(wordcloud2)\n\n# Prepare word frequencies\nword_freq &lt;- original_books |&gt;\n  unnest_tokens(word, text) |&gt;\n  anti_join(stop_words, by = \"word\") |&gt;\n  count(word, sort = TRUE)\n\nhead(word_freq)\n## # A tibble: 6 √ó 2\n##   word      n\n##   &lt;chr&gt; &lt;int&gt;\n## 1 miss   1855\n## 2 time   1337\n## 3 fanny   862\n## 4 dear    822\n## 5 lady    817\n## 6 sir     806\nCreate a basic word cloud:\nwordcloud2(data = word_freq, size = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nDescription\nExample\n\n\n\n\nsize\nScale factor for word sizes\nsize = 0.5\n\n\ncolor\nColor scheme\ncolor = \"random-light\"\n\n\nbackgroundColor\nBackground color\nbackgroundColor = \"black\"\n\n\nshape\nShape of cloud\nshape = \"circle\" or \"star\"\n\n\nminRotation, maxRotation\nWord rotation angles\nminRotation = -pi/4\n\n\n\nCustom colors:\nwordcloud2(data = word_freq, size = 0.5, color = \"random-light\")\n\n\n\n\nChange shape and background:\nwordcloud2(data = word_freq, size = 0.4, shape = \"star\", backgroundColor = \"black\", color = \"random-light\")\n\n\n\n\nWith rotation:\nwordcloud2(data = word_freq, size = 0.5, \n           minRotation = -pi/4, maxRotation = pi/4, rotateRatio = 0.3)\n\n\n\n\n\n\n\nlibrary(htmlwidgets)\nlibrary(webshot)\nwebshot::install_phantomjs()\n\n# Create word cloud\nmy_cloud &lt;- wordcloud2(word_freq, size = 1)\n\n# Save as HTML\nsaveWidget(my_cloud, \"wordcloud.html\", selfcontained = FALSE)\n\n# Save as image\nwebshot(\"wordcloud.html\", \"wordcloud.png\", delay = 5)"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#class-exercises-putting-it-all-together",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#class-exercises-putting-it-all-together",
    "title": "Computational Research",
    "section": "",
    "text": "Using either the Jane Austen or Starbucks dataset:\n\nWrite a function that takes a data frame and text column, then returns:\n\nTotal word count (after removing stopwords)\nTop 10 most frequent words\nA word cloud object\n\nUse a loop to analyze each book (or group) separately and store the results in a list.\nCreate a KWIC analysis for a keyword of your choice.\nCalculate TF-IDF and identify what makes each document/group distinctive.\nVisualize your findings with at least two different plot types.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L4-github.html#lecture-4-cheat-sheet",
    "href": "Spring2026/LectureSlides/bigdata_L4-github.html#lecture-4-cheat-sheet",
    "title": "Computational Research",
    "section": "",
    "text": "Topic\nDescription\nCode Example\n\n\n\n\nif-else\nConditional execution\nif (x &gt; 5) { ... } else { ... }\n\n\nfor loop\nFixed iterations\nfor (i in 1:10) { print(i) }\n\n\nwhile loop\nConditional iterations\nwhile (x &lt; 10) { x &lt;- x + 1 }\n\n\nseq_along()\nSafe sequence for loops\nfor (i in seq_along(vec)) { ... }\n\n\nbreak\nExit loop\nif (cond) break\n\n\nnext\nSkip iteration\nif (cond) next\n\n\nfunction()\nCreate function\nmy_func &lt;- function(arg) { ... }\n\n\nDefault arguments\nSet defaults\nfunction(x, y = 10) { ... }\n\n\nreturn()\nExplicit return\nreturn(result)\n\n\nunnest_tokens()\nTokenize text\nunnest_tokens(word, text)\n\n\nanti_join(stop_words)\nRemove stopwords\ndata |&gt; anti_join(stop_words)\n\n\nstr_detect()\nFind pattern\nfilter(str_detect(text, \"word\"))\n\n\nstr_extract()\nExtract pattern\nmutate(x = str_extract(text, \".{10}word.{10}\"))\n\n\nKWIC\nKeyword in context\nExtract surrounding text for keywords\n\n\ncast_dtm()\nCreate DTM from tidy\ncast_dtm(document, term, value)\n\n\ntidy()\nConvert DTM to tidy\ntidy(dtm_object)\n\n\ngeom_tile()\nCreate heatmap\ngeom_tile(aes(x, y, fill = value))\n\n\nbind_tf_idf()\nCalculate TF-IDF\nbind_tf_idf(word, document, n)\n\n\nwordcloud2()\nCreate word cloud\nwordcloud2(data = freq_df, size = 0.5)\n\n\ncount()\nCount occurrences\ncount(word, sort = TRUE)\n\n\nslice_max()\nTop n by value\nslice_max(n, n = 10)"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L6-github.html",
    "href": "Spring2026/LectureSlides/bigdata_L6-github.html",
    "title": "Statistics",
    "section": "",
    "text": "Dr.¬†Ayse D. Lokmanoglu Lecture 6, (B) Feb 25, (A) Mar 2\n\n\n\n\n\nSection\nTopic\n\n\n\n\n1\nHypothesis Testing\n\n\n1.1\nWhat is Hypothesis Testing?\n\n\n1.2\nT-Test: Comparing Two Groups\n\n\n1.3\nVisualizing T-Tests\n\n\n1.4\nANOVA: Comparing Multiple Groups\n\n\n1.5\nVisualizing ANOVA\n\n\n2\nRegression Models\n\n\n2.1\nSimple Linear Regression\n\n\n2.2\nInterpreting Regression Output\n\n\n2.3\nMultiple Linear Regression\n\n\n2.4\nDummy Variables\n\n\n3\nVisualizing Regression\n\n\n3.1\nScatter Plots with Regression Lines\n\n\n3.2\nCoefficient Plots\n\n\n3.3\nMarginal Effects Plots\n\n\n4\nGeneralized Linear Models\n\n\n4.1\nWhen to Use GLMs\n\n\n4.2\nPoisson Regression\n\n\n5\nInteraction Terms\n\n\n5.1\nWhat are Interactions?\n\n\n5.2\nVisualizing Interactions\n\n\n6\nModel Diagnostics\n\n\n6.1\nAssumptions of Linear Regression\n\n\n6.2\nChecking Assumptions\n\n\n6.3\nThe performance Package\n\n\n\n\nALWAYS load our libraries first\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(broom)\n\n\n\n\nWe‚Äôll use the same student dataset throughout this lecture:\n# Create dataset\nstudent_data &lt;- read_csv(\"https://raw.githubusercontent.com/aysedeniz09/IntroCSS/refs/heads/main/data/student_lifestyle_dataset.csv\")\n\nhead(student_data)\n## # A tibble: 6 √ó 12\n##   Student_ID Study_Hours_Per_Day Extracurricular_Hours_Per‚Ä¶¬π Sleep_Hours_Per_Day\n##        &lt;dbl&gt;               &lt;dbl&gt;                       &lt;dbl&gt;               &lt;dbl&gt;\n## 1          1                 6.9                         3.8                 8.7\n## 2          2                 5.3                         3.5                 8  \n## 3          3                 5.1                         3.9                 9.2\n## 4          4                 6.5                         2.1                 7.2\n## 5          5                 8.1                         0.6                 6.5\n## 6          6                 6                           2.1                 8  \n## # ‚Ñπ abbreviated name: ¬π‚ÄãExtracurricular_Hours_Per_Day\n## # ‚Ñπ 8 more variables: Social_Hours_Per_Day &lt;dbl&gt;,\n## #   Physical_Activity_Hours_Per_Day &lt;dbl&gt;, GPA &lt;dbl&gt;, Stress_Level &lt;chr&gt;,\n## #   college &lt;chr&gt;, GPA_Level &lt;chr&gt;, Stress_Level2 &lt;chr&gt;, books_read &lt;dbl&gt;\n\n\n\n\n\n\nHypothesis testing allows you to make inferences about populations based on sample data. It answers: ‚ÄúIs the pattern I see in my data real, or could it be due to chance?‚Äù\nKey Components:\n\nNull Hypothesis (H‚ÇÄ): No effect or difference exists\nAlternative Hypothesis (H‚ÇÅ): An effect or difference exists\np-value: Probability of observing your data if H‚ÇÄ is true\nSignificance level (Œ±): Threshold for rejecting H‚ÇÄ (usually 0.05)\n\nDecision Rule:\n\nIf p &lt; Œ±: Reject H‚ÇÄ (result is statistically significant)\nIf p ‚â• Œ±: Fail to reject H‚ÇÄ (result is not statistically significant)\n\nImportant: ‚ÄúFail to reject‚Äù ‚â† ‚ÄúAccept H‚ÇÄ‚Äù. We never prove the null hypothesis is true!\n\n\n\n\nA t-test compares the means of two groups to determine if they are significantly different.\nTypes of T-Tests:\n\n\n\n\n\n\n\n\nType\nUse Case\nExample\n\n\n\n\nIndependent\nTwo unrelated groups\nMales vs.¬†Females\n\n\nPaired\nSame subjects, two conditions\nBefore vs.¬†After treatment\n\n\nOne-sample\nSample vs.¬†known value\nSample mean vs.¬†population mean\n\n\n\nExample: Do male and female CS students study different hours?\n# Filter for CS students\nAS_students &lt;- student_data |&gt; \n  filter(college == \"Arts & Sciences\")\n\n# Independent t-test\nt_test_result &lt;- t.test(Study_Hours_Per_Day ~ Stress_Level2, data = AS_students)\nt_test_result\n## \n##  Welch Two Sample t-test\n## \n## data:  Study_Hours_Per_Day by Stress_Level2\n## t = 15.245, df = 310.19, p-value &lt; 2.2e-16\n## alternative hypothesis: true difference in means between group High and group LowModerate is not equal to 0\n## 95 percent confidence interval:\n##  1.526264 1.978624\n## sample estimates:\n##        mean in group High mean in group LowModerate \n##                  8.362941                  6.610497\nInterpreting the Output:\n\nt-statistic (t = 15.245): How many standard errors the difference is from 0\ndf (df = 310.19): Degrees of freedom (affects the t-distribution)\np-value (p &lt; 2.2e-16): Probability of seeing this difference by chance\n95% CI ([1.53, 1.98]): Range likely containing the true difference in means\nGroup means: High Stress = 8.36 hours, Low/Moderate Stress = 6.61 hours\n\nConclusion: p &lt; 0.001, so we reject H‚ÇÄ. There is a significant difference in study hours between stress levels. Students with high stress study approximately 1.75 more hours per day than students with low/moderate stress.\nHow to report: A Welch two-sample t-test revealed a significant difference in daily study hours between high stress (M = 8.36) and low/moderate stress (M = 6.61) students, t(310.19) = 15.25, p &lt; .001, 95% CI [1.53, 1.98].\n\n\n\n\nBoxplot with means:\nggplot(AS_students, aes(x = Stress_Level2, y = Study_Hours_Per_Day, fill = Stress_Level2)) +\n  geom_boxplot(alpha = 0.7) +\n  stat_summary(fun = mean, geom = \"point\", shape = 18, size = 4, color = \"black\") +\n  labs(\n    title = \"Study Hours by Stress Level (Arts & Sciences Students)\",\n    subtitle = paste(\"t-test p-value =\", round(t_test_result$p.value, 3)),\n    x = \"Stress Level\",\n    y = \"Study Hours\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nViolin plot:\nggplot(AS_students, aes(x = Stress_Level2, y = Study_Hours_Per_Day, fill = Stress_Level2)) +\n  geom_violin(trim = FALSE, alpha = 0.7) +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  stat_summary(fun = mean, geom = \"point\", shape = 18, size = 3, color = \"red\") +\n  labs(\n    title = \"Distribution of Study Hours by Stress Level\",\n    x = \"Stress Level\",\n    y = \"Study Hours\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nBar plot with error bars:\n# Calculate summary statistics\nstress_summary &lt;- AS_students |&gt; \n  group_by(Stress_Level2) |&gt; \n  summarize(\n    mean = mean(Study_Hours_Per_Day),\n    se = sd(Study_Hours_Per_Day) / sqrt(n()),\n    .groups = \"drop\"\n  )\n\nggplot(stress_summary, aes(x = Stress_Level2, y = mean, fill = Stress_Level2)) +\n  geom_col(alpha = 0.7) +\n  geom_errorbar(aes(ymin = mean - 1.96*se, ymax = mean + 1.96*se), \n                width = 0.2) +\n  geom_text(aes(label = round(mean, 1)), vjust = -1.5) +\n  labs(\n    title = \"Mean Study Hours by Stress Level (¬±95% CI)\",\n    x = \"Stress Level\",\n    y = \"Mean Study Hours\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nANOVA (Analysis of Variance) extends the t-test to compare means across three or more groups.\n\nH‚ÇÄ: All group means are equal (Œº‚ÇÅ = Œº‚ÇÇ = Œº‚ÇÉ)\nH‚ÇÅ: At least one group mean is different\n\nExample: Do students from different majors study different hours?\n# One-way ANOVA\nanova_result &lt;- aov(Study_Hours_Per_Day ~ college, data = student_data)\nsummary(anova_result)\n##               Df Sum Sq Mean Sq F value Pr(&gt;F)\n## college        4      3  0.6442   0.317  0.867\n## Residuals   1995   4050  2.0302\nInterpreting the Output:\n\nF value (0.317): Ratio of between-group variance to within-group variance\nPr(&gt;F) (0.867): p-value for the F-test\n\nConclusion: p = 0.867 &gt; 0.05, so we fail to reject H‚ÇÄ. There is no significant difference in GPA across colleges.\nPost-hoc test: Which majors are different from each other?\n# Tukey's HSD (Honestly Significant Difference)\nTukeyHSD(anova_result)\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = Study_Hours_Per_Day ~ college, data = student_data)\n## \n## $college\n##                                      diff        lwr       upr     p adj\n## Communication-Arts & Sciences  0.09074074 -0.2209765 0.4024580 0.9321845\n## Education-Arts & Sciences     -0.01271548 -0.2919796 0.2665486 0.9999462\n## Humanities-Arts & Sciences    -0.00905683 -0.2806320 0.2625183 0.9999845\n## Medicine-Arts & Sciences       0.04006047 -0.2382109 0.3183318 0.9949622\n## Education-Communication       -0.10345622 -0.4016537 0.1947413 0.8783008\n## Humanities-Communication      -0.09979757 -0.3908068 0.1912116 0.8827707\n## Medicine-Communication        -0.05068027 -0.3479482 0.2465877 0.9903715\n## Humanities-Education           0.00365865 -0.2522850 0.2596023 0.9999995\n## Medicine-Education             0.05277595 -0.2102621 0.3158140 0.9822233\n## Medicine-Humanities            0.04911730 -0.2057428 0.3039774 0.9847059\nHow to report: A one-way ANOVA revealed no significant differences in daily study hours among colleges, F(4, 1995) = 0.32, p = .867. Post-hoc Tukey tests confirmed no significant pairwise differences between any colleges (all p &gt; .05).\n\n\n\n\nggplot(student_data, aes(x = college, y = Study_Hours_Per_Day, fill = college)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 18, size = 4, color = \"black\") +\n  labs(\n    title = \"Study Hours by College\",\n    subtitle = paste(\"ANOVA F(4, 1995) =\", round(summary(anova_result)[[1]]$`F value`[1], 2), \n                     \", p = 0.867\"),\n    x = \"College\",\n    y = \"Study Hours\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nComparison Table:\n\n\n\nTest\nGroups\nCompares\nStatistic\n\n\n\n\nT-test\n2\nMeans\nt-statistic\n\n\nANOVA\n3+\nMeans\nF-statistic\n\n\n\n\n\n\n\nPerform a t-test comparing GPA for all stress levels (all colleges).\nPerform an ANOVA comparing GPA across colleges\nIf ANOVA is significant, run Tukey‚Äôs HSD to identify which colleges differ.\nVisualize your results with appropriate plots.\n\n### Your workspace\n\n\n\n\n\n\nRegression models examine the relationship between variables and allow us to predict outcomes.\n\n\nA simple linear regression models the relationship between one predictor (X) and one outcome (Y):\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\nWhere: - \\(\\beta_0\\) = Intercept (predicted Y when X = 0) - \\(\\beta_1\\) = Slope (change in Y for one-unit increase in X) - \\(\\epsilon\\) = Error term\nExample: Does study time predict exam scores?\n# Fit simple linear regression\nsimple_model &lt;- lm(GPA ~ Study_Hours_Per_Day, data = student_data)\nsummary(simple_model)\n## \n## Call:\n## lm(formula = GPA ~ Study_Hours_Per_Day, data = student_data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.60834 -0.13516 -0.00103  0.13606  0.79925 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)         1.964228   0.024236   81.05   &lt;2e-16 ***\n## Study_Hours_Per_Day 0.154061   0.003185   48.38   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2027 on 1998 degrees of freedom\n## Multiple R-squared:  0.5394, Adjusted R-squared:  0.5392 \n## F-statistic:  2340 on 1 and 1998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCoefficients:\n\nIntercept (1.96): Predicted GPA when Study_Hours_Per_Day = 0\nStudy_Hours_Per_Day (0.15): For each additional hour studied, GPA increases by 0.15 points\n\nModel Fit:\n\nR¬≤ (0.54): 54% of variance in GPA is explained by study hours\nAdjusted R¬≤ (0.54): R¬≤ adjusted for number of predictors\nF-statistic (2340): Overall model significance (p &lt; .001)\n\nStatistical Significance:\n\nBoth coefficients have p &lt; 0.001 (***), indicating they are significantly different from 0\n\nThe Regression Equation:\n\\[\\text{GPA} = 1.96 + 0.15 \\times \\text{Study Hours Per Day}\\]\nPrediction Example:\n# Predict score for someone who studies 8 hours per day\npredicted_gpa &lt;- 1.96 + 0.15 * 8\ncat(\"Predicted GPA for 8 study hours:\", predicted_gpa)\n## Predicted GPA for 8 study hours: 3.16\n# Or use predict()\npredict(simple_model, newdata = data.frame(Study_Hours_Per_Day = 8))\n##        1 \n## 3.196719\n\n\n\n\nMultiple regression includes multiple predictors:\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\epsilon\\]\nExample: Predict GPA from study hours per day AND college:\n# Multiple regression\nmultiple_model &lt;- lm(GPA ~ Study_Hours_Per_Day + college, data = student_data)\nsummary(multiple_model)\n## \n## Call:\n## lm(formula = GPA ~ Study_Hours_Per_Day + college, data = student_data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.59686 -0.13468 -0.00004  0.13485  0.80535 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)           1.982091   0.026106  75.925   &lt;2e-16 ***\n## Study_Hours_Per_Day   0.154089   0.003185  48.378   &lt;2e-16 ***\n## collegeCommunication -0.015106   0.016245  -0.930   0.3525    \n## collegeEducation     -0.016020   0.014551  -1.101   0.2711    \n## collegeHumanities    -0.024114   0.014151  -1.704   0.0885 .  \n## collegeMedicine      -0.029582   0.014500  -2.040   0.0415 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2027 on 1994 degrees of freedom\n## Multiple R-squared:  0.5405, Adjusted R-squared:  0.5394 \n## F-statistic: 469.2 on 5 and 1994 DF,  p-value: &lt; 2.2e-16\nInterpretation:\n\nIntercept (1.98): Predicted score for Arts & Sciences college (reference) with 0 study hours\nstudy_hours (0.15): Each additional hour increases GPA by 0.15 points, controlling for college\ncollegeCommunication (-0.02): Communication students have GPA 0.02 points lower than Arts & Sciences (not significant, p = 0.35)\ncollegeEducation (-0.02): Education students have GPA 0.02 points lower than Arts & Sciences (not significant, p = 0.27)\ncollegeHumanities (-0.02): Humanities students have GPA 0.02 points lower than Arts & Sciences (marginally significant, p = 0.09)\ncollegeMedicine (-0.03): Medicine students have GPA 0.03 points lower than Arts & Sciences (significant, p = 0.04)\n\nHow to report: A multiple linear regression was conducted to predict GPA from daily study hours and college. The overall model was significant, F(5, 1994) = 469.2, p &lt; .001, R¬≤ = .54, indicating that the predictors explained 54% of the variance in GPA. Study hours was a significant positive predictor of GPA (\\[\\beta\\] = 0.15, p &lt; .001), such that each additional hour of daily study was associated with a 0.15-point increase in GPA, controlling for college. Among colleges, only Medicine students differed significantly from Arts & Sciences students (\\[\\beta\\] = -0.03, p = .04), though this difference was negligible in magnitude.\n\n\n\n\nCategorical variables are automatically converted to dummy variables (0/1 indicators).\nFor college with 5 levels (Arts & Sciences, Communication, Education, Humanities and Medicine): - R creates 4 dummy variables (k-1 for k categories) - Reference category: Arts & Sciences (alphabetically first, or set manually) - collegeCommunication = 1 if Communication, 0 otherwise - collegeEducation = 1 if Education, 0 otherwise - collegeHumanities = 1 if Humanities, 0 otherwise - collegeMedicine = 1 if Medicine, 0 otherwise\nChanging the reference category:\n# Make CS the reference category\nstudent_data$college &lt;- factor(student_data$college, \n                             levels = c(\"Communication\", \"Arts & Sciences\", \"Education\", \"Humanities\", \"Medicine\"))\n\n# Refit model\nmultiple_model2 &lt;- lm(GPA ~ Study_Hours_Per_Day + college, data = student_data)\nsummary(multiple_model2)\n## \n## Call:\n## lm(formula = GPA ~ Study_Hours_Per_Day + college, data = student_data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.59686 -0.13468 -0.00004  0.13485  0.80535 \n## \n## Coefficients:\n##                          Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)             1.9669847  0.0269263  73.051   &lt;2e-16 ***\n## Study_Hours_Per_Day     0.1540891  0.0031851  48.378   &lt;2e-16 ***\n## collegeArts & Sciences  0.0151065  0.0162449   0.930    0.353    \n## collegeEducation       -0.0009134  0.0155413  -0.059    0.953    \n## collegeHumanities      -0.0090078  0.0151666  -0.594    0.553    \n## collegeMedicine        -0.0144753  0.0154902  -0.934    0.350    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2027 on 1994 degrees of freedom\n## Multiple R-squared:  0.5405, Adjusted R-squared:  0.5394 \n## F-statistic: 469.2 on 5 and 1994 DF,  p-value: &lt; 2.2e-16\nInterpretation:\n\nIntercept (1.97): Predicted GPA for Communication students (reference) with 0 study hours\nStudy_Hours_Per_Day (0.15): Each additional hour increases GPA by 0.15 points, controlling for college\ncollegeArts & Sciences (0.02): Arts & Sciences students have GPA 0.02 points higher than Communication (not significant, p = .35)\ncollegeEducation (-0.001): Education students have GPA 0.001 points lower than Communication (not significant, p = .95)\ncollegeHumanities (-0.01): Humanities students have GPA 0.01 points lower than Communication (not significant, p = .55)\ncollegeMedicine (-0.01): Medicine students have GPA 0.01 points lower than Communication (not significant, p = .35)\n\nHow to report: A multiple linear regression was conducted to predict GPA from daily study hours and college. The overall model was significant, F(5, 1994) = 469.2, p &lt; .001, R¬≤ = .54, indicating that the predictors explained 54% of the variance in GPA. Study hours was a significant positive predictor of GPA (Œ≤ = 0.15, p &lt; .001), such that each additional hour of daily study was associated with a 0.15-point increase in GPA, controlling for college. No significant differences in GPA were found between colleges (all p &gt; .05).\nNote: In this model, Communication is the reference category (now alphabetically first after releveling). Compare these results to the previous model where Arts & Sciences was the reference:\n\n\n\n\n\n\n\n\nCollege\nvs.¬†Arts & Sciences (previous)\nvs.¬†Communication (current)\n\n\n\n\nArts & Sciences\n‚Äî (reference)\n+0.02 (p = .35)\n\n\nCommunication\n-0.02 (p = .35)\n‚Äî (reference)\n\n\nEducation\n-0.02 (p = .27)\n-0.001 (p = .95)\n\n\nHumanities\n-0.02 (p = .09)\n-0.01 (p = .55)\n\n\nMedicine\n-0.03 (p = .04)\n-0.01 (p = .35)\n\n\n\nThe coefficients change because they now represent differences from Communication instead of Arts & Sciences. However, the overall model fit (R¬≤, F-statistic) and the effect of Study_Hours_Per_Day remain identical. Changing the reference category only changes how we interpret the college differences, not the underlying relationships in the data.\n# Reset to alphabetical order\n# Reset to alphabetical order\nstudent_data$college &lt;- as.character(student_data$college)  # Convert back to character first\nstudent_data$college &lt;- factor(student_data$college)        # Then re-factor (alphabetical by default)\nstr(student_data$college)\n##  Factor w/ 5 levels \"Arts & Sciences\",..: 3 4 1 2 4 3 2 4 4 4 ...\n\n\n\n\n\n\n\nSimple regression:\nggplot(student_data, aes(x = Study_Hours_Per_Day, y = GPA)) +\n  geom_point(color = \"steelblue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"GPA vs. Study Hours\",\n    subtitle = paste(\"R¬≤ =\", round(summary(simple_model)$r.squared, 3)),\n    x = \"Study Hours\",\n    y = \"GPA\"\n  ) +\n  theme_minimal()\n\nBy group:\nggplot(student_data, aes(x = Study_Hours_Per_Day, y = GPA, color = college)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"GPA vs. Study Hours by College\",\n    x = \"Study Hours\",\n    y = \"GPA\",\n    color = \"College\"\n  ) +\n  scale_color_brewer(palette = \"Set2\") +\n  theme_minimal()\n\n\n\n\n\nCoefficient plots visualize regression coefficients and their confidence intervals.\n# Get tidy coefficients\ncoef_data &lt;- tidy(multiple_model, conf.int = TRUE)\ncoef_data\n## # A tibble: 6 √ó 7\n##   term                 estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)            1.98     0.0261     75.9    0        1.93     2.03   \n## 2 Study_Hours_Per_Day    0.154    0.00319    48.4    0        0.148    0.160  \n## 3 collegeCommunication  -0.0151   0.0162     -0.930  0.353   -0.0470   0.0168 \n## 4 collegeEducation      -0.0160   0.0146     -1.10   0.271   -0.0446   0.0125 \n## 5 collegeHumanities     -0.0241   0.0142     -1.70   0.0885  -0.0519   0.00364\n## 6 collegeMedicine       -0.0296   0.0145     -2.04   0.0415  -0.0580  -0.00114\n# Remove intercept for better visualization\ncoef_plot_data &lt;- coef_data |&gt; \n  filter(term != \"(Intercept)\")\n\nggplot(coef_plot_data, aes(x = estimate, y = reorder(term, estimate))) +\n  geom_point(size = 3, color = \"steelblue\") +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Regression Coefficients\",\n    subtitle = \"Error bars show 95% confidence intervals\",\n    x = \"Coefficient Estimate\",\n    y = \"Predictor\"\n  ) +\n  theme_minimal()\n\nReading the coefficient plot:\n\nIf the confidence interval crosses the red dashed line (0), the coefficient is not statistically significant (p &gt; .05)\nIf the confidence interval does not cross 0, the coefficient is statistically significant (p &lt; .05)\nPoints to the right of 0 indicate positive relationships (as the predictor increases, GPA increases)\nPoints to the left of 0 indicate negative relationships (as the predictor increases, GPA decreases)\nThe further from 0, the stronger the effect\n\nWhy does crossing zero mean not significant?\nA coefficient of 0 means ‚Äúno effect‚Äù ‚Äî the predictor has no relationship with the outcome. The 95% confidence interval represents the range of plausible values for the true coefficient. If this range includes 0, we cannot rule out the possibility that the true effect is zero (i.e., no effect). Therefore, we cannot confidently say the predictor has a real effect, and the result is not statistically significant.\n\n\n\n\nMarginal effects show how predicted values change across levels of a predictor.\n# install.packages(\"ggeffects\")\nlibrary(ggeffects)\n\n# Marginal effects for study_hours\neffects_hours &lt;- ggpredict(multiple_model, terms = \"Study_Hours_Per_Day\")\n\nplot(effects_hours) +\n  labs(\n    title = \"Marginal Effect of Study Hours on GPA\",\n    x = \"Study Hours\",\n    y = \"Predicted GPA\"\n  )\n\n# Marginal effects for major\neffects_major &lt;- ggpredict(multiple_model, terms = \"college\")\n\nplot(effects_major) +\n  labs(\n    title = \"Predicted GPA by College\",\n    subtitle = \"Controlling for Study Hours\",\n    x = \"Major\",\n    y = \"Predicted GPA\"\n  )\n\n\n\n\n\nFit a simple regression predicting GPA from Sleep_Hours_Per_Day.\nAdd Stress_Level as a second predictor. Does it improve the model?\nCreate a coefficient plot for your multiple regression.\nInterpret the coefficients in plain language.\n\n### Your workspace\n\n\n\n\n\n\n\n\nLinear regression assumes the outcome is continuous and normally distributed. GLMs (Generalized Linear Models) extend regression to other types of outcomes:\n\n\n\n\n\n\n\n\n\n\nOutcome Type\nDistribution\nLink Function\nR Function\nExample\n\n\n\n\nContinuous\nNormal\nIdentity\nlm()\nGPA, income, height\n\n\nBinary (0/1)\nBinomial\nLogit\nglm(family = binomial)\nPass/fail, yes/no\n\n\nCount\nPoisson\nLog\nglm(family = poisson)\nNumber of events, frequency\n\n\n\n\n\n\n\nFor count data (e.g., number of books read):\n# Poisson regression\npoisson_model &lt;- glm(books_read ~ Study_Hours_Per_Day, \n                     family = poisson, \n                     data = student_data)\nsummary(poisson_model)\n## \n## Call:\n## glm(formula = books_read ~ Study_Hours_Per_Day, family = poisson, \n##     data = student_data)\n## \n## Coefficients:\n##                      Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)          1.641480   0.053498   30.68   &lt;2e-16 ***\n## Study_Hours_Per_Day -0.004785   0.007039   -0.68    0.497    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 2080.0  on 1999  degrees of freedom\n## Residual deviance: 2079.5  on 1998  degrees of freedom\n## AIC: 8799.6\n## \n## Number of Fisher Scoring iterations: 4\nInterpretation:\n\nIntercept (1.64): Log of expected books read when Study_Hours_Per_Day = 0\nStudy_Hours_Per_Day (-0.005): Each additional study hour decreases log(books) by 0.005 (not significant, p = .497)\n\nTo interpret on original scale, exponentiate:\n# Incident Rate Ratio\nexp(coef(poisson_model))\n##         (Intercept) Study_Hours_Per_Day \n##           5.1628048           0.9952264\n\nIntercept: exp(1.64) = 5.16 ‚Äî Students who study 0 hours are expected to read about 5 books\nStudy_Hours_Per_Day: exp(-0.005) = 0.995 ‚Äî Each additional study hour multiplies expected books read by 0.995 (a 0.5% decrease), but this is not significant\n\nConclusion: Study hours is not a significant predictor of books read (p = .497). The incident rate ratio of 0.995 is very close to 1, indicating essentially no relationship between study hours and number of books read.\nVisualize:\n# Create predictions\npred_data &lt;- data.frame(\n  Study_Hours_Per_Day = seq(min(student_data$Study_Hours_Per_Day), \n                    max(student_data$Study_Hours_Per_Day), \n                    length.out = 100)\n)\npred_data$predicted &lt;- predict(poisson_model, newdata = pred_data, type = \"response\")\n\nggplot(student_data, aes(x = Study_Hours_Per_Day, y = books_read)) +\n  geom_point(color = \"steelblue\", size = 3) +\n  geom_line(data = pred_data, aes(y = predicted), color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Poisson Regression: Books Read vs. Study Hours\",\n    x = \"Study Hours\",\n    y = \"Number of Books\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAn interaction occurs when the effect of one predictor depends on the level of another predictor.\nExample: Does the effect of study hours on GPA differ by Stress Level?\n# Model with interaction\ninteraction_model &lt;- lm(GPA ~ Study_Hours_Per_Day * Stress_Level, data = student_data)\nsummary(interaction_model)\n## \n## Call:\n## lm(formula = GPA ~ Study_Hours_Per_Day * Stress_Level, data = student_data)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.6142 -0.1346 -0.0006  0.1351  0.7902 \n## \n## Coefficients:\n##                                           Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)                               1.988037   0.043275  45.940   &lt;2e-16\n## Study_Hours_Per_Day                       0.151930   0.005106  29.757   &lt;2e-16\n## Stress_LevelLow                          -0.019358   0.234885  -0.082    0.934\n## Stress_LevelModerate                     -0.002176   0.101189  -0.022    0.983\n## Study_Hours_Per_Day:Stress_LevelLow       0.003007   0.042425   0.071    0.943\n## Study_Hours_Per_Day:Stress_LevelModerate -0.002857   0.014037  -0.204    0.839\n##                                             \n## (Intercept)                              ***\n## Study_Hours_Per_Day                      ***\n## Stress_LevelLow                             \n## Stress_LevelModerate                        \n## Study_Hours_Per_Day:Stress_LevelLow         \n## Study_Hours_Per_Day:Stress_LevelModerate    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2027 on 1994 degrees of freedom\n## Multiple R-squared:  0.5405, Adjusted R-squared:  0.5394 \n## F-statistic: 469.1 on 5 and 1994 DF,  p-value: &lt; 2.2e-16\nInterpretation:\n\nIntercept (1.99): Predicted GPA for High stress students (reference) with 0 study hours\nStudy_Hours_Per_Day (0.15): Effect of study hours on GPA for High stress students ‚Äî each additional hour increases GPA by 0.15 points\nStress_LevelLow (-0.02): Difference in intercept for Low vs.¬†High stress when study hours = 0 (not significant, p = .93)\nStress_LevelModerate (-0.002): Difference in intercept for Moderate vs.¬†High stress when study hours = 0 (not significant, p = .98)\nStudy_Hours_Per_Day:Stress_LevelLow (0.003): How much the slope differs for Low vs.¬†High stress students (not significant, p = .94)\nStudy_Hours_Per_Day:Stress_LevelModerate (-0.003): How much the slope differs for Moderate vs.¬†High stress students (not significant, p = .84)\n\nConclusion: The interaction terms are not significant (all p &gt; .05), indicating that the effect of study hours on GPA does not differ across stress levels. The relationship between study hours and GPA is consistent regardless of whether students have low, moderate, or high stress.\nHow to report: A multiple linear regression with interaction terms was conducted to examine whether the effect of daily study hours on GPA differed by stress level. The overall model was significant, F(5, 1994) = 469.1, p &lt; .001, R¬≤ = .54. Study hours was a significant positive predictor of GPA (Œ≤ = 0.15, p &lt; .001). However, the interaction terms between study hours and stress level were not significant (all p &gt; .83), indicating that the effect of study hours on GPA did not vary by stress level.\n\n\n\n\n# Marginal effects with interaction\neffects_interaction &lt;- ggpredict(interaction_model, terms = c(\"Study_Hours_Per_Day\", \"Stress_Level\"))\n\nplot(effects_interaction) +\n  labs(\n    title = \"Interaction: Study Hours √ó Stress Level\",\n    x = \"Study Hours\",\n    y = \"Predicted GPA\",\n    color = \"Stress Level\"\n  )\n\nVisual Interpretation of the Interaction Plot\nLooking at the interaction plot:\n\nX-axis (Study Hours): Ranges from 5 to 10 hours\nY-axis (Predicted GPA): Shows the expected GPA values\nThree lines: Representing the three stress levels (Moderate, Low, High)\n\nKey Observations:\n\nNearly Parallel Lines:\n\nAll three lines (Moderate, Low, and High stress) follow almost the same trajectory as study hours increase\nThey maintain roughly the same distance from each other throughout the range of study hours\nThis suggests that the effect of study hours on GPA is consistent across different stress levels\n\nPositive Slopes:\n\nAll lines show an upward trend, confirming that more study hours predict higher GPA regardless of stress level\nThe slopes appear very similar across all three stress levels\n\nMinimal Divergence:\n\nThe lines stay roughly equidistant from each other\nThere is no meaningful crossing or substantial convergence/divergence\nThe confidence intervals (shaded regions) overlap considerably\n\n\nInterpretation:\nNo Meaningful Interaction Effect\nThe parallel nature of these lines indicates that: - Study hours affect GPA in the same way regardless of whether a student experiences low, moderate, or high stress - A student who studies an additional hour can expect roughly the same GPA increase whether they are highly stressed or not stressed - Stress level does not modify the relationship between study hours and GPA\n\nGeneral Note on Interpreting Interaction Plots\nUnderstanding Line Patterns:\nParallel Lines = No Interaction When lines are parallel: - The effect of the predictor (e.g., study hours) is consistent across all levels of the moderator (e.g., stress level) - One variable does not change how the other variable affects the outcome - Example: If studying 1 more hour increases GPA by 0.2 points for low-stress students, it also increases GPA by approximately 0.2 points for high-stress students\nDiverging or Converging Lines = Interaction Present When lines diverge (spread apart) or converge (come together): - The effect of the predictor depends on the level of the moderator - The relationship between variables changes based on the context - Example: Studying might help low-stress students a lot (steep slope) but help high-stress students less (flatter slope)\nCrossing Lines = Strong Interaction When lines cross: - The direction of the effect can reverse depending on the moderator level - This is a particularly strong form of interaction - Example: At low study hours, stressed students might perform better, but at high study hours, non-stressed students might excel\nWhat to Look For: 1. Slope differences: Do the lines have different angles? 2. Spacing changes: Do the lines get closer or farther apart? 3. Crossings: Do any lines intersect? 4. Confidence intervals: Do the shaded regions overlap? (Wide overlap suggests uncertainty about whether lines truly differ)\nIn our case, the lines are nearly parallel with overlapping confidence intervals, strongly suggesting no interaction effect between study hours and stress level on GPA.\n\n\n\n\n\n\n\nLinear regression assumes:\n\nLinearity: Relationship between X and Y is linear\nIndependence: Observations are independent\nHomoscedasticity: Constant variance of residuals\nNormality: Residuals are normally distributed\n\n\n\n\n\nBuilt-in diagnostic plots:\nInterpreting the Diagnostic Plots:\n\nResiduals vs Fitted (Top Left):\n\nShould show no clear pattern\nPoints should be randomly scattered around the horizontal line at 0\nA pattern suggests non-linearity or heteroscedasticity\n\nQ-Q Plot (Top Right):\n\nPoints should fall along the diagonal line\nDeviations suggest residuals are not normally distributed\nParticularly concerning if points curve away at the ends\n\nScale-Location (Bottom Left):\n\nShould show a horizontal line with randomly scattered points\nTests for homoscedasticity (constant variance)\nAn upward or downward trend suggests heteroscedasticity\n\nResiduals vs Leverage (Bottom Right):\n\nIdentifies influential observations\nPoints outside Cook‚Äôs distance lines (dashed lines) are influential\nThese points have a large impact on the regression coefficients\n\n\n# Four diagnostic plots\npar(mfrow = c(2, 2))\nplot(simple_model)\n\npar(mfrow = c(1, 1))\nInterpreting Our Diagnostic Plots:\nLooking at our model‚Äôs diagnostic plots:\n\nResiduals vs Fitted (Top Left):\n\nPoints are randomly scattered around the horizontal line at 0\nNo clear pattern or curvature is visible\nThe red line is approximately horizontal\nConclusion: Linearity assumption is satisfied ‚úì\n\nQ-Q Plot (Top Right):\n\nPoints fall closely along the diagonal reference line\nMinor deviations at the extreme ends (points 702, 118, 485)\nMost points align well with the theoretical quantiles\nConclusion: Normality assumption is largely satisfied ‚úì\n\nScale-Location (Bottom Left):\n\nPoints are evenly distributed across fitted values\nThe red line is relatively flat with slight variation\nNo obvious funnel or cone shape\nConclusion: Homoscedasticity assumption is satisfied ‚úì\n\nResiduals vs Leverage (Bottom Right):\n\nMost points cluster near low leverage values\nA few points (702, 480, 52) have higher leverage\nNo points fall outside Cook‚Äôs distance lines (not visible on plot)\nConclusion: No highly influential outliers detected ‚úì\n\n\nOverall Assessment:\nOur interaction model meets all key assumptions of linear regression: - The relationship is linear - Residuals show constant variance - Residuals are approximately normally distributed - No problematic influential points\nThe model is reliable and the statistical inferences (p-values, confidence intervals) are valid.\n\n\n\n\nThe performance package provides easy model diagnostics:\n# install.packages(\"performance\")\nlibrary(performance)\n\n# Check all assumptions\ncheck_model(simple_model)\n\nAlternatively, use the performance package for comprehensive diagnostics:\nlibrary(performance)\ncheck_model(interaction_model)\n\nInterpreting the Posterior Predictive Check:\nThis plot compares the distribution of observed data (green line) with simulated data from the model (blue lines):\n\nGreen line: The actual distribution of GPA values in our dataset\nBlue lines: Multiple simulated datasets generated from our fitted model\nWhat to look for: The blue lines should closely match the green line\n\nIn our case:\n\nThe model-predicted distributions (blue) closely overlap with the observed data (green)\nBoth show a bell-shaped, approximately normal distribution centered around 3.0\nThe peak, spread, and shape are well-matched\nNo systematic deviations or mismatches are visible\nConclusion: The model accurately captures the distribution of GPA values ‚úì\n\nThis provides additional evidence that our model is well-specified and that the assumptions are met. The model not only fits individual data points well (as shown in the diagnostic plots) but also reproduces the overall distribution of the outcome variable.\n\nIndividual checks:\n# Normality of residuals\ncheck_normality(simple_model)\n## OK: residuals appear as normally distributed (p = 0.403).\n# Homoscedasticity\ncheck_heteroscedasticity(simple_model)\n## OK: Error variance appears to be homoscedastic (p = 0.917).\n# Multicollinearity (for multiple regression)\ncheck_collinearity(multiple_model)\n## # Check for Multicollinearity\n## \n## Low Correlation\n## \n##                 Term  VIF  VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n##  Study_Hours_Per_Day 1.00 [1.00, Inf]     1.00      1.00     [0.00, 1.00]\n##              college 1.00 [1.00, Inf]     1.00      1.00     [0.00, 1.00]\nPerformance Package Output:\nThe check_model() function provides a comprehensive summary of model diagnostics:\nNormality of Residuals: - Test result: OK (p = 0.403) - p-value &gt; 0.05 indicates we fail to reject the null hypothesis of normality - Conclusion: Residuals are normally distributed ‚úì\nHomoscedasticity (Constant Variance): - Test result: OK (p = 0.917) - p-value &gt; 0.05 indicates we fail to reject the null hypothesis of homoscedasticity - Conclusion: Error variance is constant across fitted values ‚úì\nMulticollinearity Check:\nMulticollinearity occurs when predictor variables are highly correlated with each other, which can make coefficient estimates unstable.\nVIF (Variance Inflation Factor) Interpretation: - VIF = 1: No correlation with other predictors - VIF &lt; 5: Low multicollinearity (acceptable) - VIF 5-10: Moderate multicollinearity (concerning) - VIF &gt; 10: High multicollinearity (problematic)\nIn our model: - Study_Hours_Per_Day: VIF = 1.00 - college: VIF = 1.00 - Both predictors have VIF = 1.00, indicating no multicollinearity - Tolerance values = 1.00 (inverse of VIF) confirm this - Conclusion: Predictors are independent; coefficient estimates are stable ‚úì\nOverall Model Assessment:\nAll diagnostic checks passed: ‚úì Linearity assumption met\n‚úì Normality of residuals confirmed\n‚úì Homoscedasticity confirmed\n‚úì No multicollinearity detected\n‚úì No influential outliers\nThe model is statistically sound and reliable for inference.\n\nCompare models:\nAfter fitting multiple models, we can compare their performance to determine which best explains the data.\nModels to Compare:\n\nSimple Model: GPA ~ Study_Hours_Per_Day\nMultiple Model: GPA ~ Study_Hours_Per_Day + college\n\nUse the compare_performance() function from the performance package:\ncompare_performance(simple_model, multiple_model)\n## # Comparison of Model Performance Indices\n## \n## Name           | Model |  AIC (weights) | AICc (weights) |  BIC (weights)\n## -------------------------------------------------------------------------\n## simple_model   |    lm | -703.5 (0.835) | -703.5 (0.838) | -686.7 (&gt;.999)\n## multiple_model |    lm | -700.3 (0.165) | -700.2 (0.162) | -661.1 (&lt;.001)\n## \n## Name           |    R2 | R2 (adj.) |  RMSE | Sigma\n## --------------------------------------------------\n## simple_model   | 0.539 |     0.539 | 0.203 | 0.203\n## multiple_model | 0.541 |     0.539 | 0.202 | 0.203\nUnderstanding the Output:\nThe comparison table provides several metrics to evaluate model fit:\nInformation Criteria (Lower is Better):\n\nAIC (Akaike Information Criterion): Balances model fit and complexity\n\nSimple model: -703.5 (weight = 0.835)\nMultiple model: -700.3 (weight = 0.165)\nLower AIC indicates better fit; simple model is preferred\n\nAICc (Corrected AIC): Adjusted for small sample sizes\n\nSimple model: -703.5 (weight = 0.838)\nMultiple model: -700.2 (weight = 0.162)\nResults align with AIC\n\nBIC (Bayesian Information Criterion): More strongly penalizes complexity\n\nSimple model: -686.7 (weight &gt; .999)\nMultiple model: -661.1 (weight &lt; .001)\nStrongly favors the simpler model\n\n\nModel Fit Metrics:\n\nR¬≤ (R-squared):\n\nSimple model: 0.539\nMultiple model: 0.541\nNearly identical; adding college explains only 0.2% more variance\n\nAdjusted R¬≤: Accounts for number of predictors\n\nBoth models: 0.539\nNo improvement when adjusting for additional predictors\n\nRMSE (Root Mean Square Error): Average prediction error\n\nSimple model: 0.203\nMultiple model: 0.202\nEssentially identical prediction accuracy\n\nSigma: Residual standard error\n\nBoth models: 0.203\nNo difference in error variance\n\n\nInterpretation:\nThe simple model (GPA ~ Study_Hours_Per_Day) is preferred because: - Lower AIC and BIC indicate better balance of fit and parsimony - Adding college as a predictor provides negligible improvement in R¬≤ - Prediction accuracy (RMSE) is virtually unchanged - The principle of parsimony suggests choosing the simpler model when performance is equivalent\nConclusion: Study hours alone adequately predict GPA; college affiliation does not meaningfully improve the model.\n\n\n\n\n\nUsing the mtcars dataset:\ndata(mtcars)\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nT-test: Compare mpg between automatic (am=0) and manual (am=1) transmissions.\nANOVA: Compare mpg across different numbers of cylinders (cyl).\nSimple Regression: Predict mpg from wt (weight). Interpret the coefficients.\nMultiple Regression: Add hp (horsepower) to your model. Does it improve fit?\nCoefficient Plot: Create a coefficient plot for your multiple regression.\nDiagnostics: Check the assumptions of your final model.\nReport: Write a brief summary of your findings as you would for an academic paper.\n\n### Your workspace\n\n\n\n\n\n\n\n\n\n\n\n\nFunction/Concept\nDescription\nCode Example\n\n\n\n\nt.test()\nCompare means of two groups\nt.test(y ~ group, data = df)\n\n\naov()\nOne-way ANOVA\naov(y ~ group, data = df)\n\n\nTukeyHSD()\nPost-hoc test after ANOVA\nTukeyHSD(aov_result)\n\n\nlm()\nLinear regression\nlm(y ~ x1 + x2, data = df)\n\n\nglm()\nGeneralized linear model\nglm(y ~ x, family = poisson)\n\n\nsummary()\nModel summary\nsummary(model)\n\n\ncoef()\nExtract coefficients\ncoef(model)\n\n\npredict()\nGenerate predictions\npredict(model, newdata)\n\n\nbroom::tidy()\nTidy model output\ntidy(model, conf.int = TRUE)\n\n\nbroom::glance()\nModel fit statistics\nglance(model)\n\n\ngeom_smooth(method=\"lm\")\nAdd regression line\ngeom_smooth(method = \"lm\")\n\n\nggeffects::ggpredict()\nMarginal effects\nggpredict(model, terms = \"x\")\n\n\nInteraction term\nEffect depends on another variable\nlm(y ~ x1 * x2)\n\n\nplot(model)\nDiagnostic plots\nplot(lm_model)\n\n\nperformance::check_model()\nCheck assumptions\ncheck_model(model)\n\n\ncompare_performance()\nCompare models\ncompare_performance(m1, m2)\n\n\nR¬≤\nVariance explained\nsummary(model)$r.squared\n\n\np-value\nStatistical significance\n&lt; 0.05 = significant\n\n\nConfidence interval\nRange of plausible values\n95% CI excludes 0 = significant"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L6-github.html#lecture-6-table-of-contents",
    "href": "Spring2026/LectureSlides/bigdata_L6-github.html#lecture-6-table-of-contents",
    "title": "Statistics",
    "section": "",
    "text": "Section\nTopic\n\n\n\n\n1\nHypothesis Testing\n\n\n1.1\nWhat is Hypothesis Testing?\n\n\n1.2\nT-Test: Comparing Two Groups\n\n\n1.3\nVisualizing T-Tests\n\n\n1.4\nANOVA: Comparing Multiple Groups\n\n\n1.5\nVisualizing ANOVA\n\n\n2\nRegression Models\n\n\n2.1\nSimple Linear Regression\n\n\n2.2\nInterpreting Regression Output\n\n\n2.3\nMultiple Linear Regression\n\n\n2.4\nDummy Variables\n\n\n3\nVisualizing Regression\n\n\n3.1\nScatter Plots with Regression Lines\n\n\n3.2\nCoefficient Plots\n\n\n3.3\nMarginal Effects Plots\n\n\n4\nGeneralized Linear Models\n\n\n4.1\nWhen to Use GLMs\n\n\n4.2\nPoisson Regression\n\n\n5\nInteraction Terms\n\n\n5.1\nWhat are Interactions?\n\n\n5.2\nVisualizing Interactions\n\n\n6\nModel Diagnostics\n\n\n6.1\nAssumptions of Linear Regression\n\n\n6.2\nChecking Assumptions\n\n\n6.3\nThe performance Package\n\n\n\n\nALWAYS load our libraries first\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(broom)"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L6-github.html#setup-create-example-dataset",
    "href": "Spring2026/LectureSlides/bigdata_L6-github.html#setup-create-example-dataset",
    "title": "Statistics",
    "section": "",
    "text": "We‚Äôll use the same student dataset throughout this lecture:\n# Create dataset\nstudent_data &lt;- read_csv(\"https://raw.githubusercontent.com/aysedeniz09/IntroCSS/refs/heads/main/data/student_lifestyle_dataset.csv\")\n\nhead(student_data)\n## # A tibble: 6 √ó 12\n##   Student_ID Study_Hours_Per_Day Extracurricular_Hours_Per‚Ä¶¬π Sleep_Hours_Per_Day\n##        &lt;dbl&gt;               &lt;dbl&gt;                       &lt;dbl&gt;               &lt;dbl&gt;\n## 1          1                 6.9                         3.8                 8.7\n## 2          2                 5.3                         3.5                 8  \n## 3          3                 5.1                         3.9                 9.2\n## 4          4                 6.5                         2.1                 7.2\n## 5          5                 8.1                         0.6                 6.5\n## 6          6                 6                           2.1                 8  \n## # ‚Ñπ abbreviated name: ¬π‚ÄãExtracurricular_Hours_Per_Day\n## # ‚Ñπ 8 more variables: Social_Hours_Per_Day &lt;dbl&gt;,\n## #   Physical_Activity_Hours_Per_Day &lt;dbl&gt;, GPA &lt;dbl&gt;, Stress_Level &lt;chr&gt;,\n## #   college &lt;chr&gt;, GPA_Level &lt;chr&gt;, Stress_Level2 &lt;chr&gt;, books_read &lt;dbl&gt;"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L6-github.html#hypothesis-testing",
    "href": "Spring2026/LectureSlides/bigdata_L6-github.html#hypothesis-testing",
    "title": "Statistics",
    "section": "",
    "text": "Hypothesis testing allows you to make inferences about populations based on sample data. It answers: ‚ÄúIs the pattern I see in my data real, or could it be due to chance?‚Äù\nKey Components:\n\nNull Hypothesis (H‚ÇÄ): No effect or difference exists\nAlternative Hypothesis (H‚ÇÅ): An effect or difference exists\np-value: Probability of observing your data if H‚ÇÄ is true\nSignificance level (Œ±): Threshold for rejecting H‚ÇÄ (usually 0.05)\n\nDecision Rule:\n\nIf p &lt; Œ±: Reject H‚ÇÄ (result is statistically significant)\nIf p ‚â• Œ±: Fail to reject H‚ÇÄ (result is not statistically significant)\n\nImportant: ‚ÄúFail to reject‚Äù ‚â† ‚ÄúAccept H‚ÇÄ‚Äù. We never prove the null hypothesis is true!\n\n\n\n\nA t-test compares the means of two groups to determine if they are significantly different.\nTypes of T-Tests:\n\n\n\n\n\n\n\n\nType\nUse Case\nExample\n\n\n\n\nIndependent\nTwo unrelated groups\nMales vs.¬†Females\n\n\nPaired\nSame subjects, two conditions\nBefore vs.¬†After treatment\n\n\nOne-sample\nSample vs.¬†known value\nSample mean vs.¬†population mean\n\n\n\nExample: Do male and female CS students study different hours?\n# Filter for CS students\nAS_students &lt;- student_data |&gt; \n  filter(college == \"Arts & Sciences\")\n\n# Independent t-test\nt_test_result &lt;- t.test(Study_Hours_Per_Day ~ Stress_Level2, data = AS_students)\nt_test_result\n## \n##  Welch Two Sample t-test\n## \n## data:  Study_Hours_Per_Day by Stress_Level2\n## t = 15.245, df = 310.19, p-value &lt; 2.2e-16\n## alternative hypothesis: true difference in means between group High and group LowModerate is not equal to 0\n## 95 percent confidence interval:\n##  1.526264 1.978624\n## sample estimates:\n##        mean in group High mean in group LowModerate \n##                  8.362941                  6.610497\nInterpreting the Output:\n\nt-statistic (t = 15.245): How many standard errors the difference is from 0\ndf (df = 310.19): Degrees of freedom (affects the t-distribution)\np-value (p &lt; 2.2e-16): Probability of seeing this difference by chance\n95% CI ([1.53, 1.98]): Range likely containing the true difference in means\nGroup means: High Stress = 8.36 hours, Low/Moderate Stress = 6.61 hours\n\nConclusion: p &lt; 0.001, so we reject H‚ÇÄ. There is a significant difference in study hours between stress levels. Students with high stress study approximately 1.75 more hours per day than students with low/moderate stress.\nHow to report: A Welch two-sample t-test revealed a significant difference in daily study hours between high stress (M = 8.36) and low/moderate stress (M = 6.61) students, t(310.19) = 15.25, p &lt; .001, 95% CI [1.53, 1.98].\n\n\n\n\nBoxplot with means:\nggplot(AS_students, aes(x = Stress_Level2, y = Study_Hours_Per_Day, fill = Stress_Level2)) +\n  geom_boxplot(alpha = 0.7) +\n  stat_summary(fun = mean, geom = \"point\", shape = 18, size = 4, color = \"black\") +\n  labs(\n    title = \"Study Hours by Stress Level (Arts & Sciences Students)\",\n    subtitle = paste(\"t-test p-value =\", round(t_test_result$p.value, 3)),\n    x = \"Stress Level\",\n    y = \"Study Hours\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nViolin plot:\nggplot(AS_students, aes(x = Stress_Level2, y = Study_Hours_Per_Day, fill = Stress_Level2)) +\n  geom_violin(trim = FALSE, alpha = 0.7) +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  stat_summary(fun = mean, geom = \"point\", shape = 18, size = 3, color = \"red\") +\n  labs(\n    title = \"Distribution of Study Hours by Stress Level\",\n    x = \"Stress Level\",\n    y = \"Study Hours\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nBar plot with error bars:\n# Calculate summary statistics\nstress_summary &lt;- AS_students |&gt; \n  group_by(Stress_Level2) |&gt; \n  summarize(\n    mean = mean(Study_Hours_Per_Day),\n    se = sd(Study_Hours_Per_Day) / sqrt(n()),\n    .groups = \"drop\"\n  )\n\nggplot(stress_summary, aes(x = Stress_Level2, y = mean, fill = Stress_Level2)) +\n  geom_col(alpha = 0.7) +\n  geom_errorbar(aes(ymin = mean - 1.96*se, ymax = mean + 1.96*se), \n                width = 0.2) +\n  geom_text(aes(label = round(mean, 1)), vjust = -1.5) +\n  labs(\n    title = \"Mean Study Hours by Stress Level (¬±95% CI)\",\n    x = \"Stress Level\",\n    y = \"Mean Study Hours\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nANOVA (Analysis of Variance) extends the t-test to compare means across three or more groups.\n\nH‚ÇÄ: All group means are equal (Œº‚ÇÅ = Œº‚ÇÇ = Œº‚ÇÉ)\nH‚ÇÅ: At least one group mean is different\n\nExample: Do students from different majors study different hours?\n# One-way ANOVA\nanova_result &lt;- aov(Study_Hours_Per_Day ~ college, data = student_data)\nsummary(anova_result)\n##               Df Sum Sq Mean Sq F value Pr(&gt;F)\n## college        4      3  0.6442   0.317  0.867\n## Residuals   1995   4050  2.0302\nInterpreting the Output:\n\nF value (0.317): Ratio of between-group variance to within-group variance\nPr(&gt;F) (0.867): p-value for the F-test\n\nConclusion: p = 0.867 &gt; 0.05, so we fail to reject H‚ÇÄ. There is no significant difference in GPA across colleges.\nPost-hoc test: Which majors are different from each other?\n# Tukey's HSD (Honestly Significant Difference)\nTukeyHSD(anova_result)\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = Study_Hours_Per_Day ~ college, data = student_data)\n## \n## $college\n##                                      diff        lwr       upr     p adj\n## Communication-Arts & Sciences  0.09074074 -0.2209765 0.4024580 0.9321845\n## Education-Arts & Sciences     -0.01271548 -0.2919796 0.2665486 0.9999462\n## Humanities-Arts & Sciences    -0.00905683 -0.2806320 0.2625183 0.9999845\n## Medicine-Arts & Sciences       0.04006047 -0.2382109 0.3183318 0.9949622\n## Education-Communication       -0.10345622 -0.4016537 0.1947413 0.8783008\n## Humanities-Communication      -0.09979757 -0.3908068 0.1912116 0.8827707\n## Medicine-Communication        -0.05068027 -0.3479482 0.2465877 0.9903715\n## Humanities-Education           0.00365865 -0.2522850 0.2596023 0.9999995\n## Medicine-Education             0.05277595 -0.2102621 0.3158140 0.9822233\n## Medicine-Humanities            0.04911730 -0.2057428 0.3039774 0.9847059\nHow to report: A one-way ANOVA revealed no significant differences in daily study hours among colleges, F(4, 1995) = 0.32, p = .867. Post-hoc Tukey tests confirmed no significant pairwise differences between any colleges (all p &gt; .05).\n\n\n\n\nggplot(student_data, aes(x = college, y = Study_Hours_Per_Day, fill = college)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 18, size = 4, color = \"black\") +\n  labs(\n    title = \"Study Hours by College\",\n    subtitle = paste(\"ANOVA F(4, 1995) =\", round(summary(anova_result)[[1]]$`F value`[1], 2), \n                     \", p = 0.867\"),\n    x = \"College\",\n    y = \"Study Hours\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nComparison Table:\n\n\n\nTest\nGroups\nCompares\nStatistic\n\n\n\n\nT-test\n2\nMeans\nt-statistic\n\n\nANOVA\n3+\nMeans\nF-statistic\n\n\n\n\n\n\n\nPerform a t-test comparing GPA for all stress levels (all colleges).\nPerform an ANOVA comparing GPA across colleges\nIf ANOVA is significant, run Tukey‚Äôs HSD to identify which colleges differ.\nVisualize your results with appropriate plots.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L6-github.html#regression-models",
    "href": "Spring2026/LectureSlides/bigdata_L6-github.html#regression-models",
    "title": "Statistics",
    "section": "",
    "text": "Regression models examine the relationship between variables and allow us to predict outcomes.\n\n\nA simple linear regression models the relationship between one predictor (X) and one outcome (Y):\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\nWhere: - \\(\\beta_0\\) = Intercept (predicted Y when X = 0) - \\(\\beta_1\\) = Slope (change in Y for one-unit increase in X) - \\(\\epsilon\\) = Error term\nExample: Does study time predict exam scores?\n# Fit simple linear regression\nsimple_model &lt;- lm(GPA ~ Study_Hours_Per_Day, data = student_data)\nsummary(simple_model)\n## \n## Call:\n## lm(formula = GPA ~ Study_Hours_Per_Day, data = student_data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.60834 -0.13516 -0.00103  0.13606  0.79925 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)         1.964228   0.024236   81.05   &lt;2e-16 ***\n## Study_Hours_Per_Day 0.154061   0.003185   48.38   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2027 on 1998 degrees of freedom\n## Multiple R-squared:  0.5394, Adjusted R-squared:  0.5392 \n## F-statistic:  2340 on 1 and 1998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCoefficients:\n\nIntercept (1.96): Predicted GPA when Study_Hours_Per_Day = 0\nStudy_Hours_Per_Day (0.15): For each additional hour studied, GPA increases by 0.15 points\n\nModel Fit:\n\nR¬≤ (0.54): 54% of variance in GPA is explained by study hours\nAdjusted R¬≤ (0.54): R¬≤ adjusted for number of predictors\nF-statistic (2340): Overall model significance (p &lt; .001)\n\nStatistical Significance:\n\nBoth coefficients have p &lt; 0.001 (***), indicating they are significantly different from 0\n\nThe Regression Equation:\n\\[\\text{GPA} = 1.96 + 0.15 \\times \\text{Study Hours Per Day}\\]\nPrediction Example:\n# Predict score for someone who studies 8 hours per day\npredicted_gpa &lt;- 1.96 + 0.15 * 8\ncat(\"Predicted GPA for 8 study hours:\", predicted_gpa)\n## Predicted GPA for 8 study hours: 3.16\n# Or use predict()\npredict(simple_model, newdata = data.frame(Study_Hours_Per_Day = 8))\n##        1 \n## 3.196719\n\n\n\n\nMultiple regression includes multiple predictors:\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\epsilon\\]\nExample: Predict GPA from study hours per day AND college:\n# Multiple regression\nmultiple_model &lt;- lm(GPA ~ Study_Hours_Per_Day + college, data = student_data)\nsummary(multiple_model)\n## \n## Call:\n## lm(formula = GPA ~ Study_Hours_Per_Day + college, data = student_data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.59686 -0.13468 -0.00004  0.13485  0.80535 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)           1.982091   0.026106  75.925   &lt;2e-16 ***\n## Study_Hours_Per_Day   0.154089   0.003185  48.378   &lt;2e-16 ***\n## collegeCommunication -0.015106   0.016245  -0.930   0.3525    \n## collegeEducation     -0.016020   0.014551  -1.101   0.2711    \n## collegeHumanities    -0.024114   0.014151  -1.704   0.0885 .  \n## collegeMedicine      -0.029582   0.014500  -2.040   0.0415 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2027 on 1994 degrees of freedom\n## Multiple R-squared:  0.5405, Adjusted R-squared:  0.5394 \n## F-statistic: 469.2 on 5 and 1994 DF,  p-value: &lt; 2.2e-16\nInterpretation:\n\nIntercept (1.98): Predicted score for Arts & Sciences college (reference) with 0 study hours\nstudy_hours (0.15): Each additional hour increases GPA by 0.15 points, controlling for college\ncollegeCommunication (-0.02): Communication students have GPA 0.02 points lower than Arts & Sciences (not significant, p = 0.35)\ncollegeEducation (-0.02): Education students have GPA 0.02 points lower than Arts & Sciences (not significant, p = 0.27)\ncollegeHumanities (-0.02): Humanities students have GPA 0.02 points lower than Arts & Sciences (marginally significant, p = 0.09)\ncollegeMedicine (-0.03): Medicine students have GPA 0.03 points lower than Arts & Sciences (significant, p = 0.04)\n\nHow to report: A multiple linear regression was conducted to predict GPA from daily study hours and college. The overall model was significant, F(5, 1994) = 469.2, p &lt; .001, R¬≤ = .54, indicating that the predictors explained 54% of the variance in GPA. Study hours was a significant positive predictor of GPA (\\[\\beta\\] = 0.15, p &lt; .001), such that each additional hour of daily study was associated with a 0.15-point increase in GPA, controlling for college. Among colleges, only Medicine students differed significantly from Arts & Sciences students (\\[\\beta\\] = -0.03, p = .04), though this difference was negligible in magnitude.\n\n\n\n\nCategorical variables are automatically converted to dummy variables (0/1 indicators).\nFor college with 5 levels (Arts & Sciences, Communication, Education, Humanities and Medicine): - R creates 4 dummy variables (k-1 for k categories) - Reference category: Arts & Sciences (alphabetically first, or set manually) - collegeCommunication = 1 if Communication, 0 otherwise - collegeEducation = 1 if Education, 0 otherwise - collegeHumanities = 1 if Humanities, 0 otherwise - collegeMedicine = 1 if Medicine, 0 otherwise\nChanging the reference category:\n# Make CS the reference category\nstudent_data$college &lt;- factor(student_data$college, \n                             levels = c(\"Communication\", \"Arts & Sciences\", \"Education\", \"Humanities\", \"Medicine\"))\n\n# Refit model\nmultiple_model2 &lt;- lm(GPA ~ Study_Hours_Per_Day + college, data = student_data)\nsummary(multiple_model2)\n## \n## Call:\n## lm(formula = GPA ~ Study_Hours_Per_Day + college, data = student_data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.59686 -0.13468 -0.00004  0.13485  0.80535 \n## \n## Coefficients:\n##                          Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)             1.9669847  0.0269263  73.051   &lt;2e-16 ***\n## Study_Hours_Per_Day     0.1540891  0.0031851  48.378   &lt;2e-16 ***\n## collegeArts & Sciences  0.0151065  0.0162449   0.930    0.353    \n## collegeEducation       -0.0009134  0.0155413  -0.059    0.953    \n## collegeHumanities      -0.0090078  0.0151666  -0.594    0.553    \n## collegeMedicine        -0.0144753  0.0154902  -0.934    0.350    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2027 on 1994 degrees of freedom\n## Multiple R-squared:  0.5405, Adjusted R-squared:  0.5394 \n## F-statistic: 469.2 on 5 and 1994 DF,  p-value: &lt; 2.2e-16\nInterpretation:\n\nIntercept (1.97): Predicted GPA for Communication students (reference) with 0 study hours\nStudy_Hours_Per_Day (0.15): Each additional hour increases GPA by 0.15 points, controlling for college\ncollegeArts & Sciences (0.02): Arts & Sciences students have GPA 0.02 points higher than Communication (not significant, p = .35)\ncollegeEducation (-0.001): Education students have GPA 0.001 points lower than Communication (not significant, p = .95)\ncollegeHumanities (-0.01): Humanities students have GPA 0.01 points lower than Communication (not significant, p = .55)\ncollegeMedicine (-0.01): Medicine students have GPA 0.01 points lower than Communication (not significant, p = .35)\n\nHow to report: A multiple linear regression was conducted to predict GPA from daily study hours and college. The overall model was significant, F(5, 1994) = 469.2, p &lt; .001, R¬≤ = .54, indicating that the predictors explained 54% of the variance in GPA. Study hours was a significant positive predictor of GPA (Œ≤ = 0.15, p &lt; .001), such that each additional hour of daily study was associated with a 0.15-point increase in GPA, controlling for college. No significant differences in GPA were found between colleges (all p &gt; .05).\nNote: In this model, Communication is the reference category (now alphabetically first after releveling). Compare these results to the previous model where Arts & Sciences was the reference:\n\n\n\n\n\n\n\n\nCollege\nvs.¬†Arts & Sciences (previous)\nvs.¬†Communication (current)\n\n\n\n\nArts & Sciences\n‚Äî (reference)\n+0.02 (p = .35)\n\n\nCommunication\n-0.02 (p = .35)\n‚Äî (reference)\n\n\nEducation\n-0.02 (p = .27)\n-0.001 (p = .95)\n\n\nHumanities\n-0.02 (p = .09)\n-0.01 (p = .55)\n\n\nMedicine\n-0.03 (p = .04)\n-0.01 (p = .35)\n\n\n\nThe coefficients change because they now represent differences from Communication instead of Arts & Sciences. However, the overall model fit (R¬≤, F-statistic) and the effect of Study_Hours_Per_Day remain identical. Changing the reference category only changes how we interpret the college differences, not the underlying relationships in the data.\n# Reset to alphabetical order\n# Reset to alphabetical order\nstudent_data$college &lt;- as.character(student_data$college)  # Convert back to character first\nstudent_data$college &lt;- factor(student_data$college)        # Then re-factor (alphabetical by default)\nstr(student_data$college)\n##  Factor w/ 5 levels \"Arts & Sciences\",..: 3 4 1 2 4 3 2 4 4 4 ..."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L6-github.html#visualizing-regression",
    "href": "Spring2026/LectureSlides/bigdata_L6-github.html#visualizing-regression",
    "title": "Statistics",
    "section": "",
    "text": "Simple regression:\nggplot(student_data, aes(x = Study_Hours_Per_Day, y = GPA)) +\n  geom_point(color = \"steelblue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"GPA vs. Study Hours\",\n    subtitle = paste(\"R¬≤ =\", round(summary(simple_model)$r.squared, 3)),\n    x = \"Study Hours\",\n    y = \"GPA\"\n  ) +\n  theme_minimal()\n\nBy group:\nggplot(student_data, aes(x = Study_Hours_Per_Day, y = GPA, color = college)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"GPA vs. Study Hours by College\",\n    x = \"Study Hours\",\n    y = \"GPA\",\n    color = \"College\"\n  ) +\n  scale_color_brewer(palette = \"Set2\") +\n  theme_minimal()\n\n\n\n\n\nCoefficient plots visualize regression coefficients and their confidence intervals.\n# Get tidy coefficients\ncoef_data &lt;- tidy(multiple_model, conf.int = TRUE)\ncoef_data\n## # A tibble: 6 √ó 7\n##   term                 estimate std.error statistic p.value conf.low conf.high\n##   &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)            1.98     0.0261     75.9    0        1.93     2.03   \n## 2 Study_Hours_Per_Day    0.154    0.00319    48.4    0        0.148    0.160  \n## 3 collegeCommunication  -0.0151   0.0162     -0.930  0.353   -0.0470   0.0168 \n## 4 collegeEducation      -0.0160   0.0146     -1.10   0.271   -0.0446   0.0125 \n## 5 collegeHumanities     -0.0241   0.0142     -1.70   0.0885  -0.0519   0.00364\n## 6 collegeMedicine       -0.0296   0.0145     -2.04   0.0415  -0.0580  -0.00114\n# Remove intercept for better visualization\ncoef_plot_data &lt;- coef_data |&gt; \n  filter(term != \"(Intercept)\")\n\nggplot(coef_plot_data, aes(x = estimate, y = reorder(term, estimate))) +\n  geom_point(size = 3, color = \"steelblue\") +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Regression Coefficients\",\n    subtitle = \"Error bars show 95% confidence intervals\",\n    x = \"Coefficient Estimate\",\n    y = \"Predictor\"\n  ) +\n  theme_minimal()\n\nReading the coefficient plot:\n\nIf the confidence interval crosses the red dashed line (0), the coefficient is not statistically significant (p &gt; .05)\nIf the confidence interval does not cross 0, the coefficient is statistically significant (p &lt; .05)\nPoints to the right of 0 indicate positive relationships (as the predictor increases, GPA increases)\nPoints to the left of 0 indicate negative relationships (as the predictor increases, GPA decreases)\nThe further from 0, the stronger the effect\n\nWhy does crossing zero mean not significant?\nA coefficient of 0 means ‚Äúno effect‚Äù ‚Äî the predictor has no relationship with the outcome. The 95% confidence interval represents the range of plausible values for the true coefficient. If this range includes 0, we cannot rule out the possibility that the true effect is zero (i.e., no effect). Therefore, we cannot confidently say the predictor has a real effect, and the result is not statistically significant.\n\n\n\n\nMarginal effects show how predicted values change across levels of a predictor.\n# install.packages(\"ggeffects\")\nlibrary(ggeffects)\n\n# Marginal effects for study_hours\neffects_hours &lt;- ggpredict(multiple_model, terms = \"Study_Hours_Per_Day\")\n\nplot(effects_hours) +\n  labs(\n    title = \"Marginal Effect of Study Hours on GPA\",\n    x = \"Study Hours\",\n    y = \"Predicted GPA\"\n  )\n\n# Marginal effects for major\neffects_major &lt;- ggpredict(multiple_model, terms = \"college\")\n\nplot(effects_major) +\n  labs(\n    title = \"Predicted GPA by College\",\n    subtitle = \"Controlling for Study Hours\",\n    x = \"Major\",\n    y = \"Predicted GPA\"\n  )\n\n\n\n\n\nFit a simple regression predicting GPA from Sleep_Hours_Per_Day.\nAdd Stress_Level as a second predictor. Does it improve the model?\nCreate a coefficient plot for your multiple regression.\nInterpret the coefficients in plain language.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L6-github.html#generalized-linear-models-glms",
    "href": "Spring2026/LectureSlides/bigdata_L6-github.html#generalized-linear-models-glms",
    "title": "Statistics",
    "section": "",
    "text": "Linear regression assumes the outcome is continuous and normally distributed. GLMs (Generalized Linear Models) extend regression to other types of outcomes:\n\n\n\n\n\n\n\n\n\n\nOutcome Type\nDistribution\nLink Function\nR Function\nExample\n\n\n\n\nContinuous\nNormal\nIdentity\nlm()\nGPA, income, height\n\n\nBinary (0/1)\nBinomial\nLogit\nglm(family = binomial)\nPass/fail, yes/no\n\n\nCount\nPoisson\nLog\nglm(family = poisson)\nNumber of events, frequency\n\n\n\n\n\n\n\nFor count data (e.g., number of books read):\n# Poisson regression\npoisson_model &lt;- glm(books_read ~ Study_Hours_Per_Day, \n                     family = poisson, \n                     data = student_data)\nsummary(poisson_model)\n## \n## Call:\n## glm(formula = books_read ~ Study_Hours_Per_Day, family = poisson, \n##     data = student_data)\n## \n## Coefficients:\n##                      Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)          1.641480   0.053498   30.68   &lt;2e-16 ***\n## Study_Hours_Per_Day -0.004785   0.007039   -0.68    0.497    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 2080.0  on 1999  degrees of freedom\n## Residual deviance: 2079.5  on 1998  degrees of freedom\n## AIC: 8799.6\n## \n## Number of Fisher Scoring iterations: 4\nInterpretation:\n\nIntercept (1.64): Log of expected books read when Study_Hours_Per_Day = 0\nStudy_Hours_Per_Day (-0.005): Each additional study hour decreases log(books) by 0.005 (not significant, p = .497)\n\nTo interpret on original scale, exponentiate:\n# Incident Rate Ratio\nexp(coef(poisson_model))\n##         (Intercept) Study_Hours_Per_Day \n##           5.1628048           0.9952264\n\nIntercept: exp(1.64) = 5.16 ‚Äî Students who study 0 hours are expected to read about 5 books\nStudy_Hours_Per_Day: exp(-0.005) = 0.995 ‚Äî Each additional study hour multiplies expected books read by 0.995 (a 0.5% decrease), but this is not significant\n\nConclusion: Study hours is not a significant predictor of books read (p = .497). The incident rate ratio of 0.995 is very close to 1, indicating essentially no relationship between study hours and number of books read.\nVisualize:\n# Create predictions\npred_data &lt;- data.frame(\n  Study_Hours_Per_Day = seq(min(student_data$Study_Hours_Per_Day), \n                    max(student_data$Study_Hours_Per_Day), \n                    length.out = 100)\n)\npred_data$predicted &lt;- predict(poisson_model, newdata = pred_data, type = \"response\")\n\nggplot(student_data, aes(x = Study_Hours_Per_Day, y = books_read)) +\n  geom_point(color = \"steelblue\", size = 3) +\n  geom_line(data = pred_data, aes(y = predicted), color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Poisson Regression: Books Read vs. Study Hours\",\n    x = \"Study Hours\",\n    y = \"Number of Books\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L6-github.html#interaction-terms",
    "href": "Spring2026/LectureSlides/bigdata_L6-github.html#interaction-terms",
    "title": "Statistics",
    "section": "",
    "text": "An interaction occurs when the effect of one predictor depends on the level of another predictor.\nExample: Does the effect of study hours on GPA differ by Stress Level?\n# Model with interaction\ninteraction_model &lt;- lm(GPA ~ Study_Hours_Per_Day * Stress_Level, data = student_data)\nsummary(interaction_model)\n## \n## Call:\n## lm(formula = GPA ~ Study_Hours_Per_Day * Stress_Level, data = student_data)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.6142 -0.1346 -0.0006  0.1351  0.7902 \n## \n## Coefficients:\n##                                           Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)                               1.988037   0.043275  45.940   &lt;2e-16\n## Study_Hours_Per_Day                       0.151930   0.005106  29.757   &lt;2e-16\n## Stress_LevelLow                          -0.019358   0.234885  -0.082    0.934\n## Stress_LevelModerate                     -0.002176   0.101189  -0.022    0.983\n## Study_Hours_Per_Day:Stress_LevelLow       0.003007   0.042425   0.071    0.943\n## Study_Hours_Per_Day:Stress_LevelModerate -0.002857   0.014037  -0.204    0.839\n##                                             \n## (Intercept)                              ***\n## Study_Hours_Per_Day                      ***\n## Stress_LevelLow                             \n## Stress_LevelModerate                        \n## Study_Hours_Per_Day:Stress_LevelLow         \n## Study_Hours_Per_Day:Stress_LevelModerate    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2027 on 1994 degrees of freedom\n## Multiple R-squared:  0.5405, Adjusted R-squared:  0.5394 \n## F-statistic: 469.1 on 5 and 1994 DF,  p-value: &lt; 2.2e-16\nInterpretation:\n\nIntercept (1.99): Predicted GPA for High stress students (reference) with 0 study hours\nStudy_Hours_Per_Day (0.15): Effect of study hours on GPA for High stress students ‚Äî each additional hour increases GPA by 0.15 points\nStress_LevelLow (-0.02): Difference in intercept for Low vs.¬†High stress when study hours = 0 (not significant, p = .93)\nStress_LevelModerate (-0.002): Difference in intercept for Moderate vs.¬†High stress when study hours = 0 (not significant, p = .98)\nStudy_Hours_Per_Day:Stress_LevelLow (0.003): How much the slope differs for Low vs.¬†High stress students (not significant, p = .94)\nStudy_Hours_Per_Day:Stress_LevelModerate (-0.003): How much the slope differs for Moderate vs.¬†High stress students (not significant, p = .84)\n\nConclusion: The interaction terms are not significant (all p &gt; .05), indicating that the effect of study hours on GPA does not differ across stress levels. The relationship between study hours and GPA is consistent regardless of whether students have low, moderate, or high stress.\nHow to report: A multiple linear regression with interaction terms was conducted to examine whether the effect of daily study hours on GPA differed by stress level. The overall model was significant, F(5, 1994) = 469.1, p &lt; .001, R¬≤ = .54. Study hours was a significant positive predictor of GPA (Œ≤ = 0.15, p &lt; .001). However, the interaction terms between study hours and stress level were not significant (all p &gt; .83), indicating that the effect of study hours on GPA did not vary by stress level.\n\n\n\n\n# Marginal effects with interaction\neffects_interaction &lt;- ggpredict(interaction_model, terms = c(\"Study_Hours_Per_Day\", \"Stress_Level\"))\n\nplot(effects_interaction) +\n  labs(\n    title = \"Interaction: Study Hours √ó Stress Level\",\n    x = \"Study Hours\",\n    y = \"Predicted GPA\",\n    color = \"Stress Level\"\n  )\n\nVisual Interpretation of the Interaction Plot\nLooking at the interaction plot:\n\nX-axis (Study Hours): Ranges from 5 to 10 hours\nY-axis (Predicted GPA): Shows the expected GPA values\nThree lines: Representing the three stress levels (Moderate, Low, High)\n\nKey Observations:\n\nNearly Parallel Lines:\n\nAll three lines (Moderate, Low, and High stress) follow almost the same trajectory as study hours increase\nThey maintain roughly the same distance from each other throughout the range of study hours\nThis suggests that the effect of study hours on GPA is consistent across different stress levels\n\nPositive Slopes:\n\nAll lines show an upward trend, confirming that more study hours predict higher GPA regardless of stress level\nThe slopes appear very similar across all three stress levels\n\nMinimal Divergence:\n\nThe lines stay roughly equidistant from each other\nThere is no meaningful crossing or substantial convergence/divergence\nThe confidence intervals (shaded regions) overlap considerably\n\n\nInterpretation:\nNo Meaningful Interaction Effect\nThe parallel nature of these lines indicates that: - Study hours affect GPA in the same way regardless of whether a student experiences low, moderate, or high stress - A student who studies an additional hour can expect roughly the same GPA increase whether they are highly stressed or not stressed - Stress level does not modify the relationship between study hours and GPA\n\nGeneral Note on Interpreting Interaction Plots\nUnderstanding Line Patterns:\nParallel Lines = No Interaction When lines are parallel: - The effect of the predictor (e.g., study hours) is consistent across all levels of the moderator (e.g., stress level) - One variable does not change how the other variable affects the outcome - Example: If studying 1 more hour increases GPA by 0.2 points for low-stress students, it also increases GPA by approximately 0.2 points for high-stress students\nDiverging or Converging Lines = Interaction Present When lines diverge (spread apart) or converge (come together): - The effect of the predictor depends on the level of the moderator - The relationship between variables changes based on the context - Example: Studying might help low-stress students a lot (steep slope) but help high-stress students less (flatter slope)\nCrossing Lines = Strong Interaction When lines cross: - The direction of the effect can reverse depending on the moderator level - This is a particularly strong form of interaction - Example: At low study hours, stressed students might perform better, but at high study hours, non-stressed students might excel\nWhat to Look For: 1. Slope differences: Do the lines have different angles? 2. Spacing changes: Do the lines get closer or farther apart? 3. Crossings: Do any lines intersect? 4. Confidence intervals: Do the shaded regions overlap? (Wide overlap suggests uncertainty about whether lines truly differ)\nIn our case, the lines are nearly parallel with overlapping confidence intervals, strongly suggesting no interaction effect between study hours and stress level on GPA."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L6-github.html#model-diagnostics",
    "href": "Spring2026/LectureSlides/bigdata_L6-github.html#model-diagnostics",
    "title": "Statistics",
    "section": "",
    "text": "Linear regression assumes:\n\nLinearity: Relationship between X and Y is linear\nIndependence: Observations are independent\nHomoscedasticity: Constant variance of residuals\nNormality: Residuals are normally distributed\n\n\n\n\n\nBuilt-in diagnostic plots:\nInterpreting the Diagnostic Plots:\n\nResiduals vs Fitted (Top Left):\n\nShould show no clear pattern\nPoints should be randomly scattered around the horizontal line at 0\nA pattern suggests non-linearity or heteroscedasticity\n\nQ-Q Plot (Top Right):\n\nPoints should fall along the diagonal line\nDeviations suggest residuals are not normally distributed\nParticularly concerning if points curve away at the ends\n\nScale-Location (Bottom Left):\n\nShould show a horizontal line with randomly scattered points\nTests for homoscedasticity (constant variance)\nAn upward or downward trend suggests heteroscedasticity\n\nResiduals vs Leverage (Bottom Right):\n\nIdentifies influential observations\nPoints outside Cook‚Äôs distance lines (dashed lines) are influential\nThese points have a large impact on the regression coefficients\n\n\n# Four diagnostic plots\npar(mfrow = c(2, 2))\nplot(simple_model)\n\npar(mfrow = c(1, 1))\nInterpreting Our Diagnostic Plots:\nLooking at our model‚Äôs diagnostic plots:\n\nResiduals vs Fitted (Top Left):\n\nPoints are randomly scattered around the horizontal line at 0\nNo clear pattern or curvature is visible\nThe red line is approximately horizontal\nConclusion: Linearity assumption is satisfied ‚úì\n\nQ-Q Plot (Top Right):\n\nPoints fall closely along the diagonal reference line\nMinor deviations at the extreme ends (points 702, 118, 485)\nMost points align well with the theoretical quantiles\nConclusion: Normality assumption is largely satisfied ‚úì\n\nScale-Location (Bottom Left):\n\nPoints are evenly distributed across fitted values\nThe red line is relatively flat with slight variation\nNo obvious funnel or cone shape\nConclusion: Homoscedasticity assumption is satisfied ‚úì\n\nResiduals vs Leverage (Bottom Right):\n\nMost points cluster near low leverage values\nA few points (702, 480, 52) have higher leverage\nNo points fall outside Cook‚Äôs distance lines (not visible on plot)\nConclusion: No highly influential outliers detected ‚úì\n\n\nOverall Assessment:\nOur interaction model meets all key assumptions of linear regression: - The relationship is linear - Residuals show constant variance - Residuals are approximately normally distributed - No problematic influential points\nThe model is reliable and the statistical inferences (p-values, confidence intervals) are valid.\n\n\n\n\nThe performance package provides easy model diagnostics:\n# install.packages(\"performance\")\nlibrary(performance)\n\n# Check all assumptions\ncheck_model(simple_model)\n\nAlternatively, use the performance package for comprehensive diagnostics:\nlibrary(performance)\ncheck_model(interaction_model)\n\nInterpreting the Posterior Predictive Check:\nThis plot compares the distribution of observed data (green line) with simulated data from the model (blue lines):\n\nGreen line: The actual distribution of GPA values in our dataset\nBlue lines: Multiple simulated datasets generated from our fitted model\nWhat to look for: The blue lines should closely match the green line\n\nIn our case:\n\nThe model-predicted distributions (blue) closely overlap with the observed data (green)\nBoth show a bell-shaped, approximately normal distribution centered around 3.0\nThe peak, spread, and shape are well-matched\nNo systematic deviations or mismatches are visible\nConclusion: The model accurately captures the distribution of GPA values ‚úì\n\nThis provides additional evidence that our model is well-specified and that the assumptions are met. The model not only fits individual data points well (as shown in the diagnostic plots) but also reproduces the overall distribution of the outcome variable.\n\nIndividual checks:\n# Normality of residuals\ncheck_normality(simple_model)\n## OK: residuals appear as normally distributed (p = 0.403).\n# Homoscedasticity\ncheck_heteroscedasticity(simple_model)\n## OK: Error variance appears to be homoscedastic (p = 0.917).\n# Multicollinearity (for multiple regression)\ncheck_collinearity(multiple_model)\n## # Check for Multicollinearity\n## \n## Low Correlation\n## \n##                 Term  VIF  VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n##  Study_Hours_Per_Day 1.00 [1.00, Inf]     1.00      1.00     [0.00, 1.00]\n##              college 1.00 [1.00, Inf]     1.00      1.00     [0.00, 1.00]\nPerformance Package Output:\nThe check_model() function provides a comprehensive summary of model diagnostics:\nNormality of Residuals: - Test result: OK (p = 0.403) - p-value &gt; 0.05 indicates we fail to reject the null hypothesis of normality - Conclusion: Residuals are normally distributed ‚úì\nHomoscedasticity (Constant Variance): - Test result: OK (p = 0.917) - p-value &gt; 0.05 indicates we fail to reject the null hypothesis of homoscedasticity - Conclusion: Error variance is constant across fitted values ‚úì\nMulticollinearity Check:\nMulticollinearity occurs when predictor variables are highly correlated with each other, which can make coefficient estimates unstable.\nVIF (Variance Inflation Factor) Interpretation: - VIF = 1: No correlation with other predictors - VIF &lt; 5: Low multicollinearity (acceptable) - VIF 5-10: Moderate multicollinearity (concerning) - VIF &gt; 10: High multicollinearity (problematic)\nIn our model: - Study_Hours_Per_Day: VIF = 1.00 - college: VIF = 1.00 - Both predictors have VIF = 1.00, indicating no multicollinearity - Tolerance values = 1.00 (inverse of VIF) confirm this - Conclusion: Predictors are independent; coefficient estimates are stable ‚úì\nOverall Model Assessment:\nAll diagnostic checks passed: ‚úì Linearity assumption met\n‚úì Normality of residuals confirmed\n‚úì Homoscedasticity confirmed\n‚úì No multicollinearity detected\n‚úì No influential outliers\nThe model is statistically sound and reliable for inference.\n\nCompare models:\nAfter fitting multiple models, we can compare their performance to determine which best explains the data.\nModels to Compare:\n\nSimple Model: GPA ~ Study_Hours_Per_Day\nMultiple Model: GPA ~ Study_Hours_Per_Day + college\n\nUse the compare_performance() function from the performance package:\ncompare_performance(simple_model, multiple_model)\n## # Comparison of Model Performance Indices\n## \n## Name           | Model |  AIC (weights) | AICc (weights) |  BIC (weights)\n## -------------------------------------------------------------------------\n## simple_model   |    lm | -703.5 (0.835) | -703.5 (0.838) | -686.7 (&gt;.999)\n## multiple_model |    lm | -700.3 (0.165) | -700.2 (0.162) | -661.1 (&lt;.001)\n## \n## Name           |    R2 | R2 (adj.) |  RMSE | Sigma\n## --------------------------------------------------\n## simple_model   | 0.539 |     0.539 | 0.203 | 0.203\n## multiple_model | 0.541 |     0.539 | 0.202 | 0.203\nUnderstanding the Output:\nThe comparison table provides several metrics to evaluate model fit:\nInformation Criteria (Lower is Better):\n\nAIC (Akaike Information Criterion): Balances model fit and complexity\n\nSimple model: -703.5 (weight = 0.835)\nMultiple model: -700.3 (weight = 0.165)\nLower AIC indicates better fit; simple model is preferred\n\nAICc (Corrected AIC): Adjusted for small sample sizes\n\nSimple model: -703.5 (weight = 0.838)\nMultiple model: -700.2 (weight = 0.162)\nResults align with AIC\n\nBIC (Bayesian Information Criterion): More strongly penalizes complexity\n\nSimple model: -686.7 (weight &gt; .999)\nMultiple model: -661.1 (weight &lt; .001)\nStrongly favors the simpler model\n\n\nModel Fit Metrics:\n\nR¬≤ (R-squared):\n\nSimple model: 0.539\nMultiple model: 0.541\nNearly identical; adding college explains only 0.2% more variance\n\nAdjusted R¬≤: Accounts for number of predictors\n\nBoth models: 0.539\nNo improvement when adjusting for additional predictors\n\nRMSE (Root Mean Square Error): Average prediction error\n\nSimple model: 0.203\nMultiple model: 0.202\nEssentially identical prediction accuracy\n\nSigma: Residual standard error\n\nBoth models: 0.203\nNo difference in error variance\n\n\nInterpretation:\nThe simple model (GPA ~ Study_Hours_Per_Day) is preferred because: - Lower AIC and BIC indicate better balance of fit and parsimony - Adding college as a predictor provides negligible improvement in R¬≤ - Prediction accuracy (RMSE) is virtually unchanged - The principle of parsimony suggests choosing the simpler model when performance is equivalent\nConclusion: Study hours alone adequately predict GPA; college affiliation does not meaningfully improve the model."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L6-github.html#class-exercise",
    "href": "Spring2026/LectureSlides/bigdata_L6-github.html#class-exercise",
    "title": "Statistics",
    "section": "",
    "text": "Using the mtcars dataset:\ndata(mtcars)\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nT-test: Compare mpg between automatic (am=0) and manual (am=1) transmissions.\nANOVA: Compare mpg across different numbers of cylinders (cyl).\nSimple Regression: Predict mpg from wt (weight). Interpret the coefficients.\nMultiple Regression: Add hp (horsepower) to your model. Does it improve fit?\nCoefficient Plot: Create a coefficient plot for your multiple regression.\nDiagnostics: Check the assumptions of your final model.\nReport: Write a brief summary of your findings as you would for an academic paper.\n\n### Your workspace"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L6-github.html#lecture-6-cheat-sheet",
    "href": "Spring2026/LectureSlides/bigdata_L6-github.html#lecture-6-cheat-sheet",
    "title": "Statistics",
    "section": "",
    "text": "Function/Concept\nDescription\nCode Example\n\n\n\n\nt.test()\nCompare means of two groups\nt.test(y ~ group, data = df)\n\n\naov()\nOne-way ANOVA\naov(y ~ group, data = df)\n\n\nTukeyHSD()\nPost-hoc test after ANOVA\nTukeyHSD(aov_result)\n\n\nlm()\nLinear regression\nlm(y ~ x1 + x2, data = df)\n\n\nglm()\nGeneralized linear model\nglm(y ~ x, family = poisson)\n\n\nsummary()\nModel summary\nsummary(model)\n\n\ncoef()\nExtract coefficients\ncoef(model)\n\n\npredict()\nGenerate predictions\npredict(model, newdata)\n\n\nbroom::tidy()\nTidy model output\ntidy(model, conf.int = TRUE)\n\n\nbroom::glance()\nModel fit statistics\nglance(model)\n\n\ngeom_smooth(method=\"lm\")\nAdd regression line\ngeom_smooth(method = \"lm\")\n\n\nggeffects::ggpredict()\nMarginal effects\nggpredict(model, terms = \"x\")\n\n\nInteraction term\nEffect depends on another variable\nlm(y ~ x1 * x2)\n\n\nplot(model)\nDiagnostic plots\nplot(lm_model)\n\n\nperformance::check_model()\nCheck assumptions\ncheck_model(model)\n\n\ncompare_performance()\nCompare models\ncompare_performance(m1, m2)\n\n\nR¬≤\nVariance explained\nsummary(model)$r.squared\n\n\np-value\nStatistical significance\n&lt; 0.05 = significant\n\n\nConfidence interval\nRange of plausible values\n95% CI excludes 0 = significant"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L9-github.html",
    "href": "Spring2026/LectureSlides/bigdata_L9-github.html",
    "title": "Text as Data: Topic Modeling",
    "section": "",
    "text": "Dr.¬†Ayse D. Lokmanoglu Lecture 9, (B) March 25, (A) March 30"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L9-github.html#table-of-contents",
    "href": "Spring2026/LectureSlides/bigdata_L9-github.html#table-of-contents",
    "title": "Text as Data: Topic Modeling",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n\n\nSection\nTopic\n\n\n\n\n1\nIntro to Topic Modeling\n\n\n1.1\nWhy Use Topic Modeling?\n\n\n1.2\nUnderstanding LDA\n\n\n2\nPreprocessing Text Data\n\n\n2.1\nLoad the Data\n\n\n2.2\nInitial Data Exploration\n\n\n2.3\nText Preprocessing\n\n\n2.3.1\nCreate Index, Backup Text, and Language Filtering\n\n\n2.3.2\nCreate Custom Stopwords\n\n\n2.4\nTokenization\n\n\n2.5\nTerm Frequency Filtering with TF-IDF\n\n\n2.6\nVisualize Top Words\n\n\n2.7\nCreate Document-Term Matrix\n\n\n3\nChoosing the Number of Topics (K)\n\n\n3.1\nWhat is Topic Coherence?\n\n\n3.2\nTypes of Coherence Metrics\n\n\n3.3\nWhat Makes a ‚ÄúGood‚Äù Topic?\n\n\n3.4\nComputing Topic Coherence for Different K Values\n\n\n3.5\nVisualizing Topic Coherence\n\n\n3.5.1\nCoherence Across K Values\n\n\n3.5.2\nCoherence Decline from Optimal K\n\n\n3.6\nInterpreting the Results for Optimal K\n\n\n3.7\nChoosing Optimal K\n\n\n4\nLDA Model\n\n\n4.1\nTrain the LDA Model with the Optimal Topics\n\n\n4.2\nSave Your Work\n\n\n4.3\nLook Inside the LDA Model\n\n\n5\nThe Beta & Gamma Matrices\n\n\n5.1\nBeta (Œ≤): Topic-Word Probabilities\n\n\n5.2\nTop Words per Topic - The Beta (Œ≤) Matrix using Tidy\n\n\n5.3\nTop 10 Words per Topic\n\n\n5.4\nVisualize Top Words\n\n\n5.5\nComparing Words Between Topics\n\n\n5.6\nVisualize Word Comparisons\n\n\n5.7\nTop Documents per Topic - The Gamma (Œ≥) Matrix using Tidy\n\n\n5.8\nVisualizing Document-Topic Distributions\n\n\n5.9\nFinding Representative Documents for Each Topic\n\n\n5.10\nExamining Representative Document Content\n\n\n5.11\nSummary of Representative Documents\n\n\n5.12\nDistribution of Dominant Topics\n\n\n5.13\nTopics Over Time\n\n\n\n\nALWAYS Let‚Äôs load our libraries\nlibrary(tidyverse)    # Data manipulation and visualization\nlibrary(tidytext)     # Text mining using tidy data principles\nlibrary(ggplot2)      # Data visualization (part of tidyverse)\nlibrary(stopwords)    # Stopword lists in multiple languages\nlibrary(dplyr)        # Data manipulation (part of tidyverse)\nlibrary(quanteda)     # Quantitative text analysis\nlibrary(topicmodels)  # Topic modeling (LDA, CTM)\nlibrary(topicdoc)     # Topic coherence metrics"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L9-github.html#intro-to-topic-modeling",
    "href": "Spring2026/LectureSlides/bigdata_L9-github.html#intro-to-topic-modeling",
    "title": "Text as Data: Topic Modeling",
    "section": "1. Intro to Topic Modeling",
    "text": "1. Intro to Topic Modeling\nTopic modeling is an unsupervised machine learning technique that identifies latent themes in a collection of text documents. The most widely used approach is Latent Dirichlet Allocation (LDA), which assumes each document is a mixture of topics, and each topic is a mixture of words.\n\n1.1 Why Use Topic Modeling?\n\nHelps uncover hidden structure in large text corpora\nOrganizes vast amounts of text into interpretable themes\nUseful for content analysis, social media monitoring, and recommendation systems\nAllows exploration of what people are discussing without pre-defined categories\n\n\nimage from: https://www.tidytextmining.com/topicmodeling\n\n\n1.2 Understanding LDA\nLatent Dirichlet Allocation (LDA) is one of the most widely used algorithms for topic modeling. Without delving into the complex mathematics behind it, we can conceptualize LDA through two key principles:\n\nEvery document is a mixture of topics.\n\nEach document contains words from multiple topics in different proportions.\nFor instance, in a model with two topics, we could say:\n\n‚ÄúDocument 1 is 80% Topic A and 20% Topic B.‚Äù\n‚ÄúDocument 2 is 40% Topic A and 60% Topic B.‚Äù\n\n\nEvery topic is a mixture of words.\n\nEach topic consists of words that commonly appear together.\nFor example, in a two-topic model focused on K-pop:\n\nThe ‚Äúidol performance‚Äù topic might include words like dance, stage, music, and concert.\nThe ‚Äúfandom culture‚Äù topic might contain words like fans, streaming, voting, and billboard.\nSome words, like debut, may appear in both topics but with different frequencies.\n\n\n\nLDA uses a probabilistic approach to simultaneously determine:\n\nThe composition of topics within each document\nThe key words that define each topic\n\nThis method enables us to uncover hidden themes in large text datasets without prior labeling."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L9-github.html#preprocessing-text-data",
    "href": "Spring2026/LectureSlides/bigdata_L9-github.html#preprocessing-text-data",
    "title": "Text as Data: Topic Modeling",
    "section": "2. Preprocessing Text Data",
    "text": "2. Preprocessing Text Data\n\n2.1 Load the Data\nWe‚Äôll use Reddit comments about K-pop Demon Hunters. This dataset contains community discussions, reactions, and conversations about the show.\n# Load the data\ndata &lt;- read_csv(\"kpop_comments_forclass.csv\")\n\n# Check structure\nstr(data)\n## spc_tbl_ [23,270 √ó 18] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ text            : chr [1:23270] \"kind of a tangent but kind of a telling tidbit from japan...\\n\\n\\nuniversal studio japan does these \\\"hybe x us\"| __truncated__ \"Thank you for submitting to r/kpop! Unfortunately, your post has been removed for the following reason(s):\\n\\n*\"| __truncated__ \"Thank you for submitting to r/kpop! Unfortunately, your post has been removed for the following reason(s):\\n\\n*\"| __truncated__ \"Put character skin out, I wanted to see NingNing tapped out Winter\" ...\n##  $ id              : chr [1:23270] \"n0o7gct\" \"n0o7ocv\" \"n0o94v4\" \"n0o984c\" ...\n##  $ author          : chr [1:23270] \"xxqbsxx\" \"kpop-ModTeam\" \"kpop-ModTeam\" \"Sunasoo\" ...\n##  $ link_id         : chr [1:23270] \"t3_1kf7rre\" \"t3_1loneox\" \"t3_1lonl0c\" \"t3_1lom8cm\" ...\n##  $ parent_id       : chr [1:23270] \"t3_1kf7rre\" \"t3_1loneox\" \"t3_1lonl0c\" \"t3_1lom8cm\" ...\n##  $ created_utc     : num [1:23270] 1.75e+09 1.75e+09 1.75e+09 1.75e+09 1.75e+09 ...\n##  $ score           : num [1:23270] 71 1 1 9 1 10 5 1 2 30 ...\n##  $ downs           : num [1:23270] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ likes           : logi [1:23270] NA NA NA NA NA NA ...\n##  $ controversiality: num [1:23270] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ gilded          : num [1:23270] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ subreddit       : chr [1:23270] \"kpop\" \"kpop\" \"kpop\" \"kpop\" ...\n##  $ permalink       : chr [1:23270] \"/r/kpop/comments/1kf7rre/megathread_22_hybe_ador_mhj_we_reach_one_year_of/n0o7gct/\" \"/r/kpop/comments/1loneox/bighit_musics_new_korean_boy_band_spotted_filming/n0o7ocv/\" \"/r/kpop/comments/1lonl0c/guys_i_need_help_finding_this_song/n0o94v4/\" \"/r/kpop/comments/1lom8cm/aespa_street_fighter_6_x_aespa_special_collab/n0o984c/\" ...\n##  $ datetime        : POSIXct[1:23270], format: \"2025-07-01 00:02:08\" \"2025-07-01 00:03:25\" ...\n##  $ date            : Date[1:23270], format: \"2025-07-01\" \"2025-07-01\" ...\n##  $ year            : num [1:23270] 2025 2025 2025 2025 2025 ...\n##  $ month           : num [1:23270] 7 7 7 7 7 7 7 7 7 7 ...\n##  $ text_length     : num [1:23270] 818 561 519 66 469 ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   text = col_character(),\n##   ..   id = col_character(),\n##   ..   author = col_character(),\n##   ..   link_id = col_character(),\n##   ..   parent_id = col_character(),\n##   ..   created_utc = col_double(),\n##   ..   score = col_double(),\n##   ..   downs = col_double(),\n##   ..   likes = col_logical(),\n##   ..   controversiality = col_double(),\n##   ..   gilded = col_double(),\n##   ..   subreddit = col_character(),\n##   ..   permalink = col_character(),\n##   ..   datetime = col_datetime(format = \"\"),\n##   ..   date = col_date(format = \"\"),\n##   ..   year = col_double(),\n##   ..   month = col_double(),\n##   ..   text_length = col_double()\n##   .. )\n##  - attr(*, \"problems\")=&lt;externalptr&gt;\nhead(data)\n## # A tibble: 6 √ó 18\n##   text              id    author link_id parent_id created_utc score downs likes\n##   &lt;chr&gt;             &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;\n## 1 \"kind of a tange‚Ä¶ n0o7‚Ä¶ xxqbs‚Ä¶ t3_1kf‚Ä¶ t3_1kf7r‚Ä¶  1751328128    71     0 NA   \n## 2 \"Thank you for s‚Ä¶ n0o7‚Ä¶ kpop-‚Ä¶ t3_1lo‚Ä¶ t3_1lone‚Ä¶  1751328205     1     0 NA   \n## 3 \"Thank you for s‚Ä¶ n0o9‚Ä¶ kpop-‚Ä¶ t3_1lo‚Ä¶ t3_1lonl‚Ä¶  1751328709     1     0 NA   \n## 4 \"Put character s‚Ä¶ n0o9‚Ä¶ Sunas‚Ä¶ t3_1lo‚Ä¶ t3_1lom8‚Ä¶  1751328740     9     0 NA   \n## 5 \"Thank you for s‚Ä¶ n0o9‚Ä¶ kpop-‚Ä¶ t3_1lo‚Ä¶ t3_1lonk‚Ä¶  1751328826     1     0 NA   \n## 6 \"Where‚Äôs the Aku‚Ä¶ n0oa‚Ä¶ Optio‚Ä¶ t3_1lo‚Ä¶ t3_1lom8‚Ä¶  1751329139    10     0 NA   \n## # ‚Ñπ 9 more variables: controversiality &lt;dbl&gt;, gilded &lt;dbl&gt;, subreddit &lt;chr&gt;,\n## #   permalink &lt;chr&gt;, datetime &lt;dttm&gt;, date &lt;date&gt;, year &lt;dbl&gt;, month &lt;dbl&gt;,\n## #   text_length &lt;dbl&gt;\n\n\n2.2 Initial Data Exploration\nLet‚Äôs understand what we‚Äôre working with:\n# How many comments?\nnrow(data)\n## [1] 23270\n# Date range\nrange(data$date)\n## [1] \"2025-07-01\" \"2025-07-31\"\n# Score distribution\nsummary(data$score)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##  -85.00    2.00    7.00   19.98   21.00 1779.00\n# Text length distribution\nsummary(data$text_length)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    51.0    89.0   152.0   274.3   308.0  9673.0\n# Look at a few comments\nhead(data$text, 5)\n## [1] \"kind of a tangent but kind of a telling tidbit from japan...\\n\\n\\nuniversal studio japan does these \\\"hybe x usj summer dance night\\\" thing in july and aug where they play hybe artists music for ppl to dance to¬†\\n\\n\\nits an official event and i think &team actually came to perform live last year\\n\\n\\nyou can search on socials for the setlist, but this year theyve completely cut nj from the lineup (eta and supershy were on there last year)\\n\\n\\ntokkis may use this to cry mistreatment and erasure, but how could they promote a group that refuses to acknowledge their companyü§∑\\n\\n\\nand tbh the setlist with illit and tws added this year does not look lacking at all, such is the fast paced world of kpop¬†\\n\\n\\ni hope the members and their fans realize that the world wont be waiting for them but just move on and find the next big thing\"\n## [2] \"Thank you for submitting to r/kpop! Unfortunately, your post has been removed for the following reason(s):\\n\\n* Submissions that are not substantially newsworthy should be posted to the [artist's subreddit](https://www.reddit.com/r/kpop/wiki/relatedsubs#wiki_group.2Fartist_subreddits.3A). Please check each subreddit's rules before posting.\\n\\nIf you have any questions regarding the ruleset of r/kpop, please refer to the [Rules](https://www.reddit.com/r/kpop/wiki/rules) or [message the moderators](https://www.reddit.com/message/compose/?to=/r/kpop). Thank you!\"                                                                                                                                                                                                                                                                                  \n## [3] \"Thank you for submitting to r/kpop! Unfortunately, your post has been removed for the following reason(s):\\n\\n* Please go to r/kpophelp and use the **Monthly 'Who's this?' & Merch Authentication Post** to get help for your question. You should be able to find it pinned to the top of that subreddit.\\n\\nIf you have any questions regarding the ruleset of r/kpop, please refer to the [Rules](https://www.reddit.com/r/kpop/wiki/rules) or [message the moderators](https://www.reddit.com/message/compose/?to=/r/kpop). Thank you!\"                                                                                                                                                                                                                                                                                                                            \n## [4] \"Put character skin out, I wanted to see NingNing tapped out Winter\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n## [5] \"Thank you for submitting to r/kpop! Unfortunately, your post has been removed for the following reason(s):\\n\\n* Your submission is off-topic for this subreddit.  All submissions must be directly relevant to Korean music, artists, companies, or fans.\\n\\nIf you have any questions regarding the ruleset of r/kpop, please refer to the [Rules](https://www.reddit.com/r/kpop/wiki/rules) or [message the moderators](https://www.reddit.com/message/compose/?to=/r/kpop). Thank you!\"\n\n\n\n2.3 Text Preprocessing\nBefore we can run topic models, we need to clean and prepare our text data.\n\n2.3.1 Create Index, Backup Text, and Language Filtering\nImportant: We‚Äôll create new objects at each step so we don‚Äôt lose our original data.\nWhy these steps matter:\n\nLanguage filtering: Reddit has international users, so we remove non-English comments\nDuplicate removal: Same comment posted multiple times (e.g., copypasta, bots)\nCreating new objects: We keep data, data2, data3 so we can backtrack if needed\n\n# Create document index\ndata$comment_index &lt;- seq_len(nrow(data))\n\n# Backup original text\ndata$textBU &lt;- data$text\n\n\ndata$CLD2&lt;-cld2::detect_language(data$text) ## a package by Google\ntable(data$CLD2)\n## \n##    en    es    fr    id    it    ko    ms    pt    sk    tl    vi \n## 23133     9     3     2     3    25     1     2     1     1     1\ndata2 &lt;- data |&gt;  filter(CLD2==\"en\")\ndata3 &lt;- data |&gt; \n  distinct(text, .keep_all = TRUE)\n\n\n2.3.2 Create Custom Stopwords\nWe‚Äôll remove common words that don‚Äôt add meaning to our topics. For K-pop discussions, we might want to remove very generic terms.\n# Create custom stopword list\nmystopwords &lt;- c(\n  stopwords(\"en\"),\n  stopwords::stopwords(source = \"smart\"),\n  # Add domain-specific stopwords\n  \"https\", \"http\", \"t.co\", \"amp\",  # URLs and HTML\n  \"reddit\", \"comment\", \"post\", \"thread\",  # Reddit-specific\n  \"edit\", \"edited\", \"update\",  # Reddit conventions\n  \"kpop\"  # Remove your own search words as they will be in every document\n)\n\nmystopwords &lt;- unique(mystopwords)\nmystopwords &lt;- tolower(mystopwords)\n\n# Check how many stopwords we have\nlength(mystopwords)\n## [1] 592\n\n\n\n\n2.4 Tokenization\nNow we‚Äôll break our text into individual words (tokens) and remove stopwords.\n# Tokenize the text\ntidy_data &lt;- data3 |&gt; \n  unnest_tokens(word, text) |&gt;  # Break into words\n  anti_join(data.frame(word = mystopwords)) |&gt;  # Remove stopwords\n  mutate(nchar = nchar(word)) |&gt;  # Count characters per word\n  filter(nchar &gt; 2) |&gt;  # Keep words with more than 2 characters\n  filter(!grepl(\"[0-9]{1}\", word)) |&gt;  # Remove words with numbers\n  filter(!grepl(\"\\\\W\", word))  # Remove words with special characters\n\n# Check results\nhead(tidy_data)\n## # A tibble: 6 √ó 22\n##   id     author link_id parent_id created_utc score downs likes controversiality\n##   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;            &lt;dbl&gt;\n## 1 n0o7g‚Ä¶ xxqbs‚Ä¶ t3_1kf‚Ä¶ t3_1kf7r‚Ä¶  1751328128    71     0 NA                   0\n## 2 n0o7g‚Ä¶ xxqbs‚Ä¶ t3_1kf‚Ä¶ t3_1kf7r‚Ä¶  1751328128    71     0 NA                   0\n## 3 n0o7g‚Ä¶ xxqbs‚Ä¶ t3_1kf‚Ä¶ t3_1kf7r‚Ä¶  1751328128    71     0 NA                   0\n## 4 n0o7g‚Ä¶ xxqbs‚Ä¶ t3_1kf‚Ä¶ t3_1kf7r‚Ä¶  1751328128    71     0 NA                   0\n## 5 n0o7g‚Ä¶ xxqbs‚Ä¶ t3_1kf‚Ä¶ t3_1kf7r‚Ä¶  1751328128    71     0 NA                   0\n## 6 n0o7g‚Ä¶ xxqbs‚Ä¶ t3_1kf‚Ä¶ t3_1kf7r‚Ä¶  1751328128    71     0 NA                   0\n## # ‚Ñπ 13 more variables: gilded &lt;dbl&gt;, subreddit &lt;chr&gt;, permalink &lt;chr&gt;,\n## #   datetime &lt;dttm&gt;, date &lt;date&gt;, year &lt;dbl&gt;, month &lt;dbl&gt;, text_length &lt;dbl&gt;,\n## #   comment_index &lt;int&gt;, textBU &lt;chr&gt;, CLD2 &lt;chr&gt;, word &lt;chr&gt;, nchar &lt;int&gt;\nnrow(tidy_data)  # How many tokens do we have?\n## [1] 385257\n\n\n\n2.5 Term Frequency Filtering with TF-IDF\nNot all words are equally useful for topic modeling. We want to remove:\n\nVery common words that appear in almost every document (too general)\nVery rare words that appear in only 1-2 documents (too specific)\n\nWe use TF-IDF principles to filter our vocabulary:\n# Set thresholds\nmaxndoc &lt;- 0.75   # Remove words in more than 75% of documents\nminndoc &lt;- 0.001 # Remove words in less than 0.1% of documents\n\n# Calculate document frequency for each word\ntemplength &lt;- length(unique(tidy_data$comment_index))\n\ngood_common_words &lt;- tidy_data |&gt; \n  count(comment_index, word, sort = TRUE) |&gt; \n  group_by(word) |&gt; \n  summarize(doc_freq = n() / templength) |&gt; \n  filter(doc_freq &lt; maxndoc) |&gt;   # Not too common\n  filter(doc_freq &gt; minndoc)      # Not too rare\n\n# How many words passed our filter?\nnrow(good_common_words)\n## [1] 2683\n# Clean our tidy data to only include these words\ntidy_data_pruned &lt;- tidy_data |&gt; \n  inner_join(good_common_words)\n\n# NOTE: This is where you might lose some documents that had no remaining words\n\n\n\n2.6 Visualize Top Words\nBefore modeling, let‚Äôs see what words are most common in our corpus:\n# Plot top 50 words\ntidy_data_pruned |&gt; \n  group_by(word) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(desc(n)) |&gt; \n  mutate(word = reorder(word, n)) |&gt; \n  top_n(50) |&gt;     \n  ggplot(aes(n, word)) +\n  geom_col(fill = \"steelblue\") +\n  labs(\n    title = \"Top 50 Words in K-pop Demon Hunters Comments\",\n    y = NULL,\n    x = \"Frequency\"\n  ) +\n  theme_minimal()\n\nQuestions to ask: - Do these words make sense for K-pop discussions?\n\nAre there any words we should add to our stopword list?\nDo we see evidence of different themes already?\n\n\n\n\n2.7 Create Document-Term Matrix\nFor topic modeling, we need to convert our tidy data into a Document-Term Matrix (DTM):\n\nRows = documents (comments)\nColumns = words (terms)\nValues = word counts\n\n# Create DTM using tidytext\ntidy_dfm &lt;- tidy_data_pruned |&gt; \n  count(comment_index, word) |&gt; \n  cast_dtm(comment_index, word, n)\n\n# Check dimensions\ntidy_dfm\n## &lt;&lt;DocumentTermMatrix (documents: 22316, terms: 2683)&gt;&gt;\n## Non-/sparse entries: 259840/59613988\n## Sparsity           : 100%\n## Maximal term length: 15\n## Weighting          : term frequency (tf)\ndim(tidy_dfm)\n## [1] 22316  2683\nUnderstanding the Document-Term Matrix Output\nWhen you print the DTM, you‚Äôll see output like this:\n&lt;&lt;DocumentTermMatrix (documents: 22316, terms: 2683)&gt;&gt;\nNon-/sparse entries: 259840/59613988\nSparsity           : 100%\nMaximal term length: 15\nWeighting          : term frequency (tf)\n[1] 22316  2683\nLet‚Äôs break down what each line means:\nLine 1: Dimensions - documents: 22,316 = We have 22,316 comments in our dataset\n\nterms: 2,683 = We have 2,683 unique words in our vocabulary\nThis creates a matrix with 22,216 rows √ó 2,683 columns = 59,873,828 total cells\n\nLine 2: Entries\n\nNon-sparse entries: 259,840 = Only 259,840 cells contain actual word counts (non-zero values)\nSparse entries: 59,613,988 = Most cells are empty (zeros) because most words don‚Äôt appear in most documents\n\nLine 3: Sparsity\n\nSparsity:100% = Essentially 100% of the matrix is empty (zeros)\nThis is normal! Most comments only use a small fraction of the total vocabulary\nHigh sparsity is expected in text data\n\nLine 4: Term Length\n\nMaximal term length: 15 = The longest word in our vocabulary has 15 characters\n\nLine 5: Weighting\n\nterm frequency (tf) = Cells contain simple counts of how many times each word appears in each document\nAlternative weightings include TF-IDF (which we used for filtering)\n\nWhy does this matter?\nText data is naturally sparse because:\n# Calculate actual sparsity statistics from our DTM\ndtm_matrix &lt;- as.matrix(tidy_dfm)\nwords_per_doc &lt;- rowSums(dtm_matrix &gt; 0)\n\n# Summary statistics\nmean(words_per_doc)      # Average: 11.6 unique words per comment\n## [1] 11.64366\nmedian(words_per_doc)    # Median: 7 unique words per comment\n## [1] 7\nquantile(words_per_doc, c(0.25, 0.75))  # 25th percentile: 4, 75th percentile: 13\n## 25% 75% \n##   4  13\n# Vocabulary usage\nmean(words_per_doc) / 2683 * 100   # Average: 0.43% of vocabulary\n## [1] 0.4339793\nmedian(words_per_doc) / 2683 * 100 # Median: 0.26% of vocabulary\n## [1] 0.260902\nrm(dtm_matrix)\nOur data shows:\n\nAverage comment uses only 11.6 unique words (median: 7 words) - Vocabulary has 2683 words total\nEach comment uses only 0.43% of the available vocabulary (median: 0.26%)\nRange: Comments use between 1 to 294 unique words\nMost comments (50%) use between 4-13 unique words (interquartile range)\n\nThis extreme sparsity is why topic modeling is powerful - it finds patterns in this sparse, high-dimensional data by identifying which words tend to co-occur across documents, revealing hidden thematic structure!\nVisualization:\n# Visualize distribution of words per comment\ndata.frame(words = words_per_doc) |&gt; \n  ggplot(aes(x = words)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(xintercept = mean(words_per_doc), \n             color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_vline(xintercept = median(words_per_doc), \n             color = \"darkgreen\", linetype = \"dashed\", size = 1) +\n  annotate(\"text\", x = mean(words_per_doc) + 15, y = 2000, \n           label = paste(\"Mean:\", round(mean(words_per_doc), 1)), \n           color = \"red\") +\n  annotate(\"text\", x = median(words_per_doc) + 15, y = 1800, \n           label = paste(\"Median:\", median(words_per_doc)), \n           color = \"darkgreen\") +\n  labs(\n    title = \"Distribution of Unique Words per Comment\",\n    subtitle = \"Most comments use very few unique words after preprocessing\",\n    x = \"Number of Unique Words\",\n    y = \"Number of Comments\"\n  ) +\n  theme_minimal()\n\n\nImportant: Notice if we lost any documents during preprocessing. This happens when comments had no words left after stopword removal and filtering.\n# How many documents did we start with?\nnrow(data3)\n## [1] 22408\n# How many documents in our DTM?\nnrow(tidy_dfm)\n## [1] 22316\n# How many did we lose?\nnrow(data3) - nrow(tidy_dfm)\n## [1] 92\nSummary of data filtering:\n\n\n\n\n\n\n\n\n\nStep\n# Comments\nComments Lost\nReason\n\n\n\n\nOriginal data\n23,270\n-\nRaw July 2025 r/kpop comments\n\n\nLanguage filter\n23,133\n137\nRemoved non-English comments\n\n\nDuplicate removal\n22,408\n725\nRemoved duplicate/repeated comments\n\n\nPreprocessing filter\n22,316\n92\nRemoved comments with no words after stopword removal and TF-IDF filtering\n\n\nFinal dataset\n22,316\n954 total\nReady for topic modeling\n\n\n\nWe lost 954 comments total (4.1% of original data) - this is a very reasonable amount of data loss during preprocessing!\nThe breakdown:\n\n137 non-English comments (0.6%) - mostly Korean, Spanish, French\n725 duplicates (3.1%) - copypasta, bot comments, repeated phrases\n92 empty after preprocessing (0.4%) - comments with only stopwords/URLs"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L9-github.html#choosing-the-number-of-topics-k",
    "href": "Spring2026/LectureSlides/bigdata_L9-github.html#choosing-the-number-of-topics-k",
    "title": "Text as Data: Topic Modeling",
    "section": "3. Choosing the Number of Topics (K)",
    "text": "3. Choosing the Number of Topics (K)\nOne of the most important decisions in topic modeling is choosing how many topics (K) to extract. There‚Äôs no perfect answer, but we can use topic coherence metrics to guide our decision.\n\n3.1 What is Topic Coherence?\nTopic coherence measures how interpretable and meaningful a topic is by calculating how often the top words in a topic appear together in documents.\nHigh coherence = words in a topic frequently co-occur (topic makes sense) Low coherence = words in a topic rarely appear together (topic is random/unclear)\nThink of it like this:\n\nA topic with words {dance, performance, stage, choreography} has HIGH coherence (these words naturally go together)\nA topic with words {dance, breakfast, politics, ocean} has LOW coherence (these words don‚Äôt relate)\n\n\n\n3.2 Types of Coherence Metrics\nThere are several ways to measure topic coherence:\n\n\n\n\n\n\n\n\nMetric\nDescription\nWhat It Measures\n\n\n\n\nC_V\nBased on sliding window word co-occurrence\nMost commonly used; balances accuracy and interpretability\n\n\nC_UMass\nBased on document co-occurrence\nHow often topic words appear in same documents\n\n\nC_UCI\nBased on pointwise mutual information\nStatistical association between words\n\n\nC_NPMI\nNormalized pointwise mutual information\nNormalized version of UCI\n\n\n\nFor this class, we‚Äôll focus on C_V (most popular) and C_UMass (fastest to compute).\n\n\n3.3 What Makes a ‚ÄúGood‚Äù Topic?\nA good topic should have:\n\nHigh coherence - Words make sense together\nHigh exclusivity - Words are specific to this topic (not shared across all topics)\nInterpretability - Humans can understand what the topic is about\nCoverage - Topics cover the main themes in your corpus\n\nTrade-off Alert: Sometimes increasing K gives you more specific topics (higher exclusivity) but lower coherence. We need to find the sweet spot!\n\n\n\n3.4 Computing Topic Coherence for Different K Values\nNow we‚Äôll fit LDA models with different numbers of topics (K = 5, 10, 15, 20, 25) and calculate their coherence scores.\nNote: This step can take 5-10 minutes to run (even longer in larger data), so be patient!\n# We'll test these K values\nk_values &lt;- c(5, 10, 15, 20, 25, 50)\n\n# Create empty lists to store results\nlda_models &lt;- list()\ncoherence_scores &lt;- data.frame(\n  k = integer(),\n  coherence_cv = numeric(),\n  coherence_umass = numeric()\n)\n\n# Fit models for each K\nfor (k in k_values) {\n  cat(\"Fitting LDA with K =\", k, \"...\\n\")\n  \n  # Fit LDA model\n  lda_model &lt;- LDA(\n    tidy_dfm, \n    k = k, \n    method = \"Gibbs\",\n    control = list(\n      verbose = 500, \n      seed = 9898,\n      iter = 1000,\n      burnin = 100\n    )\n  )\n  \n  # Store model\n  lda_models[[as.character(k)]] &lt;- lda_model\n  \n  # Calculate coherence using topicdoc\n  # The function is topic_coherence() with different syntax\n  coherence_result &lt;- topic_coherence(lda_model, tidy_dfm)\n  \n  # Store results\n  coherence_scores &lt;- rbind(\n    coherence_scores,\n    data.frame(\n      k = k,\n      mean_coherence = mean(coherence_result)\n    )\n  )\n  \n  cat(\"K =\", k, \"Mean Coherence =\", mean(coherence_result), \"\\n\\n\")\n}\n## Fitting LDA with K = 5 ...\n## K = 5; V = 2683; M = 22316\n## Sampling 1100 iterations!\n## Iteration 500 ...\n## Iteration 1000 ...\n## Gibbs sampling completed!\n## K = 5 Mean Coherence = -160.4584 \n## \n## Fitting LDA with K = 10 ...\n## K = 10; V = 2683; M = 22316\n## Sampling 1100 iterations!\n## Iteration 500 ...\n## Iteration 1000 ...\n## Gibbs sampling completed!\n## K = 10 Mean Coherence = -178.2801 \n## \n## Fitting LDA with K = 15 ...\n## K = 15; V = 2683; M = 22316\n## Sampling 1100 iterations!\n## Iteration 500 ...\n## Iteration 1000 ...\n## Gibbs sampling completed!\n## K = 15 Mean Coherence = -188.5915 \n## \n## Fitting LDA with K = 20 ...\n## K = 20; V = 2683; M = 22316\n## Sampling 1100 iterations!\n## Iteration 500 ...\n## Iteration 1000 ...\n## Gibbs sampling completed!\n## K = 20 Mean Coherence = -188.3172 \n## \n## Fitting LDA with K = 25 ...\n## K = 25; V = 2683; M = 22316\n## Sampling 1100 iterations!\n## Iteration 500 ...\n## Iteration 1000 ...\n## Gibbs sampling completed!\n## K = 25 Mean Coherence = -192.205 \n## \n## Fitting LDA with K = 50 ...\n## K = 50; V = 2683; M = 22316\n## Sampling 1100 iterations!\n## Iteration 500 ...\n## Iteration 1000 ...\n## Gibbs sampling completed!\n## K = 50 Mean Coherence = -201.1114\n# View results\nprint(coherence_scores)\n##    k mean_coherence\n## 1  5      -160.4584\n## 2 10      -178.2801\n## 3 15      -188.5915\n## 4 20      -188.3172\n## 5 25      -192.2050\n## 6 50      -201.1114\n\n\n\n3.5 Visualizing Topic Coherence\nLet‚Äôs visualize how coherence changes with the number of topics:\n\n3.5.1 Coherence Across K Values\n# Line plot showing coherence scores\nggplot(coherence_scores, aes(x = k, y = mean_coherence)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  geom_point(color = \"steelblue\", size = 4) +\n  geom_point(data = coherence_scores[1,], \n             color = \"darkgreen\", size = 5, shape = 17) +  # Highlight K=5\n  labs(\n    title = \"Topic Coherence by Number of Topics (K)\",\n    subtitle = \"Higher (less negative) is better ‚Ä¢ Green triangle = optimal K\",\n    x = \"Number of Topics (K)\",\n    y = \"Mean Coherence Score (UMass)\"\n  ) +\n  scale_x_continuous(breaks = k_values) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    axis.title = element_text(size = 12),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n3.5.2 Coherence Decline from Optimal K\n# Calculate percentage decline from optimal\ncoherence_scores &lt;- coherence_scores |&gt; \n  mutate(\n    coherence_diff = mean_coherence - max(mean_coherence),\n    percent_decline = abs((mean_coherence - max(mean_coherence)) / max(mean_coherence) * 100)\n  )\n\n# Plot percentage decline\nggplot(coherence_scores, aes(x = k, y = percent_decline)) +\n  geom_line(color = \"darkred\", size = 1.2) +\n  geom_point(color = \"darkred\", size = 4) +\n  geom_area(fill = \"darkred\", alpha = 0.2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"darkgreen\", size = 1) +\n  labs(\n    title = \"Coherence Decline from Optimal K\",\n    subtitle = \"Shows how much coherence degrades as K increases from optimal (K=5)\",\n    x = \"Number of Topics (K)\",\n    y = \"Percent Decline in Coherence (%)\"\n  ) +\n  scale_x_continuous(breaks = k_values) +\n  scale_y_continuous(labels = function(x) paste0(x, \"%\")) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    axis.title = element_text(size = 12)\n  )\n\n# Print the summary table\ncoherence_scores |&gt; \n  select(k, mean_coherence, percent_decline) |&gt; \n  mutate(\n    mean_coherence = round(mean_coherence, 2),\n    percent_decline = round(percent_decline, 1)\n  )\n##    k mean_coherence percent_decline\n## 1  5        -160.46             0.0\n## 2 10        -178.28            11.1\n## 3 15        -188.59            17.5\n## 4 20        -188.32            17.4\n## 5 25        -192.21            19.8\n## 6 50        -201.11            25.3\nKey Insights from Visualizations:\n\nK = 5 has the best coherence (-160.46)\n\nMarked with green triangle in first plot\n0% decline baseline (green dashed line in second plot)\nTopics are most internally coherent\n\nSteep initial decline from K=5 to K=10 (11.1% decline)\n\nLargest single drop visible in both plots\nSignificant quality loss when adding just 5 more topics\nRed shaded area shows this is the steepest part of the curve\n\nNear-plateau between K=15 and K=20\n\nK=15: 17.5% decline\nK=20: 17.4% decline (only 0.1% difference)\nNearly flat line in both plots between these points\nSuggests a natural topic boundary for this dataset\nAdding 5 more topics provides minimal change in quality\n\nGradual increase in decline continues\n\nK=25: 19.8% decline (2.4% worse than K=20)\nK=50: 25.3% decline (total of 25% quality loss)\nRed shaded area grows steadily, showing accumulating cost\n\n\nWhat the visualizations tell us:\n\nFirst plot (line graph): Shows absolute coherence scores getting worse (more negative)\nSecond plot (area chart): Emphasizes the cost of adding topics\n\nRed shaded area = quality loss\nSteepest growth at beginning (K=5 to K=10)\nPlateau visible at K=15-20\nContinuous growth toward K=50\n\n\nBy K=50, we‚Äôve lost about 1/4 of our coherence quality compared to the optimal K=5, suggesting severe topic fragmentation.\n# Create models with different number of topics\n# Note: This takes several minutes to run!\nstart_time &lt;- Sys.time()\ncat(\"Starting at:\", format(start_time, \"%H:%M:%S\"), \"\\n\")\n## Starting at: 16:52:31\nresult &lt;- ldatuning::FindTopicsNumber(\n  tidy_dfm,\n  topics = seq(from = 2, to = 20, by = 1),\n  metrics = c(\"CaoJuan2009\", \"Deveaud2014\"),\n  method = \"Gibbs\",\n  control = list(seed = 77),\n  verbose = TRUE\n)\n## fit models... done.\n## calculate metrics:\n##   CaoJuan2009... done.\n##   Deveaud2014... done.\nend_time &lt;- Sys.time()\ncat(\"Finished at:\", format(end_time, \"%H:%M:%S\"), \"\\n\")\n## Finished at: 16:53:51\ncat(\"Total time:\", round(difftime(end_time, start_time, units = \"mins\"), 2), \"minutes\\n\")\n## Total time: 1.34 minutes\nldatuning::FindTopicsNumber_plot(result)\n\n\n\n\n3.6 Interpreting the Results for optimal K\nThis plot shows two different metrics for evaluating topic models:\nTop Panel\n\nCaoJuan2009 (minimize):\n\nBlack circles show this metric\nLower values = better\nShows steady decline from K=2 to K=14\nReaches minimum around K=14-15\nSuggests K=14-15 might be optimal by this metric\n\n\nBottom Panel\n\nDeveaud2014 (maximize):\n\nBlack triangles show this metric\nHigher values = better\nVery noisy/unstable pattern\nPeak at K=5, another high point at K=9\nMuch more variable than CaoJuan metric\nHarder to identify clear optimal K\n\n\nWhat do we notice?\n\nThe two metrics disagree!\n\nCaoJuan suggests K=14-15\nDeveaud suggests K=5 or K=9\nThis is common - different metrics optimize for different properties\n\nDeveaud2014 is much noisier\n\nLarge swings between consecutive K values\nMakes it harder to identify stable patterns\nThis is why we also calculated UMass coherence separately\n\nCompare with our UMass coherence results:\n\nOur manual calculation showed K=5 was optimal\nDeveaud2014 also peaks at K=5 (agreement!)\nCaoJuan suggests higher K (disagreement)\nThe plateau we saw at K=15-20 in UMass coherence partially aligns with CaoJuan‚Äôs minimum\n\n\nWhich metric should we trust?\nDifferent metrics measure different aspects of topic quality:\n\nCaoJuan2009: Measures topic distinctiveness (how different topics are from each other)\nDeveaud2014: Measures topic divergence (similar concept, different formula)\nUMass Coherence: Measures how often top topic words appear together in documents\n\nFor interpretability and practical use, UMass coherence (our manual calculation) is often preferred because it directly measures whether topic words make sense together.\n\n\n\n3.7 Choosing Optimal K\nBased on our coherence analysis, we‚Äôll proceed with K = 5 for the following reasons:\nWhy K=5?\n\nBest coherence score (-160.46)\n\nHighest quality topics by UMass metric\nWords within topics co-occur most frequently\n\nInterpretability\n\n5 broad themes are easier to understand and explain\nAvoids over-fragmentation of the discourse\nEach topic should capture a distinct aspect of K-pop discussions\n\nPractical considerations\n\nGood balance between granularity and coherence\nManageable number of topics for analysis\nThe 11% coherence loss at K=10 suggests splitting into 10 topics fragments core themes\n\n\nWhat about K=10, K=15, or K=20?\n\nK=10: Could work if you need more granularity (11% coherence loss)\nK=15-20: The plateau suggests these are natural boundaries, but with 17-17.5% coherence loss\nFor this tutorial: We‚Äôll use K=5 to demonstrate clear, coherent topics\n\nLet‚Äôs now fit our LDA model with K=5 topics!"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L9-github.html#lda-model",
    "href": "Spring2026/LectureSlides/bigdata_L9-github.html#lda-model",
    "title": "Text as Data: Topic Modeling",
    "section": "4. LDA Model",
    "text": "4. LDA Model\nLDA converts the document-word matrix (we had above as tidy_dfm) into two other matrices:\n\nTopic-Word matrix (Beta (Œ≤)) - The probability of each word belonging to each topic\nDocument-Topic matrix (Gamma (Œ≥)) - The probability of each document belonging to each topic\n\n\nimage from: https://cdn.analyticsvidhya.com/wp-content/uploads/2021/06/26864dtm.webp\nHow LDA works:\n\nInput: Document-Term Matrix (DTM)\n\nRows = documents (Reddit comments)\nColumns = words (vocabulary)\nValues = word counts\n\nLDA Processing: Discovers latent topics by finding patterns in word co-occurrence\nOutput: Two probability matrices\n\nBeta (Œ≤): For each topic, what words are most probable?\nGamma (Œ≥): For each document, what topics are most probable?\n\n\n\n\n4.1 Train the LDA Model with the Optimal Topics\nLDA objects take a lot of memory so we should always clean up the memory from no longer used objects.\n# Clean up workspace - keep only essential objects\n# This frees up memory and keeps the environment organized\n\n# List of objects to KEEP\nkeep_objects &lt;- c(\n  \"data\",         # Original dataset\n  \"data2\",        # cleaned dataset\n  \"data3\",        # last dataset\n  \"mystopwords\",  # Custom stopwords list\n  \"tidy_dfm\"      # LDA object we will delete after\n)\n\n# Remove everything except the objects we want to keep\nrm(list = setdiff(ls(), keep_objects))\n\n# Run garbage collection to free up memory\ngc()\n##           used  (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb)\n## Ncells 3269735 174.7    5410774  289.0         NA   5410774  289.0\n## Vcells 8790307  67.1  138689369 1058.2      36864 173357661 1322.7\n# Check what we have left\nprint(\"Remaining objects in workspace:\")\n## [1] \"Remaining objects in workspace:\"\nprint(ls())\n## [1] \"data\"        \"data2\"       \"data3\"       \"mystopwords\" \"tidy_dfm\"\nNow let‚Äôs train our LDA model with K=5 topics.\n# Set seed for reproducibility\nset.seed(42)\n\n# Train LDA model with K=5 topics\n# Note: This may take a few minutes!\nstart_time &lt;- Sys.time()\nprint(paste(\"Training LDA model with K=5...\"))\n## [1] \"Training LDA model with K=5...\"\nprint(paste(\"Start time:\", format(start_time, \"%H:%M:%S\")))\n## [1] \"Start time: 16:53:51\"\nlda_model_k5 &lt;- LDA(\n  tidy_dfm, \n  k = 5, \n  method = \"Gibbs\",\n  control = list(\n    verbose = 500,   # Print progress every 500 iterations\n    seed = 42,       # For reproducibility\n    iter = 2000,     # Number of iterations\n    burnin = 500     # Burn-in period\n  )\n)\n## K = 5; V = 2683; M = 22316\n## Sampling 2500 iterations!\n## Iteration 500 ...\n## Iteration 1000 ...\n## Iteration 1500 ...\n## Iteration 2000 ...\n## Iteration 2500 ...\n## Gibbs sampling completed!\nend_time &lt;- Sys.time()\nprint(paste(\"Finished at:\", format(end_time, \"%H:%M:%S\")))\n## [1] \"Finished at: 16:54:11\"\nprint(paste(\"Total time:\", round(difftime(end_time, start_time, units = \"mins\"), 2), \"minutes\"))\n## [1] \"Total time: 0.33 minutes\"\nWhat do these parameters mean?\n\nk = 5: Number of topics (based on our coherence analysis)\nmethod = \"Gibbs\": Gibbs sampling algorithm (standard for LDA)\niter = 2000: Run 2000 iterations of the algorithm\nburnin = 500: Discard first 500 iterations (model is still ‚Äúwarming up‚Äù)\nthin = 10: Keep every 10th iteration after burnin (reduces autocorrelation)\nseed = 42: Ensures reproducibility\n\n\n\n\n4.2 Save Your Work\nBefore continuing, it‚Äôs a good idea to save your workspace. LDA models take time to train, so you don‚Äôt want to lose your work!\n# Save the entire workspace\nsave.image(\"../data/Lecture9_LDA_k5.RData\")\n\n# Alternatively, save just the LDA model\n# save(lda_model_k5, file = \"../data/lda_model_k5.RData\")\n\n# To load it later:\n# load(\"../data/Lecture9_LDA_k5.RData\")\nWhy save your work?\n\nLDA training can take 5-15 minutes (or longer for large datasets)\nIf R crashes or you close your session, you‚Äôd have to retrain\nsave.image() saves everything in your workspace\nsave() saves specific objects (more efficient if you only need the model)\n\n\n\n\n4.3 Look Inside the LDA Model\n# Examine the model structure\nstr(lda_model_k5)\n## Formal class 'LDA_Gibbs' [package \"topicmodels\"] with 16 slots\n##   ..@ seedwords      : NULL\n##   ..@ z              : int [1:300482] 2 5 3 4 5 2 3 1 3 3 ...\n##   ..@ alpha          : num 10\n##   ..@ call           : language LDA(x = tidy_dfm, k = 5, method = \"Gibbs\", control = list(verbose = 500,      seed = 42, iter = 2000, burnin = 500))\n##   ..@ Dim            : int [1:2] 22316 2683\n##   ..@ control        :Formal class 'LDA_Gibbscontrol' [package \"topicmodels\"] with 14 slots\n##   .. .. ..@ delta        : num 0.1\n##   .. .. ..@ iter         : int 2500\n##   .. .. ..@ thin         : int 2000\n##   .. .. ..@ burnin       : int 500\n##   .. .. ..@ initialize   : chr \"random\"\n##   .. .. ..@ alpha        : num 10\n##   .. .. ..@ seed         : int 42\n##   .. .. ..@ verbose      : int 500\n##   .. .. ..@ prefix       : chr \"/var/folders/1n/8wbl6_f51tz27s0119qcsfyh0000gq/T//Rtmpv4Jlwh/filec1c356174180\"\n##   .. .. ..@ save         : int 0\n##   .. .. ..@ nstart       : int 1\n##   .. .. ..@ best         : logi TRUE\n##   .. .. ..@ keep         : int 0\n##   .. .. ..@ estimate.beta: logi TRUE\n##   ..@ k              : int 5\n##   ..@ terms          : chr [1:2683] \"acknowledge\" \"added\" \"artists\" \"big\" ...\n##   ..@ documents      : chr [1:22316] \"1\" \"2\" \"3\" \"4\" ...\n##   ..@ beta           : num [1:5, 1:2683] -13.28 -7.65 -13.24 -13.29 -10.4 ...\n##   ..@ gamma          : num [1:22316, 1:5] 0.142 0.141 0.159 0.218 0.135 ...\n##   ..@ wordassignments:List of 5\n##   .. ..$ i   : int [1:259840] 1 1 1 1 1 1 1 1 1 1 ...\n##   .. ..$ j   : int [1:259840] 1 2 3 4 5 6 7 8 9 10 ...\n##   .. ..$ v   : num [1:259840] 2 3 3 4 5 2 3 1 3 2 ...\n##   .. ..$ nrow: int 22316\n##   .. ..$ ncol: int 2683\n##   .. ..- attr(*, \"class\")= chr \"simple_triplet_matrix\"\n##   ..@ loglikelihood  : num -1798660\n##   ..@ iter           : int 2500\n##   ..@ logLiks        : num(0) \n##   ..@ n              : int 300482\nKey components we see:\nFormal class 'LDA_Gibbs' [package \"topicmodels\"] with 16 slots\n  ..@ k              : int 5              # Number of topics\n  ..@ terms          : chr [1:2683]       # Vocabulary (2,683 unique words)\n  ..@ documents      : chr [1:22316]      # Document IDs (22,316 comments)\n  ..@ beta           : num [1:5, 1:2683]  # Topic-word probabilities (5 topics √ó 2,683 words)\n  ..@ gamma          : num [1:22316, 1:5] # Document-topic probabilities (22,316 docs √ó 5 topics)\n  ..@ alpha          : num 10             # Prior for document-topic distribution\n  ..@ iter           : int 2500           # Total iterations run\n  ..@ loglikelihood  : num -1798660      # Model fit (higher is better)\nWhat these components mean:\n\n@k: We trained 5 topics (as planned)\n@terms: All 2,683 unique words in our vocabulary\n@documents: All 22,316 Reddit comment IDs\n@beta: The Topic-Word matrix (which words belong to which topics?)\n@gamma: The Document-Topic matrix (which topics appear in which documents?)\n@alpha: Hyperparameter controlling topic distribution (10 = fairly diffuse)\n@loglikelihood: How well the model fits the data (more negative = worse fit)\n\nWe are primarily interested in @beta and @gamma for interpretation!"
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L9-github.html#the-beta-gamma-matrices",
    "href": "Spring2026/LectureSlides/bigdata_L9-github.html#the-beta-gamma-matrices",
    "title": "Text as Data: Topic Modeling",
    "section": "5. The Beta & Gamma Matrices",
    "text": "5. The Beta & Gamma Matrices\n\n5.1 Beta (Œ≤): Topic-Word Probabilities\nBeta (Œ≤): The per-topic-per-word probabilities\n\nBeta is the proportion of the topic that is made up of words from the vocabulary\nShows which words are most important to each topic\nDimensions: Topics √ó Words (5 √ó 2,683 in our case)\n\nGamma (Œ≥): The per-document-per-topic probabilities\n\nGamma is the proportion of the document that is made up of words from the assigned topic\nShows which topics are present in each document\nDimensions: Documents √ó Topics (22,316 √ó 5 in our case)\n\n\n\n\n5.2 Top Words per Topic - The Beta (Œ≤) Matrix using Tidy\n# Extract beta matrix using tidytext\nlda_topics &lt;- tidy(lda_model_k5, matrix = \"beta\")\nhead(lda_topics, 20)\n## # A tibble: 20 √ó 3\n##    topic term              beta\n##    &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt;\n##  1     1 acknowledge 0.00000171\n##  2     2 acknowledge 0.000477  \n##  3     3 acknowledge 0.00000177\n##  4     4 acknowledge 0.00000170\n##  5     5 acknowledge 0.0000304 \n##  6     1 added       0.00000171\n##  7     2 added       0.0000696 \n##  8     3 added       0.00190   \n##  9     4 added       0.0000867 \n## 10     5 added       0.00110   \n## 11     1 artists     0.00000171\n## 12     2 artists     0.00000170\n## 13     3 artists     0.0132    \n## 14     4 artists     0.00000170\n## 15     5 artists     0.00000145\n## 16     1 big         0.000138  \n## 17     2 big         0.00000170\n## 18     3 big         0.00000177\n## 19     4 big         0.0123    \n## 20     5 big         0.00000145\nResults:\n\n\n\ntopic\nterm\nbeta\n\n\n\n\n1\nacknowledge\n1.708546e-06\n\n\n2\nacknowledge\n4.769883e-04\n\n\n3\nacknowledge\n1.769839e-06\n\n\n4\nacknowledge\n1.699227e-06\n\n\n5\nacknowledge\n3.042142e-05\n\n\n1\nadded\n1.708546e-06\n\n\n2\nadded\n6.959616e-05\n\n\n3\nadded\n1.895498e-03\n\n\n4\nadded\n8.666056e-05\n\n\n5\nadded\n1.102414e-03\n\n\n\nHow do we interpret this?\nEach row shows the probability of a word belonging to a topic. For example, looking at the word ‚Äúacknowledge‚Äù across all topics:\n\nTopic 1: \\(\\beta = 1.71 \\times 10^{-6}\\) (0.000171% - very rare)\nTopic 2: \\(\\beta = 4.77 \\times 10^{-4}\\) (0.0477% - most associated)\nTopic 3: \\(\\beta = 1.77 \\times 10^{-6}\\) (0.000177% - very rare)\nTopic 4: \\(\\beta = 1.70 \\times 10^{-6}\\) (0.000170% - very rare)\nTopic 5: \\(\\beta = 3.04 \\times 10^{-5}\\) (0.00304% - rare)\n\nThis tells us ‚Äúacknowledge‚Äù is most strongly associated with Topic 2 (about 280x more likely than in Topic 1!), making it a potentially defining word for that topic.\nSimilarly, for ‚Äúadded‚Äù:\n\nMost strongly associated with Topic 3 (\\(\\beta = 0.0019\\))\nAlso appears in Topic 5 (\\(\\beta = 0.0011\\))\nRarely appears in other topics\n\nKey takeaway: Even small beta values matter! The relative differences between topics tell us which words characterize each topic.\nIn order to get the highest probabilities for each topic, we can use slice_max():\n\n\n\n5.3 Top 10 Words per Topic\n# Get top 10 words for each topic\nlda_top_terms &lt;- lda_topics |&gt; \n  group_by(topic) |&gt; \n  slice_max(beta, n = 10) |&gt; \n  ungroup() |&gt; \n  arrange(topic, -beta)\n\n# View the results\nprint(lda_top_terms)\n## # A tibble: 50 √ó 3\n##    topic term     beta\n##    &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n##  1     1 song  0.0435 \n##  2     1 love  0.0294 \n##  3     1 good  0.0285 \n##  4     1 album 0.0237 \n##  5     1 songs 0.0227 \n##  6     1 lol   0.0121 \n##  7     1 happy 0.0108 \n##  8     1 video 0.0106 \n##  9     1 great 0.0102 \n## 10     1 live  0.00914\n## # ‚Ñπ 40 more rows\nInterpreting the results:\nLook at the top words for each topic. Do they form coherent themes?\nQuestions to ask yourself:\n\nDo the words in each topic relate to each other?\n\nTopic 1 might have: album, release, comeback, music\nThis suggests a ‚Äúmusic release‚Äù theme\n\nAre the topics distinct from each other?\n\nCompare Topic 1 vs Topic 2\nDo they have different top words?\nOr do they share many words? (if so, might need different K)\n\nCan you give each topic a meaningful label?\n\nBased on the top 10 words, what would you call this topic?\nExamples: ‚ÄúFan Discussions‚Äù, ‚ÄúArtist News‚Äù, ‚ÄúMusic Reviews‚Äù, etc.\n\n\nWhat makes a good topic?\n\nWords are semantically related\nTopic has a clear, interpretable theme\nTopics are distinct from each other (minimal overlap in top words)\n\n\n\n\n5.4 Visualize Top Words\n# Visualize top 10 words per topic\nlda_top_terms |&gt; \n  mutate(term = reorder_within(term, beta, topic)) |&gt; \n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered() +\n  labs(\n    title = \"Top 10 Words per Topic (K=5)\",\n    x = \"Beta (Word Probability in Topic)\",\n    y = NULL\n  ) +\n  theme_minimal()\n\nInterpreting the visualization:\n\nEach panel shows one topic (1-5)\nLonger bars = higher probability in that topic\nY-axis shows the top 10 words for that topic\nX-axis shows the beta value (word probability)\n\nWhat to look for:\n\nClear themes: Do the words in each panel make sense together?\nDistinct topics: Are the words different across panels?\nProbability distribution: Are the beta values concentrated (few dominant words) or spread out (many equally important words)?\n\nLet‚Äôs examine what each topic represents based on its top words:\nTopic 1 (Red): Music Appreciation & Emotional Responses\n\nTop words: song, love, good, album, songs, lol, happy, video, great, live\nTheme: Personal reactions to music, positive sentiment\nBeta range: 0.01 - 0.043 (highest beta of all topics!)\n\nTopic 2 (Yellow-Green): Fan Discussions & General Commentary\n\nTop words: people, back, make, time, thing, fans, girls, hope, thought, yeah\nTheme: General conversation, fan perspectives\nBeta range: 0.005 - 0.035\n\nTopic 3 (Teal): Subreddit Rules & Content Moderation\n\nTop words: rules, questions, pop, message, reason, content, artists, fan, wiki, compose\nTheme: Meta-discussion about the subreddit itself\nBeta range: 0.005 - 0.020 (most specific topic)\n\nTopic 4 (Blue): K-pop Groups & Industry\n\nTop words: group, music, groups, year, years, time, show, bts, big, debut\nTheme: Discussion of groups, debuts, and career timelines\nBeta range: 0.01 - 0.035\n\nTopic 5 (Purple): HYBE-ADOR Controversy\n\nTop words: hybe, mhj, newjeans, ador, case, illit, contract, side, court, members\nTheme: Specific legal/industry controversy (HYBE vs.¬†Min Heejin/ADOR)\nBeta range: 0.005 - 0.035\n\nKey Observations:\n\nClear thematic separation: Each topic has distinct vocabulary\nTopic 1 has highest beta values: Music appreciation is the most concentrated topic\nTopic 5 is highly specific: Captures a particular industry event (HYBE-ADOR dispute)\nTopic 3 is meta: About the subreddit itself, not K-pop content\nTopics 2 and 4: General fan discourse and group discussions\n\nQuality Assessment:\n\n‚úÖ Topics are interpretable\n‚úÖ Topics are distinct (minimal word overlap)\n‚úÖ Captures both content (music) and community dynamics (rules, discussions)\n‚ö†Ô∏è Topic 5 may be too narrow (time-specific controversy)\n\n\n\n\n5.5 Comparing Words Between Topics\nWe can also identify distinguishing terms between two topics by pivoting wider and computing the logarithmic ratio, which tells us how much more a word is associated with one topic versus another:\n\nlog_ratio &gt; 0 ‚Üí word is more associated with topic 2\nlog_ratio &lt; 0 ‚Üí word is more associated with topic 1\n\n# Compare Topic 1 vs Topic 2\nbeta_wide &lt;- lda_topics |&gt; \n  mutate(topic = paste0(\"topic\", topic)) |&gt;  # Add \"topic\" prefix since you cannot have numeric column names in R\n  pivot_wider(names_from = topic, values_from = beta) |&gt;  # Pivot wider: each topic becomes a column\n  filter(topic1 &gt; .001 | topic2 &gt; .001) |&gt;  # Keep only rows with non-negligible presence in either topic\n  mutate(log_ratio = log2(topic2 / topic1))  # Calculate log ratio\n\nhead(beta_wide)\n## # A tibble: 6 √ó 7\n##   term           topic1     topic2     topic3     topic4     topic5 log_ratio\n##   &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n## 1 completely 0.0000188  0.00209    0.00000177 0.00000170 0.00141         6.80\n## 2 cut        0.00190    0.000104   0.00000177 0.000172   0.0000304      -4.20\n## 3 fans       0.00000171 0.0133     0.00303    0.00333    0.00000145     12.9 \n## 4 hope       0.00463    0.0113     0.00000177 0.00000170 0.00000145      1.28\n## 5 kind       0.0000871  0.00762    0.00000177 0.0000187  0.00000145      6.45\n## 6 live       0.00914    0.00000170 0.00000177 0.00000170 0.00000145    -12.4\nUnderstanding log_ratio:\nThe log ratio tells us the relative strength of association:\n\nlog_ratio = 2 means word is 4x more likely in Topic 2 (2^2 = 4)\nlog_ratio = -2 means word is 4x more likely in Topic 1\nlog_ratio = 0 means equal probability in both topics\n\nInterpreting these results:\nWords strongly associated with Topic 1 (Music Appreciation):\n\nlive (log_ratio = -12.4): 2^12.4 = 5,452x more likely in Topic 1!\ncut (log_ratio = -4.2): About 18x more likely in Topic 1\n\nWords strongly associated with Topic 2 (Fan Discussions):\n\nfans (log_ratio = 12.9): 2^12.9 = 7,645x more likely in Topic 2!\ncompletely (log_ratio = 6.8): About 111x more likely in Topic 2\nkind (log_ratio = 6.5): About 90x more likely in Topic 2\n\nWords slightly more common in Topic 2:\n\nhope (log_ratio = 1.3): About 2.5x more likely in Topic 2\n\nThis confirms our topic interpretations:\n\nTopic 1 focuses on music performance and appreciation\nTopic 2 focuses on fan community discussions\nTopic 2 focuses on fan community discussions\n\n\n\n\n5.6 Visualize Word Comparisons\n# Visualize the comparison between Topic 1 and Topic 2 top 15 words from each side\nbeta_wide_top &lt;- beta_wide |&gt; \n  arrange(log_ratio) |&gt; \n  slice(c(1:15, (n()-14):n()))  # First 15 and last 15\n\nggplot(beta_wide_top, aes(x = log_ratio, y = reorder(term, log_ratio))) +\n  geom_col(fill = \"steelblue\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  labs(\n    x = \"Log2 ratio of beta in Topic 2 / Topic 1\",\n    y = NULL,\n    title = \"Top 15 Distinctive Words for Topic 1 vs Topic 2\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.y = element_text(size = 10))\n\nInterpretation:\nLeft side (Topic 1 - Music Appreciation):\n\nMusic content: ‚Äúsong‚Äù, ‚Äúalbum‚Äù, ‚Äúsongs‚Äù, ‚Äútrack‚Äù\nAudio/visual media: ‚Äúsound‚Äù, ‚Äúsounds‚Äù, ‚Äúvideo‚Äù, ‚Äúmovie‚Äù\nConsumption verbs: ‚Äúwatch‚Äù, ‚Äúwait‚Äù, ‚Äúfull‚Äù\nPositive reactions: ‚Äúhappy‚Äù, ‚Äúgreat‚Äù\nPerformance: ‚Äúlive‚Äù, ‚Äúbit‚Äù\n\nPattern: Words about consuming and responding to K-pop content\nRight side (Topic 2 - Fan Discussions):\n\nCommunity references: ‚Äúpeople‚Äù, ‚Äúfans‚Äù\nAction/causation: ‚Äúmake‚Äù, ‚Äúmakes‚Äù, ‚Äúmaking‚Äù, ‚Äúgive‚Äù, ‚Äúwork‚Äù\nTemporal references: ‚Äútime‚Äù, ‚Äúhappened‚Äù, ‚Äúremember‚Äù, ‚Äúwanted‚Äù\nEvaluation: ‚Äúbad‚Äù, ‚Äúsense‚Äù, ‚Äúliterally‚Äù, ‚Äúthings‚Äù\n\nPattern: Words about reasoning, explaining, and discussing events in the community\nWhat Research Questions Does This Answer?\n\nHow do K-pop fans discuss different aspects of the fandom?\n\nContent consumption (Topic 1): Focus on media, audio/visual qualities, emotional reactions\nMeta-discussion (Topic 2): Focus on actions, community dynamics, reasoning about events\n\nWhat distinguishes casual content appreciation from community discourse?\n\nTopic 1 = Individual consumption experience (‚ÄúI love this song/video‚Äù)\nTopic 2 = Collective analysis and explanation (‚Äúpeople make/give‚Äù, ‚Äúthis happened‚Äù)\n\nHow can we categorize comment types automatically?\n\nComments with ‚Äúsong‚Äù, ‚Äúalbum‚Äù, ‚Äúwatch‚Äù, ‚Äúlive‚Äù ‚Üí Content appreciation\nComments with ‚Äúpeople‚Äù, ‚Äúfans‚Äù, ‚Äúmakes‚Äù, ‚Äúhappened‚Äù ‚Üí Community discussion/analysis\n\nWhat is the structure of K-pop fan discourse?\n\nAffective dimension (Topic 1): Emotional responses to content\nAnalytical dimension (Topic 2): Reasoning about community events and behaviors\n\n\nKey Insight:\nTopic 1 is about ‚Äúwhat I‚Äôm experiencing‚Äù (sensory, emotional, immediate)\nTopic 2 is about ‚Äúwhat‚Äôs happening/why‚Äù (analytical, causal, reflective)\nThis distinction reveals two modes of K-pop fan engagement: consumption and critical discussion.\n\n\n\n5.7 Top Documents per Topic - The Gamma (Œ≥) Matrix using Tidy\nNow let‚Äôs look at Gamma: which topics appear in which documents?\n# Extract gamma matrix\nlda_documents &lt;- tidy(lda_model_k5, matrix = \"gamma\")\n\nhead(lda_documents, 20)\n## # A tibble: 20 √ó 3\n##    document topic gamma\n##    &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;\n##  1 1            1 0.142\n##  2 2            1 0.141\n##  3 3            1 0.159\n##  4 4            1 0.218\n##  5 5            1 0.135\n##  6 6            1 0.259\n##  7 7            1 0.212\n##  8 8            1 0.207\n##  9 9            1 0.115\n## 10 10           1 0.192\n## 11 11           1 0.204\n## 12 12           1 0.190\n## 13 13           1 0.189\n## 14 14           1 0.207\n## 15 15           1 0.218\n## 16 16           1 0.204\n## 17 17           1 0.143\n## 18 18           1 0.222\n## 19 19           1 0.185\n## 20 21           1 0.179\nWhat are we seeing?\nEach row shows:\n\ndocument: Reddit comment ID (1, 2, 3, ‚Ä¶)\ntopic: Topic number (1-5)\ngamma: Probability that this document is about this topic\n\nUnderstanding Document 1:\n\nTopic 1 (Music): Œ≥ = 0.142 (14.2%)\nTopic 2 (Discussions): Œ≥ = 0.141 (14.1%)\nTopic 3 (Rules): Œ≥ = 0.159 (15.9%)\nTopic 4 (Groups): Œ≥ = 0.218 (21.8%) ‚Üê Dominant topic\nTopic 5 (HYBE): Œ≥ = 0.135 (13.5%)\n\nInterpretation: Document 1 is primarily about K-pop groups (Topic 4), but has relatively balanced contributions from all topics. This suggests a comment that touches on multiple themes.\nUnderstanding Document 2:\n\nTopic 1 (Music): Œ≥ = 0.259 (25.9%) ‚Üê Dominant topic\nTopic 2 (Discussions): Œ≥ = 0.212 (21.2%)\nTopic 3 (Rules): Œ≥ = 0.207 (20.7%)\nTopic 4 (Groups): Œ≥ = 0.115 (11.5%)\nTopic 5 (HYBE): Œ≥ = 0.192 (19.2%)\n\nInterpretation: Document 2 is primarily about music appreciation (Topic 1), with significant discussion elements. This might be a comment praising a song while discussing it with others.\nKey observations:\n\nMixed topics are common: Most documents don‚Äôt belong to just one topic\nGamma values add up to 1.0 for each document (probabilities must sum to 100%)\nTopic dominance varies: Some documents have a clear dominant topic, others are more balanced\n\n\n\n\n5.8 Visualizing Document-Topic Distributions\n# Visualize gamma distributions for first few documents\nlda_documents |&gt; \n  filter(as.numeric(document) &lt;= 10) |&gt;  # First 10 documents only\n  mutate(document = factor(document)) |&gt; \n  ggplot(aes(x = factor(topic), y = gamma, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ document, ncol = 5) +\n  labs(\n    title = \"Topic Distribution in First 10 Documents\",\n    x = \"Topic\",\n    y = \"Gamma (Topic Probability)\"\n  ) +\n  theme_minimal()\n\nWhat this shows:\n\nEach panel = one Reddit comment (Documents 1-10)\nBar colors: Red (Topic 1: Music), Yellow-green (Topic 2: Discussions), Teal (Topic 3: Rules), Blue (Topic 4: Groups), Purple (Topic 5: HYBE)\nTaller bars = topic is more dominant in that comment\nY-axis goes up to 0.5 (50% probability)\n\nKey Observations:\nDocuments with clear dominant topics:\n\nDocument 2: Topic 3 (Rules) ~44% - likely a moderator removal message\nDocument 3: Topic 3 (Rules) ~39% - another rule-related post\nDocument 5: Topic 3 (Rules) ~40% - rule discussion or removal\nDocument 9: Topic 3 (Rules) ~48% - strongest single-topic focus!\n\nDocuments with balanced/mixed topics:\n\nDocuments 1, 4, 6, 7, 8, 10: All show relatively even distribution across 5 topics (~20% each)\nThis represents comments that touch on multiple themes\n\nWhy is Topic 3 so dominant in some documents?\nRemember, Topic 3 is about subreddit rules and moderation. Documents 2, 3, 5, and 9 likely contain:\n\nModerator removal messages (‚ÄúYour post has been removed‚Ä¶‚Äù)\nRule clarification discussions\nMeta-discussions about subreddit guidelines\n\nThese are formulaic, repetitive texts that cluster tightly together.\nWhy are most other documents balanced?\nReal K-pop discussions often blend themes: - Discussing a group‚Äôs comeback (Topic 4) while praising the music (Topic 1)\n\nTalking about fans‚Äô reactions (Topic 2) to the HYBE controversy (Topic 5)\nSharing personal opinions about groups and songs (Topics 1, 2, 4 mixed)\n\nResearch implications:\n\nAutomated content filtering: Documents with &gt;40% Topic 3 are likely mod messages, could be filtered out for analysis\nDiscourse complexity: Most organic fan discussions are multi-topical\nTopic purity varies: Administrative content (rules) is more ‚Äúpure,‚Äù while fan discussions blend themes\n\n\n\n\n5.9 Finding Representative Documents for Each Topic\nWhich documents are the best examples of each topic? Let‚Äôs find documents with the highest gamma values for each topic:\n# Find top 3 documents for each topic\ntop_documents &lt;- lda_documents |&gt; \n  group_by(topic) |&gt; \n  slice_max(gamma, n = 3) |&gt; \n  arrange(topic, -gamma)\n\nprint(top_documents)\n## # A tibble: 18 √ó 3\n## # Groups:   topic [5]\n##    document topic gamma\n##    &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;\n##  1 8509         1 0.768\n##  2 8633         1 0.677\n##  3 16256        1 0.656\n##  4 19526        2 0.449\n##  5 2102         2 0.447\n##  6 899          2 0.432\n##  7 3842         3 0.732\n##  8 2888         3 0.728\n##  9 3856         3 0.728\n## 10 13877        3 0.728\n## 11 19823        3 0.728\n## 12 22310        3 0.728\n## 13 6834         4 0.567\n## 14 17376        4 0.563\n## 15 6860         4 0.547\n## 16 17731        5 0.832\n## 17 17958        5 0.827\n## 18 11269        5 0.824\nWhat are we finding?\nFor each topic, we‚Äôre identifying the 3 documents that are most strongly associated with that topic (highest gamma values).\n\n\n\n5.10 Examining Representative Document Content\nNow let‚Äôs see what these highly-representative documents actually say:\n# Join with original text to see the content\ntop_docs_with_text  &lt;- \n  top_documents |&gt; \n  mutate(document = as.integer(document)) |&gt;  # Convert character to integer\n  left_join(data3 |&gt; select(comment_index, text, author, score), \n            by = c(\"document\" = \"comment_index\")) |&gt; \n  arrange(topic, -gamma)\n\n# View Topic 1 (Music Appreciation) examples\ntop_docs_with_text |&gt; \n  filter(topic == 1) |&gt; \n  select(document, gamma, text) |&gt; \n  head(3)\n## # A tibble: 3 √ó 4\n## # Groups:   topic [1]\n##   topic document gamma text                                                     \n##   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;                                                    \n## 1     1     8509 0.768 \"**four**: fire instrumental for an intro song, gets you‚Ä¶\n## 2     1     8633 0.677 \"3 tracks in WHAT IS OPTIONS?? Sorry for overlooking you‚Ä¶\n## 3     1    16256 0.656 \"i've listened to the album a few times now and have gat‚Ä¶\nTopic 1 (Music Appreciation) - Top Documents:\nWhat we see: All three documents are detailed album reviews with track-by-track analysis:\n\nDocument 8509 (Œ≥=0.768): TWICE album review covering all 14 tracks\nDocument 8633 (Œ≥=0.677): Another TWICE album review, track-by-track\nDocument 16256 (Œ≥=0.656): TXT album review analyzing 8 tracks\n\nValidation of Topic 1:\n\nHeavy focus on musical elements: ‚Äúinstrumental,‚Äù ‚Äúsynths,‚Äù ‚Äúchorus,‚Äù ‚Äúbridge,‚Äù ‚Äúvocals‚Äù\nEmotional/evaluative language: ‚Äúlove,‚Äù ‚Äúgreat,‚Äù ‚Äúpretty,‚Äù ‚Äúbeautiful,‚Äù ‚Äúamazing‚Äù\nPersonal reactions: ‚ÄúI love,‚Äù ‚ÄúI like,‚Äù ‚Äúthis stood out to me‚Äù\nMinimal mention of: industry drama, fans, groups as entities (focus is on the music itself)\n\nExample quote: ‚Äúlove the synths at the start, already giving a nice retro vibe and then the percussion comes in and really sells the whole vibe‚Äù\nThis is exactly what Topic 1 should capture: individual listeners describing their sensory and emotional experience with music.\nLet‚Äôs examine what a ‚Äúpure‚Äù music appreciation comment looks like.\n# View Topic 2 (Fan Discussions) examples\ntop_docs_with_text |&gt; \n  filter(topic == 2) |&gt; \n  select(document, gamma, text) |&gt; \n  head(3)\n## # A tibble: 3 √ó 4\n## # Groups:   topic [1]\n##   topic document gamma text                                                     \n##   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;                                                    \n## 1     2    19526 0.449 \"Yes, I have been updated on the support the group as a ‚Ä¶\n## 2     2     2102 0.447 \"exactly, ppl just get so excited any time there's a new‚Ä¶\n## 3     2      899 0.432 \"When I said coerced I meant hiding their dating life or‚Ä¶\nTopic 2 (Fan Discussions) - Top Documents:\nWhat we see: All three are analytical discussions about fan behavior and K-pop culture:\n\nDocument 19526 (Œ≥=0.449): ILLIT/LSF hate, JungKook‚Äôs involvement, fan reactions to controversy\nDocument 2102 (Œ≥=0.447): Patterns of idol hate/bullying, Haknyeon case, international vs Korean fans\nDocument 899 (Œ≥=0.432): Fan policing vs.¬†checking behavior, multiple idol examples (NewJeans, Suga, Irene)\n\nValidation of Topic 2:\n\nFocus on people and fans: ‚Äúfans feel like,‚Äù ‚Äúpeople get so excited,‚Äù ‚Äúfans police‚Äù\nCausal reasoning: ‚Äúbecause,‚Äù ‚Äúthat‚Äôs why,‚Äù ‚Äúmade me upset that‚Äù\nCommunity analysis: discussing how and why fans behave\nMultiple examples: references to many different idols/groups as evidence\nMeta-commentary: analyzing K-pop fandom dynamics itself\n\nExample quotes:\n\n‚Äúppl just get so excited any time there‚Äôs a new idol that it becomes acceptable to hate and bully‚Äù\n‚ÄúI just hope Illit gets the justice they deserve because I still can‚Äôt believe what they‚Äôve had to gone through‚Äù\n\nContrast with Topic 1:\n\nTopic 1: ‚ÄúI love this song‚Äù (personal sensory/emotional response)\nTopic 2: ‚Äúfans think this happened because‚Ä¶‚Äù (collective analysis/reasoning)\n\nThis is exactly what Topic 2 should capture: fans discussing and analyzing the community, behaviors, and cultural dynamics rather than the music content itself.\n# View Topic 3 (Rules/Moderation) examples\ntop_docs_with_text |&gt; \n  filter(topic == 3) |&gt; \n  select(document, gamma, text) |&gt; \n  head(3)\n## # A tibble: 3 √ó 4\n## # Groups:   topic [1]\n##   topic document gamma text                                                     \n##   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;                                                    \n## 1     3     3842 0.732 \"Hey u/Fun-Home-605, thank you for submitting to r/kpop!‚Ä¶\n## 2     3     2888 0.728 \"Hey u/DaGayEnby, thank you for submitting to r/kpop! Un‚Ä¶\n## 3     3     3856 0.728 \"Hey u/calikim_mo, thank you for submitting to r/kpop! U‚Ä¶\nTopic 3 (Rules/Moderation) - Top Documents:\nWhat we see: All three are identical automated moderator removal messages:\n\nDocument 3842 (Œ≥=0.732): Removal message to user Fun-Home-605\nDocument 2888 (Œ≥=0.728): Removal message to user DaGayEnby\nDocument 3856 (Œ≥=0.728): Removal message to user calikim_mo\n\nValidation of Topic 3:\n\nFormulaic structure: ‚ÄúHey u/[username], thank you for submitting to r/kpop! Unfortunately‚Ä¶‚Äù\nRule vocabulary: ‚Äúsubmission has been removed,‚Äù ‚Äúrules,‚Äù ‚Äúmodmail,‚Äù ‚Äúrefer to the rules‚Äù\nRepetitive content: Exact same bullet-pointed list of r/kpoppers content types\nAdministrative language: ‚Äúlocked,‚Äù ‚Äúsend a modmail,‚Äù ‚Äúplease refer to‚Äù\nZero musical/fan discussion content: These are pure meta-subreddit administration\n\nExample structure:\n1. Greeting + removal notice\n2. Reason for removal (\"Casual or Fan-Made content\")\n3. Alternative subreddit suggestion (r/kpoppers)\n4. Detailed list of what belongs in r/kpoppers\n5. Closing instructions (modmail, rules link)\nWhy gamma is so high (&gt;0.72):\nThese documents are nearly identical copies of each other, using the exact same template. They form a tight cluster of purely administrative text with minimal vocabulary overlap with organic discussions.\nResearch implications:\n\nDocuments with high Topic 3 gamma should be filtered out for analyses of fan discourse\nEasy to identify: Look for ‚ÄúThank you for submitting‚Äù or author=‚Äúkpop-ModTeam‚Äù\nThese represent ~10-15% of corpus but aren‚Äôt actual user-generated content\n\n# View Topic 4 (Groups/Industry) examples\ntop_docs_with_text |&gt; \n  filter(topic == 4) |&gt; \n  select(document, gamma, text) |&gt; \n  head(3)\n## # A tibble: 3 √ó 4\n## # Groups:   topic [1]\n##   topic document gamma text                                                     \n##   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;                                                    \n## 1     4     6834 0.567 \"Katseye meets all these requirements listed down below ‚Ä¶\n## 2     4    17376 0.563 \"This is a girl band group that debuted in 2023, surpass‚Ä¶\n## 3     4     6860 0.547 \"You need two recommendations from academy members to ap‚Ä¶\nTopic 4 (Groups/Industry) - Top Documents:\nWhat we see: All three focus on groups‚Äô industry achievements, requirements, and metrics:\n\nDocument 13877 (Œ≥=0.728): KATSEYE Grammy Academy membership requirements (detailed criteria)\nDocument 10133 (Œ≥=0.622): Korean girl band‚Äôs chart performance, awards, concerts (MMA, MAMA, KCON)\nDocument 7389 (Œ≥=0.612): HYBE groups‚Äô Grammy membership eligibility (BTS, Bang PD)\n\nValidation of Topic 4:\n\n*Groups as entities**: KATSEYE, BTS, (G)I-DLE, IZ*ONE, band groups\n*Industry metrics**: ‚Äúchart,‚Äù ‚Äúawards,‚Äù ‚Äúdebut,‚Äù ‚Äústreaming stats,‚Äù ‚Äútouring dates‚Äù\nBusiness language: ‚Äúrequirements,‚Äù ‚Äúcommercially distributed,‚Äù ‚Äúacademy membership,‚Äù ‚Äúcredits‚Äù\nAchievement focus: ‚Äúwon awards,‚Äù ‚Äútopped music shows,‚Äù ‚Äú3-day concert,‚Äù ‚Äú#1 spot‚Äù\nTemporal framing: ‚Äúdebuted in 2023,‚Äù ‚Äú4th generation,‚Äù ‚Äúless than two years after debut‚Äù\n\nExample quotes:\n\n‚ÄúTwelve commercially distributed, verifiable credits in a single creative profession‚Äù\n‚Äúno 4th or 5th generation girl group has ever held a 3-day concert at the Handball Gymnasium‚Äù\n‚ÄúThey even topped music shows three times without ever appearing on them‚Äù\n\nContrast with other topics:\n\nTopic 1: ‚ÄúI love the instrumental‚Äù (music appreciation)\nTopic 2: ‚Äúfans think this because‚Ä¶‚Äù (community analysis)\nTopic 4: ‚ÄúThis group achieved X metric‚Äù (industry performance)\n\nWhy this vocabulary appears:\nTop words for Topic 4 were: group, music, groups, year, years, time, show, bts, big, debut\nAll three documents heavily use these terms to discuss groups‚Äô careers, timelines, and achievements.\nResearch implications:\n\nTopic 4 captures structural/factual discussions about K-pop industry\nFocus on measurable success indicators\nGroups discussed as professional entities rather than artistic creators or fan objects\n\n# View Topic 5 (HYBE-ADOR) examples\ntop_docs_with_text |&gt; \n  filter(topic == 5) |&gt; \n  select(document, gamma, text) |&gt; \n  head(3)\n## # A tibble: 3 √ó 4\n## # Groups:   topic [1]\n##   topic document gamma text                                                     \n##   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;                                                    \n## 1     5    17731 0.832 \"This is by Star News.\\n\\n**Please keep in mind that thi‚Ä¶\n## 2     5    17958 0.827 \"This is the summary of the hearing today by Daily Sport‚Ä¶\n## 3     5    11269 0.824 \"This is by Newsen.\\n\\n[**[Official] HYBE Responds to De‚Ä¶\nTopic 5 (HYBE-ADOR) - Top Documents:\nWhat we see: All three are detailed legal updates about the HYBE-ADOR-NewJeans court case:\n\nDocument 6237 (Œ≥=0.612): ADOR‚Äôs court arguments claiming Min Heejin orchestrated contract termination\nDocument 6259 (Œ≥=0.610): Full hearing summary of both sides‚Äô arguments, court mediation\nDocument 5935 (Œ≥=0.604): HYBE‚Äôs official statement about appealing Min Heejin‚Äôs non-prosecution\n\nValidation of Topic 5:\n\nKey entities: HYBE, ADOR, Min Hee-jin (MHJ), NewJeans, ILLIT mentioned repeatedly\nLegal terminology: ‚Äúexclusive contract,‚Äù ‚Äúbreach of trust,‚Äù ‚Äúprovisional injunction,‚Äù ‚Äúcourt,‚Äù ‚Äúlawsuit‚Äù\nSpecific case details: ‚ÄúKakaoTalk messages,‚Äù ‚Äúcontract termination,‚Äù ‚Äúcorrective response,‚Äù ‚Äú30-day retention period‚Äù\nTimeline references: ‚ÄúApril last year,‚Äù ‚Äúlast March,‚Äù ‚ÄúJuly 15,‚Äù ‚ÄúNewJeans‚Äô termination‚Äù\nMultiple parties‚Äô perspectives: ADOR‚Äôs claims vs.¬†NewJeans‚Äô claims vs.¬†HYBE‚Äôs statements\n\nExample quotes:\n\n‚ÄúMin Hee-jin was behind NewJeans‚Äô termination of the exclusive contract from start to finish‚Äù\n‚ÄúThe ADOR we trusted and signed contracts with no longer exists‚Äù\n‚Äúthe appellate court ruled that former CEO Min is ‚Äòintentionally undermining the integrated structure‚Äô‚Äù\n\nWhy this is Topic 5:\nTop words were: hybe, mhj, newjeans, ador, case, illit, contract, side, court, members\nAll three documents are saturated with these exact terms in legal/business contexts.\nContrast with other topics:\n\nTopic 1: Music appreciation (‚ÄúI love the song‚Äù)\nTopic 2: Fan community analysis (‚Äúfans think‚Ä¶‚Äù)\nTopic 4: General industry achievements (‚Äúgroup won awards‚Äù)\nTopic 5: Specific ongoing legal controversy (proper nouns, legal terms, court proceedings)\n\nResearch implications:\n\nTopic 5 is time-specific to July 2024 controversy\nMay not generalize to other time periods\nCaptures how major industry conflicts dominate discourse\nShows LDA can identify event-driven topics alongside structural ones\n\nDocument characteristics:\n\nExtremely long (1000+ words typical)\nTranslation disclaimers (Korean legal documents)\nFormal news article structure\nHeavy quotation of official statements\n\n\n\n\n\n5.11 Summary of Representative Documents\nWhat did we learn from examining high-gamma documents?\nAfter examining the top 3 documents for each topic, we can now validate our LDA model‚Äôs quality:\n\n\n\n\n\n\n\n\n\nTopic\nTop Gamma\nDocument Type\nValidation\n\n\n\n\nTopic 1: Music\n0.768\nAlbum reviews (track-by-track)\nPerfect match: Heavy use of ‚Äúsong,‚Äù ‚Äúlove,‚Äù ‚Äúinstrumental,‚Äù ‚Äúchorus‚Äù\n\n\nTopic 2: Discussions\n0.449\nFan behavior analysis\nPerfect match: Focuses on ‚Äúpeople,‚Äù ‚Äúfans,‚Äù causal reasoning\n\n\nTopic 3: Rules\n0.732\nAutomated mod removal messages\nPerfect match: Identical formulaic administrative text\n\n\nTopic 4: Groups\n0.728\nIndustry achievements/metrics\nPerfect match: Chart performance, awards, debut timelines\n\n\nTopic 5: HYBE\n0.612\nLegal case updates\nPerfect match: Court proceedings, contract disputes\n\n\n\nKey Findings:\n1. Topic 3 has the highest gamma values (0.73+)\n\nWhy? Moderator messages are identical copies of template text\nVery narrow vocabulary, minimal variation\nThese cluster tightly because they‚Äôre literally the same document repeated\n\n2. Topic 2 has the lowest gamma values (0.45 max)\n\nWhy? Fan discussions are naturally multifaceted\nReal discourse blends reasoning, examples, multiple topics\nEven ‚Äúpure‚Äù discussion comments touch on multiple themes\n\n3. All topics validated with actual content\n\nTop documents perfectly match our beta-based interpretations\nWord lists (from Section 5.4) align with document themes\nNo topic needs reinterpretation\n\n4. Gamma thresholds reveal document types:\n\nGamma &gt; 0.7: Formulaic/template text (mod messages)\nGamma = 0.6-0.7: Long-form focused content (album reviews, legal updates)\nGamma = 0.4-0.5: Organic multi-topical discussions\n\nWhat makes a document ‚Äúrepresentative‚Äù?\nA high gamma value (close to 1.0) means:\n\nVocabulary concentration: Document uses words almost exclusively from one topic‚Äôs vocabulary\nThematic purity: Little mixing with other topics‚Äô themes\nPredictability: If you see high gamma, you know exactly what the document contains\n\nExpected vs.¬†Actual:\n\n\n\n\n\n\n\n\nTopic\nExpected High-Gamma Docs\nWhat We Actually Found\n\n\n\n\nTopic 1\nAlbum reviews\nTrack-by-track reviews (TWICE, TXT)\n\n\nTopic 2\nGeneral fan chats\nMeta-discussions about fandom behavior\n\n\nTopic 3\nMod messages\nIdentical automated removal notices\n\n\nTopic 4\nGroup news\nAchievement lists, Grammy requirements\n\n\nTopic 5\nHYBE news\nCourt case legal summaries\n\n\n\nWhy is this validation important?\n1. Confirms model quality:\n\nIf top documents didn‚Äôt match topic interpretations, we‚Äôd need to retrain\nPerfect alignment means K=5 captures meaningful structure\n\n2. Enables filtering:\n\nCan remove Topic 3 documents (gamma &gt; 0.7) for analysis of organic discourse\nCan identify event-specific content (Topic 5) vs.¬†structural patterns (Topics 1, 2, 4)\n\n3. Supports coding/annotation:\n\nHigh-gamma documents are perfect training examples\nCan use for manual content analysis or supervised learning\n\n4. Reveals discourse patterns:\n\nTopic 1 and 3: Highly focused (album reviews, admin messages)\nTopic 2: Naturally multifaceted (fan discussions blend themes)\nTopic 5: Event-driven (July 2024 specific)\n\nLimitations to note:\n1. Sample bias: We only looked at top 3 documents per topic\n\nMost documents have mixed topics (lower gamma)\nThese extremes show the ‚Äúideal types‚Äù but aren‚Äôt typical\n\n2. Time specificity: Topic 5 (HYBE) is July 2025-specific\n\nWouldn‚Äôt appear in data from other months/years\nShows how LDA captures temporal events alongside structural patterns\n\n3. Template text dominates: Topic 3 has artificially high gamma\n\nShould be filtered out for analysis of user-generated content\nRepresents approximately 10-15% of corpus but minimal informational content\n\nNext Steps:\nNow that we‚Äôve validated topics qualitatively, let‚Äôs examine them quantitatively:\n\nHow many documents belong primarily to each topic?\nWhat‚Äôs the distribution of dominant topics across the corpus?\nAre most documents single-topic or multi-topic?\n\n\n\n\n5.12 Distribution of Dominant Topics\nLet‚Äôs see how many documents are primarily about each topic:\n# Find the dominant topic for each document\ndominant_topics &lt;- lda_documents |&gt; \n  group_by(document) |&gt; \n  slice_max(gamma, n = 1) |&gt; \n  ungroup()\n\n# Count documents by dominant topic\ntopic_distribution &lt;- dominant_topics |&gt; \n  count(topic) |&gt; \n  mutate(percentage = n / sum(n) * 100)\n\n# Display as a formatted table\nlibrary(knitr)\nkable(topic_distribution, digits = 2, \n      col.names = c(\"Topic\", \"Number of Documents\", \"Percentage (%)\"),\n      caption = \"Distribution of Dominant Topics\")\n\n\n\nTopic\nNumber of Documents\nPercentage (%)\n\n\n\n\n1\n7861\n28.28\n\n\n2\n6286\n22.61\n\n\n3\n2865\n10.31\n\n\n4\n6049\n21.76\n\n\n5\n4736\n17.04\n\n\n\nDistribution of Dominant Topics\n\n\n\n5.12 Distribution of Dominant Topics\nLet‚Äôs see how many documents are primarily about each topic:\n# Find the dominant topic for each document\ndominant_topics &lt;- lda_documents |&gt; \n  group_by(document) |&gt; \n  slice_max(gamma, n = 1) |&gt; \n  ungroup()\n\n# Count documents by dominant topic\ntopic_distribution &lt;- dominant_topics |&gt; \n  count(topic) |&gt; \n  mutate(percentage = n / sum(n) * 100)\n\nknitr::kable(topic_distribution, \n             col.names = c(\"Topic\", \"Number of Documents\", \"Percentage (%)\"),\n             caption = \"Distribution of Dominant Topics\",\n             digits = 2)\n\n\n\nTopic\nNumber of Documents\nPercentage (%)\n\n\n\n\n1\n7861\n28.28\n\n\n2\n6286\n22.61\n\n\n3\n2865\n10.31\n\n\n4\n6049\n21.76\n\n\n5\n4736\n17.04\n\n\n\nDistribution of Dominant Topics\nResults:\n\nTopic 1 (Music Appreciation): 7,861 documents (28.28%)\nTopic 2 (Fan Discussions): 6,286 documents (22.61%)\nTopic 3 (Rules/Moderation): 2,865 documents (10.31%)\nTopic 4 (Groups/Industry): 6,049 documents (21.76%)\nTopic 5 (HYBE-ADOR Controversy): 4,736 documents (17.04%)\n\nTotal: 27,797 documents analyzed\nKey Findings:\n1. Relatively Balanced Distribution\n\nNo single topic dominates discourse (largest is only 28%)\nTopics 1, 2, 4 form the ‚Äúbig three‚Äù (72% combined)\nSuggests diverse community interests\n\n2. Music Appreciation is Most Common (28.28%)\n\nAlbum reviews, track-by-track analysis, listening experiences\nConfirms r/kpop is fundamentally about the music itself\nNearly 1 in 3 comments focuses on musical content\n\n3. Fan Discussions are Second (22.61%)\n\nMeta-commentary about fandom behavior\nCommunity analysis, fan culture discussions\nShows community is self-reflective\n\n4. Rules/Moderation is Smallest (10.31%)\n\nOnly ~3,000 automated mod messages out of 28,000 documents\nConfirms these are repetitive but not overwhelming\nSafe to filter out for organic discourse analysis\n\n5. HYBE Controversy is Substantial (17.04%)\n\nNearly 1 in 5 comments about the legal dispute\nShows July 2025 was heavily influenced by this event\nTime-specific: would likely be 0% in other months\n\n6. Groups/Industry News is Major Topic (21.76%)\n\nCharts, achievements, debuts, awards\nShows community tracks industry metrics closely\nNearly equal to fan discussions\n\nInterpretation:\nWhat this tells us about r/kpop in July 2025:\nBalanced Community Interests:\n\nUnlike some fan communities that focus solely on one aspect\nr/kpop engages with music (28%), meta-discussion (23%), industry (22%), and events (17%)\nDiverse discourse ecosystem\n\nEvent Impact:\n\nTopic 5 (HYBE) at 17% shows significant but not overwhelming event influence\n83% of discourse continues on structural topics (music, fans, groups)\nCommunity maintains baseline interests even during major controversies\n\nMusic Remains Central:\n\nDespite drama, music appreciation is #1 topic\nValidates subreddit‚Äôs mission as music-focused community\nPositive sign: not purely gossip/drama-driven\n\n# Visualize the distribution\nggplot(topic_distribution, aes(x = factor(topic), y = n, fill = factor(topic))) +\n  geom_col() +\n  geom_text(aes(label = paste0(format(n, big.mark = \",\"), \"\\n(\", \n                                round(percentage, 1), \"%)\")), \n            vjust = -0.5, size = 4, fontface = \"bold\") +\n  scale_fill_manual(\n    values = c(\"1\" = \"coral\", \"2\" = \"darkgoldenrod3\", \"3\" = \"darkseagreen\", \n               \"4\" = \"steelblue\", \"5\" = \"orchid3\"),\n    labels = c(\"1: Music\", \"2: Discussions\", \"3: Rules\", \"4: Groups\", \"5: HYBE\")\n  ) +\n  labs(\n    title = \"Distribution of Dominant Topics Across All Documents\",\n    subtitle = \"Each document assigned to topic with highest gamma value (July 2025)\",\n    x = \"Topic\",\n    y = \"Number of Documents\",\n    fill = \"Topic\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_y_continuous(labels = scales::comma, \n                     expand = expansion(mult = c(0, 0.15)))\n\nResearch Questions Answered:\n1. What do K-pop fans talk about most?\n\nAnswer: Music appreciation (28%), but closely followed by fan discussions (23%) and industry news (22%)\nNo single obsession - balanced engagement\n\n2. Is the HYBE controversy dominating discourse?\n\nAnswer: No.¬†At 17%, it‚Äôs substantial but not dominant\n5 out of 6 comments are about other topics\nShows community resilience to event-driven discourse\n\n3. How much is meta-discussion vs.¬†content discussion?\n\nMeta (Rules): 10.31%\nContent (Music + Groups + Events): 67.08%\nCommunity/Fans: 22.61%\nClear preference for content over administration\n\n4. Are discussions focused or diffuse?\n\nThis shows forced single-topic assignment\nRemember: most documents are mixed (low gamma values from Section 5.8)\nThese percentages show plurality, not purity\n\nImportant Caveat:\nThis analysis assigns each document to its dominant topic, but:\n# How many documents have a CLEAR dominant topic (gamma &gt; 0.5)?\nclear_dominance &lt;- dominant_topics |&gt; \n  filter(gamma &gt; 0.5) |&gt; \n  nrow()\n\ntotal_docs &lt;- nrow(dominant_topics)\n\nmixed_topics &lt;- total_docs - clear_dominance\n\ncat(\"Documents with clear dominant topic (gamma &gt; 0.5):\", \n    format(clear_dominance, big.mark = \",\"), \n    paste0(\"(\", round(clear_dominance/total_docs * 100, 1), \"%)\"), \"\\n\")\n## Documents with clear dominant topic (gamma &gt; 0.5): 462 (1.7%)\ncat(\"Documents with mixed topics (gamma &lt; 0.5):\", \n    format(mixed_topics, big.mark = \",\"),\n    paste0(\"(\", round(mixed_topics/total_docs * 100, 1), \"%)\"), \"\\n\")\n## Documents with mixed topics (gamma &lt; 0.5): 27,335 (98.3%)\nWhat this means:\n\nA document with gamma = [0.25, 0.24, 0.21, 0.18, 0.12] gets assigned to Topic 1\nBut it‚Äôs really balanced across all topics (only 25% Topic 1)\nThe gamma &gt; 0.5 analysis shows how many are truly single-topic\n\nClass exercise\nRedo the graph and the table for gamma &gt; 0.5 and compare the difference\n\n\n\n5.13 Topics Over Time\nNow let‚Äôs see how topics change throughout July 2025 using average gamma values (not just dominant topics):\n# First, let's check the date range in our data\ndate_range &lt;- data3 |&gt; \n  summarize(\n    start_date = min(date),\n    end_date = max(date),\n    total_days = as.numeric(difftime(max(date), min(date), units = \"days\"))\n  )\n\ncat(\"Date Range:\", format(date_range$start_date, \"%B %d, %Y\"), \n    \"to\", format(date_range$end_date, \"%B %d, %Y\"), \"\\n\")\n## Date Range: July 01, 2025 to July 31, 2025\ncat(\"Total Days:\", date_range$total_days, \"\\n\")\n## Total Days: 30\nResults:\n\nStart Date: July 1, 2025\nEnd Date: July 31, 2025\nTotal Days: 30 days (complete month)\n\nPrepare temporal data with ALL topics:\n# Join ALL topic-document probabilities with date information\ntopics_over_time &lt;- lda_documents |&gt; \n  mutate(document = as.integer(document)) |&gt; \n  left_join(\n    data3 |&gt; select(comment_index, date),\n    by = c(\"document\" = \"comment_index\")\n  ) \n\n# Check for any missing dates\nmissing_dates &lt;- topics_over_time |&gt; \n  filter(is.na(date)) |&gt; \n  nrow()\n\ncat(\"Documents with missing dates:\", missing_dates, \"\\n\")\n## Documents with missing dates: 0\ncat(\"Total topic-document pairs:\", nrow(topics_over_time), \"\\n\")\n## Total topic-document pairs: 111580\ncat(\"Unique documents:\", length(unique(topics_over_time$document)), \"\\n\")\n## Unique documents: 22316\nImportant difference from Section 5.12:\n\nSection 5.12: Counted documents by dominant topic (forced single assignment)\nSection 5.13: Uses average gamma across ALL documents each day\nWhy better: Captures mixed-topic nature and true topic prevalence\n\nDaily Average Gamma by Topic:\n# Calculate daily average gamma for each topic\ndaily_topic_averages &lt;- topics_over_time |&gt; \n  group_by(date, topic) |&gt; \n  summarize(\n    mean_gamma = mean(gamma),\n    median_gamma = median(gamma),\n    n_docs = n_distinct(document),\n    .groups = \"drop\"\n  )\n\n# Plot daily average gamma\nggplot(daily_topic_averages, aes(x = date, y = mean_gamma, color = factor(topic))) +\n  geom_line(size = 1) +\n  geom_point(size = 2, alpha = 0.6) +\n  scale_color_manual(\n    values = c(\"1\" = \"coral\", \"2\" = \"darkgoldenrod3\", \"3\" = \"darkseagreen\", \n               \"4\" = \"steelblue\", \"5\" = \"orchid3\"),\n    labels = c(\"1: Music\", \"2: Discussions\", \"3: Rules\", \"4: Groups\", \"5: HYBE\")\n  ) +\n  scale_x_date(date_breaks = \"3 days\", date_labels = \"%b %d\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    title = \"Daily Average Topic Prevalence (July 2025)\",\n    subtitle = \"Mean gamma (topic probability) across all documents each day\",\n    x = \"Date\",\n    y = \"Average Topic Probability (Gamma)\",\n    color = \"Topic\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"bottom\"\n  )\n\nWhat this shows:\n\nY-axis: Average probability that a random comment on that day belongs to each topic\nHigher line: Topic is more prevalent in discourse that day\nLines sum to 1.0: On any given day, all topic probabilities add to 100%\nChanges over time: Shows how topic prevalence shifts through the month\n\nSmoothed Trends (LOESS):\n# Smooth trends with loess\nggplot(daily_topic_averages, aes(x = date, y = mean_gamma, color = factor(topic))) +\n  geom_smooth(method = \"loess\", se = TRUE, size = 1.2, span = 0.3, alpha = 0.2) +\n  scale_color_manual(\n    values = c(\"1\" = \"coral\", \"2\" = \"darkgoldenrod3\", \"3\" = \"darkseagreen\", \n               \"4\" = \"steelblue\", \"5\" = \"orchid3\"),\n    labels = c(\"1: Music\", \"2: Discussions\", \"3: Rules\", \"4: Groups\", \"5: HYBE\")\n  ) +\n  scale_x_date(date_breaks = \"3 days\", date_labels = \"%b %d\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    title = \"Smoothed Topic Prevalence Trends (July 2025)\",\n    subtitle = \"LOESS smoothing (span=0.3) shows underlying patterns in average gamma\",\n    x = \"Date\",\n    y = \"Average Topic Probability (Gamma)\",\n    color = \"Topic\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"bottom\"\n  )\n\nWhy smoothing matters:\n\nDaily data has noise (random variation in comment mix)\nLOESS reveals the trend beneath the noise\nConfidence bands show uncertainty in the trend\nEasier to see if topics are rising, falling, or stable\n\nIndividual Topic Trends (Faceted View):\n# Faceted view for each topic\nggplot(daily_topic_averages, aes(x = date, y = mean_gamma)) +\n  geom_line(size = 1, color = \"steelblue\") +\n  geom_point(size = 2, color = \"steelblue\", alpha = 0.6) +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"red\", size = 0.8, \n              span = 0.3, alpha = 0.2) +\n  facet_wrap(~ topic, ncol = 1, scales = \"free_y\",\n             labeller = labeller(topic = c(\n               \"1\" = \"Topic 1: Music Appreciation\",\n               \"2\" = \"Topic 2: Fan Discussions\",\n               \"3\" = \"Topic 3: Rules/Moderation\",\n               \"4\" = \"Topic 4: Groups/Industry\",\n               \"5\" = \"Topic 5: HYBE-ADOR Controversy\"\n             ))) +\n  scale_x_date(date_breaks = \"3 days\", date_labels = \"%b %d\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    title = \"Individual Topic Trends Over Time (July 2025)\",\n    subtitle = \"Daily average gamma with smoothed trend lines (red)\",\n    x = \"Date\",\n    y = \"Average Topic Probability (Gamma)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    strip.text = element_text(face = \"bold\", size = 11)\n  )\n\nAnalyze each panel:\n\nTopic 1 (Music): Look for spikes on album release dates\nTopic 2 (Discussions): Should be relatively stable (baseline chatter)\nTopic 3 (Rules): Tracks moderation activity intensity\nTopic 4 (Groups): Event-driven (awards, charts, debuts)\nTopic 5 (HYBE): Should spike on court dates and major announcements\n\nDay of Week Patterns:\n# Add day of week\ntopics_by_weekday &lt;- topics_over_time |&gt; \n  mutate(weekday = weekdays(date)) |&gt; \n  group_by(weekday, topic) |&gt; \n  summarize(mean_gamma = mean(gamma), .groups = \"drop\") |&gt; \n  mutate(weekday = factor(weekday, \n                         levels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \n                                   \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\")))\n\nggplot(topics_by_weekday, aes(x = weekday, y = mean_gamma, fill = factor(topic))) +\n  geom_col(position = \"fill\") +\n  scale_fill_manual(\n    values = c(\"1\" = \"coral\", \"2\" = \"darkgoldenrod3\", \"3\" = \"darkseagreen\", \n               \"4\" = \"steelblue\", \"5\" = \"orchid3\"),\n    labels = c(\"1: Music\", \"2: Discussions\", \"3: Rules\", \"4: Groups\", \"5: HYBE\")\n  ) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    title = \"Topic Prevalence by Day of Week\",\n    subtitle = \"Average gamma (stacked to 100%) by day\",\n    x = \"Day of Week\",\n    y = \"Average Topic Probability\",\n    fill = \"Topic\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"bottom\"\n  )\n\nResearch Questions:\n\nDo music discussions increase on weekends (listening time)?\nDoes moderation intensity vary by day?\nAre industry announcements timed to specific days?\nIs controversy discussion consistent across the week?\n\nWeekend vs.¬†Weekday Analysis:\nInterpretation:\n\nPositive difference: Topic probability increases on weekends\nNegative difference: Topic probability decreases on weekends\n% Change: Magnitude of weekend effect\n\n# Categorize weekend vs weekday\ntopics_weekend &lt;- topics_over_time |&gt; \n  mutate(\n    weekday = weekdays(date),\n    is_weekend = weekday %in% c(\"Saturday\", \"Sunday\")\n  ) |&gt; \n  group_by(is_weekend, topic) |&gt; \n  summarize(mean_gamma = mean(gamma), .groups = \"drop\")\n\n# Calculate difference\nweekend_effect &lt;- topics_weekend |&gt; \n  pivot_wider(names_from = is_weekend, values_from = mean_gamma, \n              names_prefix = \"weekend_\") |&gt; \n  mutate(\n    difference = weekend_TRUE - weekend_FALSE,\n    pct_change = (difference / weekend_FALSE) * 100\n  ) |&gt; \n  arrange(desc(abs(difference)))\n\nknitr::kable(weekend_effect, \n             digits = 4,\n             col.names = c(\"Topic\", \"Weekday Gamma\", \"Weekend Gamma\", \n                          \"Difference\", \"% Change\"),\n             caption = \"Weekend Effect on Topic Prevalence\")\n\n\n\nTopic\nWeekday Gamma\nWeekend Gamma\nDifference\n% Change\n\n\n\n\n3\n0.1934\n0.2030\n0.0095\n4.9179\n\n\n1\n0.2034\n0.1976\n-0.0058\n-2.8552\n\n\n5\n0.2004\n0.1983\n-0.0021\n-1.0606\n\n\n2\n0.2019\n0.2008\n-0.0011\n-0.5343\n\n\n4\n0.2009\n0.2004\n-0.0005\n-0.2501\n\n\n\nWeekend Effect on Topic Prevalence\nBased on the weekend vs.¬†weekday analysis, we observe minimal but meaningful differences:\nCorrelation Analysis Between Topics:\n\nPositive correlation (red): Topics tend to increase together\n\nMight indicate general activity spikes (more comments = more of everything)\n\nNegative correlation (blue): Topics compete for attention\n\nWhen one increases, the other decreases\nSuggests zero-sum competition for discourse space\n\nNear-zero correlation (white): Topics vary independently\n\n# Check if topics compete or co-occur\ntopic_correlations &lt;- daily_topic_averages |&gt; \n  select(date, topic, mean_gamma) |&gt; \n  pivot_wider(names_from = topic, values_from = mean_gamma, \n              names_prefix = \"topic_\") |&gt; \n  select(-date) |&gt; \n  cor()\n\n# Display as heatmap\nlibrary(reshape2)\ncor_melted &lt;- melt(topic_correlations)\n\nggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), color = \"white\", size = 4) +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", \n                       midpoint = 0, limits = c(-1, 1)) +\n  labs(\n    title = \"Topic Correlation Matrix\",\n    subtitle = \"Do topics rise and fall together over time?\",\n    x = \"Topic\",\n    y = \"Topic\",\n    fill = \"Correlation\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nInterpretation of Correlation Results:\nBased on the correlation matrix, we can identify three distinct patterns:\n1. Strong Competition: Controversy vs.¬†Content (-0.85 and -0.77)\n\nTopic 1 (Music) ‚ÜîÔ∏é Topic 5 (HYBE): r = -0.85 (very strong negative)\nTopic 4 (Groups) ‚ÜîÔ∏é Topic 5 (HYBE): r = -0.77 (strong negative)\n\nMeaning: When HYBE controversy discussion increases, music appreciation and group/industry discussion decrease proportionally\nInterpretation:\n\nZero-sum attention economy: controversy ‚Äúcrowds out‚Äù content discussion\nCommunity has fixed attention capacity\nDrama displaces substantive music/industry talk\nSuggests controversy is disruptive to normal discourse patterns\n\n2. Positive Co-occurrence: Content Topics (0.47)\n\nTopic 1 (Music) ‚ÜîÔ∏é Topic 4 (Groups): r = 0.47 (moderate positive)\n\nMeaning: When music appreciation increases, group/industry discussion also increases\nInterpretation:\n\nBoth are content-focused topics\nLikely rise together during comeback periods (new music + charts/achievements)\nHigh-activity days see more of both\nSuggests these are complementary rather than competitive\n\n3. Moderate Competition: Meta-Discussion (-0.59)\n\nTopic 2 (Discussions) ‚ÜîÔ∏é Topic 3 (Rules): r = -0.59 (moderate negative)\n\nMeaning: When fan meta-discussions increase, moderation messages decrease\nInterpretation:\n\nWhen community is actively discussing (Topic 2), fewer posts are removed\nOr: Heavy moderation (Topic 3) suppresses meta-discussion\nPossible explanation: controversial meta-discussions trigger more removals\n\n4. Weak/Neutral Relationships:\n\nTopic 2 (Discussions) ‚ÜîÔ∏é Topic 5 (HYBE): r = 0.30 (weak positive)\n\nFan discussions incorporate controversy but aren‚Äôt dominated by it\n\nTopic 1 (Music) ‚ÜîÔ∏é Topic 2 (Discussions): r = -0.37 (weak negative)\n\nSlight trade-off between music appreciation and fan meta-talk\n\nTopic 3 (Rules) ‚ÜîÔ∏é Topic 4 (Groups): r = 0.22 (weak positive)\n\nIndustry news posts sometimes trigger moderation\n\nTopic 2 (Discussions) ‚ÜîÔ∏é Topic 4 (Groups): r = -0.13 (very weak)\n\nNearly independent\n\n\nKey Research Findings:\n1. Attention is Zero-Sum for Events\n\nThe -0.85 correlation between Music and HYBE is extremely strong\nSuggests community attention is a fixed pie\nControversy doesn‚Äôt expand discussion, it displaces other topics\nWhen HYBE controversy peaked, music discussion was at its lowest\n\n2. Content Topics are Complementary\n\nThe +0.47 correlation between Music and Groups shows they co-occur\nActive music discussion days = active industry discussion days\nSuggests these topics feed each other (new releases ‚Üí chart performance)\nNon-zero-sum: both can increase simultaneously\n\n3. Moderation Reflects Discussion Patterns\n\nTopic 3 (Rules) negatively correlates with both Discussion (-0.59) and Controversy (-0.51)\nSuggests moderation is reactive to problematic content\nWhen controversy or meta-discussion surge, moderation increases\n\n4. Implications for Community Health:\nPositive Sign: - Music and Groups topics are positively correlated (content-focused community)\nConcerning Sign:\n\nStrong negative correlation with controversy suggests it disrupts normal patterns\nCommunity loses content-focused discussion during drama periods\nAt r = -0.85, this is nearly a perfect trade-off\n\nStatistical Note:\nThese are Pearson correlations based on only 30 data points (days in July), so:\n\nCorrelations &gt; 0.36 are statistically significant (p &lt; 0.05)\nCorrelations &gt; 0.46 are statistically significant (p &lt; 0.01)\n\nSignificant correlations in our data:\n\nTopic 1 ‚ÜîÔ∏é Topic 5: -0.85 (p &lt; 0.001)\nTopic 4 ‚ÜîÔ∏é Topic 5: -0.77 (p &lt; 0.001)\nTopic 2 ‚ÜîÔ∏é Topic 3: -0.59 (p &lt; 0.001)\nTopic 1 ‚ÜîÔ∏é Topic 4: 0.47 (p &lt; 0.01)\nTopic 1 ‚ÜîÔ∏é Topic 2: -0.37 (p &lt; 0.05)\n\nAll major patterns are statistically significant and not due to chance."
  },
  {
    "objectID": "Spring2026/LectureSlides/bigdata_L9-github.html#lecture-9-cheat-sheet-topic-modeling-lda",
    "href": "Spring2026/LectureSlides/bigdata_L9-github.html#lecture-9-cheat-sheet-topic-modeling-lda",
    "title": "Text as Data: Topic Modeling",
    "section": "Lecture 9 Cheat Sheet: Topic Modeling (LDA)",
    "text": "Lecture 9 Cheat Sheet: Topic Modeling (LDA)\n\n\n\n\n\n\n\n\nFunction/Concept\nDescription\nCode Example\n\n\n\n\nLDA Concept\nUnsupervised ML technique that discovers latent topics. Each document = mixture of topics; each topic = mixture of words.\nTwo key principles:1. Documents contain multiple topics2. Topics contain multiple words\n\n\nDocument-Term Matrix (DTM)\nRows = documents, columns = words, values = counts. Sparse matrix (mostly zeros).\ncast_dtm(comment_index, word, n)\n\n\nSparsity\nPercentage of matrix that is zeros. Text data is naturally 95%+ sparse.\ntidy_dfm@Dim shows dimensions\n\n\nLanguage Detection\nIdentify language of text using Google‚Äôs CLD2 package.\ncld2::detect_language(data$text)\n\n\nRemove Duplicates\nKeep only unique texts to avoid bias from repeated content.\ndistinct(text, .keep_all = TRUE)\n\n\nCustom Stopwords\nCombine multiple stopword sources + domain-specific terms.\nc(stopwords(\"en\"), stopwords(source = \"smart\"), \"custom\")\n\n\nTF-IDF Filtering\nRemove words that are too common (appear in &gt;75% of docs) or too rare (&lt;0.1% of docs).\ndoc_freq = n() / templengthfilter(doc_freq &lt; maxndoc & doc_freq &gt; minndoc)\n\n\nTopic Coherence\nMeasures how often top words in a topic co-occur. Higher (less negative) = better.\ntopic_coherence(lda_model, dtm) from topicdoc package\n\n\nC_UMass Coherence\nDocument co-occurrence based metric. Values are negative; closer to 0 is better.\nMost common metric for topic quality\n\n\nFindTopicsNumber()\nTests multiple K values with different coherence metrics (CaoJuan, Deveaud).\nFindTopicsNumber(dtm, topics = seq(2, 20, 1), metrics = c(\"CaoJuan2009\"))\n\n\nCaoJuan2009\nMinimize this metric. Lower = topics are more distinct.\nShows steady decline, minimum = optimal K\n\n\nDeveaud2014\nMaximize this metric. Higher = better topic divergence. Often noisy.\nMore variable than CaoJuan\n\n\nChoosing K\nBalance coherence (quality) with interpretability. Look for ‚Äúelbow‚Äù in coherence plot.\nK=5 had best coherence (-160.46) in our example\n\n\nLDA()\nTrain LDA model using Gibbs sampling method.\nLDA(dtm, k = 5, method = \"Gibbs\", control = list(seed = 42, iter = 2000))\n\n\nLDA Parameters\nk: number of topicsiter: iterations to runburnin: discard first N iterationsseed: reproducibility\ncontrol = list(verbose = 500, seed = 42, iter = 2000, burnin = 500)\n\n\nBeta (Œ≤) Matrix\nTopic-word probabilities. Dimensions: Topics √ó Words (5 √ó 2,683). Shows which words define each topic.\ntidy(lda_model, matrix = \"beta\")\n\n\nGamma (Œ≥) Matrix\nDocument-topic probabilities. Dimensions: Documents √ó Topics (22,316 √ó 5). Shows which topics appear in each document.\ntidy(lda_model, matrix = \"gamma\")\n\n\nslice_max()\nGet top N rows by a value. Used to find top words per topic.\ngroup_by(topic) |&gt; slice_max(beta, n = 10)\n\n\nreorder_within()\nReorder terms within each facet for better visualization.\nmutate(term = reorder_within(term, beta, topic))\n\n\nscale_y_reordered()\nScale y-axis for faceted plots with reorder_within().\nPairs with reorder_within() in faceted plots\n\n\nLog Ratio Comparison\nlog2(topic2_beta / topic1_beta). Positive = more associated with topic2; negative = more associated with topic1.\nmutate(log_ratio = log2(topic2 / topic1))\n\n\nInterpreting Beta\nHigher beta = word is more important to that topic. Compare relative values across topics, not absolute.\nŒ≤ = 0.043 for ‚Äúsong‚Äù in Topic 1 is very high\n\n\nInterpreting Gamma\nGamma values sum to 1.0 per document. Gamma &gt; 0.5 = clear dominant topic; gamma &lt; 0.5 = mixed topics.\nDocument with [0.25, 0.24, 0.21, 0.18, 0.12] is mixed\n\n\nDominant Topic\nTopic with highest gamma for a document. Useful for categorizing documents.\ngroup_by(document) |&gt; slice_max(gamma, n = 1)\n\n\nRepresentative Documents\nDocuments with highest gamma for each topic. Best examples of ‚Äúpure‚Äù topics.\nGamma &gt; 0.7 = very pure (e.g., template text)\n\n\nTopic Distribution\nCount how many documents belong primarily to each topic.\ncount(topic) |&gt; mutate(percentage = n/sum(n)*100)\n\n\nTemporal Analysis\nTrack how topic prevalence changes over time using average gamma by date.\ngroup_by(date, topic) |&gt; summarize(mean_gamma = mean(gamma))\n\n\nLOESS Smoothing\nSmooth noisy time series to reveal underlying trends.\ngeom_smooth(method = \"loess\", span = 0.3)\n\n\nWeekend Effect\nCompare topic prevalence on weekends vs.¬†weekdays.\nmutate(is_weekend = weekday %in% c(\"Saturday\", \"Sunday\"))\n\n\nTopic Correlation\nCorrelation between topics‚Äô daily prevalence. Positive = co-occur; negative = compete.\npivot_wider(names_from = topic) |&gt; cor()\n\n\nZero-Sum Attention\nStrong negative correlation (r &lt; -0.7) means topics compete for attention.\nTopic 1 ‚ÜîÔ∏é Topic 5: r = -0.85 (very strong competition)\n\n\nComplementary Topics\nPositive correlation (r &gt; 0.4) means topics rise together.\nTopic 1 ‚ÜîÔ∏é Topic 4: r = 0.47 (content topics co-occur)\n\n\nValidation Strategy\nCheck if high-gamma documents match beta-based topic interpretations.\nExamine top 3 docs per topic for thematic consistency\n\n\nTemplate Detection\nVery high gamma (&gt;0.7) often indicates formulaic/template text (mod messages).\nCan filter out for organic discourse analysis\n\n\nData Loss Tracking\nTrack documents removed at each preprocessing step.\nOriginal ‚Üí Language filter ‚Üí Deduplication ‚Üí TF-IDF filter\n\n\nMemory Management\nRemove unused objects and run garbage collection for large models.\nrm(list = setdiff(ls(), keep_objects))gc()\n\n\nSave Workspace\nSave entire workspace or specific objects for later use.\nsave.image(\"file.RData\")save(lda_model, file = \"model.RData\")\n\n\nInterpretation Principle\nTopics are probabilistic and mixed. No document is purely one topic. Focus on relative probabilities.\nMost documents have gamma &lt; 0.5 for dominant topic\n\n\nQuality Indicators\n‚úÖ High coherence‚úÖ Distinct topics (minimal word overlap)‚úÖ Interpretable themes‚úÖ Representative docs match beta words\nK=5 achieved all quality criteria in example\n\n\nCommon Pitfall\nInterpreting gamma as hard categories. Documents are mixtures!\nDocument [0.28, 0.23, 0.21, 0.18, 0.10] isn‚Äôt ‚ÄúTopic 1‚Äù\n\n\nResearch Applications\nContent categorization, trend analysis, event detection, community discourse structure, attention dynamics.\nUsed to answer ‚ÄúWhat do fans discuss?‚Äù and ‚ÄúHow does discourse shift over time?‚Äù"
  },
  {
    "objectID": "install.html",
    "href": "install.html",
    "title": "How to Install R Studio",
    "section": "",
    "text": "It is very important to install in order: first R, then R Studio\n\nInstall R from the download link. You can pick which mirror to choose. For example, Duke Mirror: https://archive.linux.duke.edu/cran/\nInstall R Studio (free desktop version) AFTER you installed R, from the link. You will only use R Studio but need to have both on your computer.",
    "crumbs": [
      "Home",
      "Getting Started",
      "How to Install R Studio"
    ]
  },
  {
    "objectID": "install.html#installation-steps",
    "href": "install.html#installation-steps",
    "title": "How to Install R Studio",
    "section": "",
    "text": "It is very important to install in order: first R, then R Studio\n\nInstall R from the download link. You can pick which mirror to choose. For example, Duke Mirror: https://archive.linux.duke.edu/cran/\nInstall R Studio (free desktop version) AFTER you installed R, from the link. You will only use R Studio but need to have both on your computer.",
    "crumbs": [
      "Home",
      "Getting Started",
      "How to Install R Studio"
    ]
  },
  {
    "objectID": "install.html#how-to-use-r-studio",
    "href": "install.html#how-to-use-r-studio",
    "title": "How to Install R Studio",
    "section": "How to Use R Studio",
    "text": "How to Use R Studio\n\nStep 1: Open R Studio from applications folder\n\n\n\nStep 2: Create a new project\n\n\n\nStep 3: Click on ‚ÄúFile/New File/R script‚Äù\n\n\n\nStep 4: Running commands\nCommands in R Studio can be written directly into the console area (bottom left), or in the script. To run a completed command press Control + Enter (Windows/Linux) or Command + Enter (Mac).\n\n\nStep 5: Install Required Packages\nType the following code in the script and run it:\ninstall.packages(\"tidyverse\")    # data wrangling\ninstall.packages(\"tidyr\")        # data wrangling\ninstall.packages(\"tidytext\")     # text analysis\ninstall.packages(\"dplyr\")        # data wrangling\ninstall.packages(\"tm\")           # topic modeling\ninstall.packages(\"topicmodels\")  # topic modeling\ninstall.packages(\"quanteda\")     # text analysis\ninstall.packages(\"lubridate\")    # dates\ninstall.packages(\"ggplot2\")      # visualizations\ninstall.packages(\"ggthemes\")     # visualizations\ninstall.packages(\"scales\")       # visualizations\ninstall.packages(\"wesanderson\")  # visualizations\n\n\nStep 6: Load Packages into Library\nAfter installation, load the packages before using them:\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(tm)\nlibrary(topicmodels)\nlibrary(quanteda)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(scales)\nlibrary(wesanderson)\n\n\nStep 7: Getting Help\nFor help with any command, type ? followed by the command name and run:\n?merge\n?ggplot\n?install.packages\nFor keyboard shortcuts in R Studio, see the official documentation.\nYou‚Äôre Ready!\nNow you are ready to work in R Studio! If you have any issues with installation, please email the instructor or TA, or come to office hours.",
    "crumbs": [
      "Home",
      "Getting Started",
      "How to Install R Studio"
    ]
  },
  {
    "objectID": "install.html#troubleshooting",
    "href": "install.html#troubleshooting",
    "title": "How to Install R Studio",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nProblem: Package installation fails\n\nMake sure you‚Äôre connected to the internet\nTry selecting a different CRAN mirror\nCheck if you have write permissions\n\nProblem: R Studio won‚Äôt open\n\nMake sure R is installed first\nTry reinstalling R Studio\nCheck system requirements\n\nProblem: Can‚Äôt load a package\n\nMake sure the package is installed first using install.packages()\nCheck the package name spelling\nTry reinstalling the package",
    "crumbs": [
      "Home",
      "Getting Started",
      "How to Install R Studio"
    ]
  },
  {
    "objectID": "install.html#additional-resources",
    "href": "install.html#additional-resources",
    "title": "How to Install R Studio",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nRStudio Cheatsheets\nStack Overflow R Questions",
    "crumbs": [
      "Home",
      "Getting Started",
      "How to Install R Studio"
    ]
  },
  {
    "objectID": "resources/datasets.html",
    "href": "resources/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "List of datasets used in this course.",
    "crumbs": [
      "Home",
      "Resources",
      "Datasets"
    ]
  },
  {
    "objectID": "resources/datasets.html#course-datasets",
    "href": "resources/datasets.html#course-datasets",
    "title": "Datasets",
    "section": "",
    "text": "List of datasets used in this course.",
    "crumbs": [
      "Home",
      "Resources",
      "Datasets"
    ]
  },
  {
    "objectID": "resources/datasets.html#where-to-find-data",
    "href": "resources/datasets.html#where-to-find-data",
    "title": "Datasets",
    "section": "Where to Find Data",
    "text": "Where to Find Data\nResources for finding data for your projects.",
    "crumbs": [
      "Home",
      "Resources",
      "Datasets"
    ]
  },
  {
    "objectID": "schedule_phd.html",
    "href": "schedule_phd.html",
    "title": "Course Schedule - PhD",
    "section": "",
    "text": "Session\nDate\nTopic\nAssignment\nReadings\n\n\n\n\n1\nJan 21\nWhy Computational Social Science?\n\nView\n\n\n2\nJan 28\nBig Data: Data Collection and Wrangling\n\nView\n\n\n3\nFeb 4\nComputational Research\nBenchmark 1 Due\nView\n\n\n4\nFeb 11\nData Cleaning & Wrangling, GGPlot\n\nView\n\n\n5\nFeb 18\nStatistics\n\nView\n\n\n6\nFeb 25\nOptional: AI API, Images\n\n\n\n\n7\nMar 4\nText as Data: Dictionary Methods\nBenchmark 2 Due\nView\n\n\n8\nMar 18\nNo Class\n\n\n\n\n9\nMar 25\nText as Data: Topic Modeling\n\nView\n\n\n10\nApr 1\nUnsupervised Machine Learning\n\nView\n\n\n11\nApr 8\nNetworks\n\nView\n\n\n12\nApr 15\nOnline Social Movements\n\nView\n\n\n13\nApr 29\nFrom Data to Conclusions\n\nView\n\n\n\nMay 4\n\nFinal Paper Due",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - PhD"
    ]
  },
  {
    "objectID": "schedule_phd.html#mondays-230-515-pm",
    "href": "schedule_phd.html#mondays-230-515-pm",
    "title": "Course Schedule - PhD",
    "section": "",
    "text": "Session\nDate\nTopic\nAssignment\nReadings\n\n\n\n\n1\nJan 21\nWhy Computational Social Science?\n\nView\n\n\n2\nJan 28\nBig Data: Data Collection and Wrangling\n\nView\n\n\n3\nFeb 4\nComputational Research\nBenchmark 1 Due\nView\n\n\n4\nFeb 11\nData Cleaning & Wrangling, GGPlot\n\nView\n\n\n5\nFeb 18\nStatistics\n\nView\n\n\n6\nFeb 25\nOptional: AI API, Images\n\n\n\n\n7\nMar 4\nText as Data: Dictionary Methods\nBenchmark 2 Due\nView\n\n\n8\nMar 18\nNo Class\n\n\n\n\n9\nMar 25\nText as Data: Topic Modeling\n\nView\n\n\n10\nApr 1\nUnsupervised Machine Learning\n\nView\n\n\n11\nApr 8\nNetworks\n\nView\n\n\n12\nApr 15\nOnline Social Movements\n\nView\n\n\n13\nApr 29\nFrom Data to Conclusions\n\nView\n\n\n\nMay 4\n\nFinal Paper Due",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - PhD"
    ]
  },
  {
    "objectID": "schedule_phd.html#required-readings",
    "href": "schedule_phd.html#required-readings",
    "title": "Course Schedule - PhD",
    "section": "Required Readings",
    "text": "Required Readings\n\nSession 1: Why Computational Social Science?\n\nHofman, J. M., Watts, D. J., Athey, S., Garip, F., Griffiths, T. L., Kleinberg, J., Margetts, H., Mullainathan, S., Salganik, M. J., Vazire, S., Vespignani, A., & Yarkoni, T. (2021). Integrating explanation and prediction in computational social science. Nature, 595(7866), 181-188. https://doi.org/10.1038/s41586-021-03659-0\nLazer, D. M. J., Pentland, A., Watts, D. J., Aral, S., Athey, S., Contractor, N., Freelon, D., Gonzalez-Bailon, S., King, G., Margetts, H., Nelson, A., Salganik, M. J., Strohmaier, M., Vespignani, A., & Wagner, C. (2020). Computational social science: Obstacles and opportunities. Science, 369(6507), 1060-1062. https://doi.org/10.1126/science.aaz8170\nPeng, Y., Lu, Y., & Shen, C. (2023). An Agenda for Studying Credibility Perceptions of Visual Misinformation. Political Communication, 40(2), 225-237. https://doi.org/10.1080/10584609.2023.2175398\nPetchler, R., & Gonz√°lez-Bail√≥n, S. (2015). Automated content analysis of online political communication. In S. Coleman & D. Freelon (Eds.), Handbook of Digital Politics (pp.¬†433-450). Edward Elgar Publishing. https://doi.org/10.4337/9781782548768\nWickham, H., √áetinkaya-Rundel, M., & Grolemund, G. (2023). R for Data Science - Whole Section. Available at: https://r4ds.hadley.nz/\n\nBack to Schedule\n\n\n\nSession 2: Big Data - Data Collection and Wrangling\n\nCappella, J. N. (2017). Vectors into the Future of Mass and Interpersonal Communication Research: Big Data, Social Media, and Computational Social Science. Human Communication Research, 43(4), 545-558. https://doi.org/10.1111/hcre.12114\nFreelon, D. (2018). Computational Research in the Post-API Age. Political Communication, 35(4), 665-668. https://doi.org/10.1080/10584609.2018.1477506\nHealy, K. (2018). Data Visualization: A Practical Introduction - Chapter 2: Make a Plot.\nTheocharis, Y., & Jungherr, A. (2021). Computational Social Science and the Study of Political Communication. Political Communication, 38(1-2), 1-22. https://doi.org/10.1080/10584609.2020.1833121\nWickham, H., √áetinkaya-Rundel, M., & Grolemund, G. (2023). R for Data Science - Section: Visualize. Available at: https://r4ds.hadley.nz/\n\nBack to Schedule\n\n\n\nSession 3: Computational Research\n\nPeng, R. D. (2022). R Programming for Data Science - Chapters 13 & 14. Available at: https://bookdown.org/rdpeng/rprogdatascience/functions.html\nWickham, H., √áetinkaya-Rundel, M., & Grolemund, G. (2023). R for Data Science - Section: Transform. Available at: https://r4ds.hadley.nz/\n\nBack to Schedule\n\n\n\nSession 4: Data Cleaning & Wrangling\n\nThulin, M. (2024). Modern Statistics with R - Chapter 5: Dealing with messy data. Available at: https://www.modernstatisticswithr.com/messychapter.html\n\nBack to Schedule\n\n\n\nSession 5: Statistics\n\nCernat, A. (2014). Introduction to Regression Models. In Longitudinal Data Analysis Using R.\nHealy, K. (2018). Data Visualization: A Practical Introduction - Section 6: Work with models.\nThulin, M. (2024). Modern Statistics with R - Chapter 8: Regression models. Available at: https://www.modernstatisticswithr.com/\n\nBack to Schedule\n\n\n\nSession 7: Text as Data - Dictionary Methods\n\nCarley, K. (1994). Extracting culture through textual analysis. Poetics, 22(4), 291-312. https://doi.org/10.1016/0304-422X(94)90011-6\nFraser, K. C., Zeller, F., Smith, D. H., Mohammad, S., & Rudzicz, F. (2019). How do we feel when a robot dies? Emotions expressed on Twitter before and after hitch. Proceedings of the Tenth Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, 62-71. https://doi.org/10.18653/v1/W19-1308\nHart, R. P. (2013). The Rhetoric of Political Comedy: A Tragedy?\nMora, M., Dupas de Matos, A., Fern√°ndez-Ruiz, V., Briz, T., & Chaya, C. (2020). Comparison of methods to develop an emotional lexicon of wine: Conventional vs rapid-method approach. Food Quality and Preference, 83, 103920. https://doi.org/10.1016/j.foodqual.2020.103920\nOphir, Y., & Walter, D. (2023). Computational Sentiment Analysis. In R. L. Nabi & J. G. Myrick, Emotions in the Digital World: Exploring Affective Experience and Expression in Online Interactions (pp.¬†114-133). Oxford University Press.\nPang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up? Sentiment Classification using Machine Learning Techniques. arXiv. http://arxiv.org/abs/cs/0205070\n\nBack to Schedule\n\n\n\nSession 9: Text as Data - Topic Modeling\n\nDiMaggio, P., Nag, M., & Blei, D. (2013). Exploiting affinities between topic modeling and the sociological perspective on culture: Application to newspaper coverage of U.S. government arts funding. Poetics, 41(6), 570-606. https://doi.org/10.1016/j.poetic.2013.08.004\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267-297. https://doi.org/10.1093/pan/mps028\nRudkowsky, E., Haselmayer, M., Wastian, M., Jenny, M., Emrich, ≈†., & Sedlmair, M. (2018). More than Bags of Words: Sentiment Analysis with Word Embeddings. Communication Methods and Measures, 12(2-3), 140-157. https://doi.org/10.1080/19312458.2018.1455817\nStracqualursi, L., & Agati, P. (2022). Tweet topics and sentiments relating to distance learning among Italian Twitter users. Scientific Reports, 12(1). https://doi.org/10.1038/s41598-022-12915-w\n\nBack to Schedule\n\n\n\nSession 10: Unsupervised Machine Learning\nNo assigned readings for this session.\nBack to Schedule\n\n\n\nSession 11: Networks\n\nBreiger, R. L. (1974). The Duality of Persons and Groups. Social Forces, 53(2), 181. https://doi.org/10.2307/2576011\nOphir, Y., Walter, D., Arnon, D., Lokmanoglu, A. D., Tizzoni, M., Carota, J., D‚ÄôAntiga, L., & Nicastro, E. (2021). The Framing of COVID-19 in Italian Media and Its Relationship with Community Mobility: A Mixed-Method Approach. Journal of Health Communication, 26(3), 161-173. https://doi.org/10.1080/10810730.2021.1899344\nTorres, M., & Cant√∫, F. (2022). Learning to See: Convolutional Neural Networks for the Analysis of Social Science Data. Political Analysis, 30(1), 113-131. https://doi.org/10.1017/pan.2021.9\n\nBack to Schedule\n\n\n\nSession 12: Online Social Movements\n\nKraidy, M. (2017). Burning Man. In Naked Blogger of Cairo: Creative Insurgency in the Arab World (pp.¬†23-52). Harvard University Press.\nCasas, A., & Williams, N. W. (2019). Images that Matter: Online Protests and the Mobilizing Role of Pictures. Political Research Quarterly, 72(2), 360-375. https://doi.org/10.1177/1065912918786805\nFreelon, D. (2018). Computational Research in the Post-API Age. Political Communication, 35(4), 665-668. https://doi.org/10.1080/10584609.2018.1477506\nJackson, S. J., Bailey, M., & Welles, B. F. (2020). From #Ferguson to #FalconHeights: The Networked Case of Black Lives. In #hashtagactivism: Networks of race and gender justice (pp.¬†123-152). The MIT Press.\nThulin, M. (2024). Modern Statistics with R - Chapter 11: Predictive modelling and machine learning. Available at: https://www.modernstatisticswithr.com/\nTufekci, Z. (2017). Movement of Cultures. In Twitter and tear gas: The power and fragility of networked protest (pp.¬†83-114). Yale University Press.\n\nBack to Schedule\n\n\n\nSession 13: From Data to Conclusions\n\nBower, B. (2018, July 27). ‚ÄòReplication crisis‚Äô spurs reforms in how science studies are done. Science News. https://www.sciencenews.org/blog/science-the-public/replication-crisis-psychology-science-studies-statistics\nBrown, M. A., Gruen, A., Maldoff, G., Messing, S., Sanderson, Z., & Zimmer, M. (2024). Web Scraping for Research: Legal, Ethical, Institutional, and Scientific Considerations. arXiv. https://doi.org/10.48550/arXiv.2410.23432\nGrimmer, J., & Stewart, B. M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267-297. https://doi.org/10.1093/pan/mps028\nKasy, M. (2019). No Data in the Void: Values and Distributional Conflicts in Empirical Policy Research and Artificial Intelligence. Economics for Inclusive Prosperity. https://econfip.org/policy-briefs/no-data-in-the-void-values-and-distributional-conflicts-in-empirical-policy-research-and-artificial-intelligence/\nLazer, D., Kennedy, R., King, G., & Vespignani, A. (2014). The Parable of Google Flu: Traps in Big Data Analysis. Science, 343(6176), 1203-1205. https://doi.org/10.1126/science.1248506\n\nBack to Schedule",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - PhD"
    ]
  },
  {
    "objectID": "schedule_phd.html#notes",
    "href": "schedule_phd.html#notes",
    "title": "Course Schedule - PhD",
    "section": "Notes",
    "text": "Notes\nAll readings are available on Blackboard under the ‚ÄúReadings‚Äù folder.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Course Schedule - PhD"
    ]
  },
  {
    "objectID": "sessions/02/session.html",
    "href": "sessions/02/session.html",
    "title": "Session 02 - Big Data: Data Collection and Wrangling",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select ‚ÄúSave Link As‚Äù\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n‚Üê Session 01\n\n\nSchedule\n\n\nSession 03 ‚Üí",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 2 - Data Collection and Wrangling"
    ]
  },
  {
    "objectID": "sessions/04/session.html",
    "href": "sessions/04/session.html",
    "title": "Session 04 - Computational Research",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select ‚ÄúSave Link As‚Äù\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n‚Üê Session 03\n\n\nSchedule\n\n\nSession 05 ‚Üí",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 4 - Computational Research"
    ]
  },
  {
    "objectID": "sessions/06/session.html",
    "href": "sessions/06/session.html",
    "title": "Session 06 - Statistics",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select ‚ÄúSave Link As‚Äù\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n‚Üê Session 05\n\n\nSchedule\n\n\nSession 07 ‚Üí",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 6 - Statistics"
    ]
  },
  {
    "objectID": "sessions/08/session.html",
    "href": "sessions/08/session.html",
    "title": "Session 08 - MIDTERM EXAM",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select ‚ÄúSave Link As‚Äù\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n‚Üê Session 07\n\n\nSchedule\n\n\nSession 09 ‚Üí",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 8 - Midterm Exam"
    ]
  },
  {
    "objectID": "sessions/10/session.html",
    "href": "sessions/10/session.html",
    "title": "Session 10 - Unsupervised Machine Learning",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select ‚ÄúSave Link As‚Äù\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n‚Üê Session 09\n\n\nSchedule\n\n\nSession 11 ‚Üí",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 10 - Unsupervised Machine Learning"
    ]
  },
  {
    "objectID": "sessions/12/session.html",
    "href": "sessions/12/session.html",
    "title": "Session 12 - Online Social Movements",
    "section": "",
    "text": "LectureExercise\n\n\n\nLecture Materials\n\nView Lecture Notes\nView Source on GitHub\n\n\n\n\n\n\nExercise Materials\n\nDownload Exercise (.Rmd file)\nView on GitHub\n\nInstructions:\n\nRight-click the download link above and select ‚ÄúSave Link As‚Äù\nSave the .Rmd file to your computer\nOpen the file in RStudio\nComplete the exercises\n\n\n\n\n\n\n\n\n‚Üê Session 11\n\n\nSchedule\n\n\nSession 13 ‚Üí",
    "crumbs": [
      "Home",
      "Class Sessions",
      "Session 12 - Online Social Movements"
    ]
  }
]